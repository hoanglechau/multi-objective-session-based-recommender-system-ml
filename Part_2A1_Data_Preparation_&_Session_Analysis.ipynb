{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu0TMJJk2Bv_"
   },
   "source": [
    "# Part 2A1 Data Preparation & Session Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AccKnnK0qI6",
    "outputId": "50614e87-941b-44ef-be43-8110f5b714b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars==0.20.31 in /usr/local/lib/python3.11/dist-packages (0.20.31)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install polars==0.20.31\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEcVJMSt2Iz_",
    "outputId": "571cb183-6ba2-477e-e848-7d18a73ddc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzcTdqCJ2Kkk"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output'\n",
    "\n",
    "    # Memory and performance settings\n",
    "    TARGET_MEMORY_USAGE_PCT = 70  # Target memory usage percentage\n",
    "    MIN_CHUNK_SIZE = 1000         # Minimum sessions per chunk\n",
    "    MAX_CHUNK_SIZE = 50000        # Maximum sessions per chunk\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtYbX39A2Mg6"
   },
   "source": [
    "## LOGGING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3JVPlrw2Qrd",
    "outputId": "29eadfc2-9285-4192-fd2f-db7ae3a9bf92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 14:52:59] ================================================================================\n",
      "[2025-08-07 14:52:59] OTTO PART 2A1: DATA PREPARATION & SESSION ANALYSIS STARTED\n",
      "[2025-08-07 14:52:59] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Setup logging for this notebook\"\"\"\n",
    "    log_file = f\"{config.OUTPUT_PATH}/data_preparation_log.txt\"\n",
    "\n",
    "    def log_message(message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        print(log_entry)\n",
    "\n",
    "        # Also write to file\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_entry + \"\\n\")\n",
    "\n",
    "    return log_message\n",
    "\n",
    "log = setup_logging()\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"OTTO PART 2A1: DATA PREPARATION & SESSION ANALYSIS STARTED\")\n",
    "log(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84zjQFxq2S8f"
   },
   "source": [
    "## SYSTEM RESOURCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6qtCpq02U9w"
   },
   "outputs": [],
   "source": [
    "def analyze_system_resources():\n",
    "    \"\"\"\n",
    "    Analyze available system resources to optimize processing\n",
    "\n",
    "    Returns:\n",
    "        dict: System resource information\n",
    "    \"\"\"\n",
    "    log(\"Analyzing system resources...\")\n",
    "\n",
    "    # Memory analysis\n",
    "    memory = psutil.virtual_memory()\n",
    "    total_memory_gb = memory.total / (1024**3)\n",
    "    available_memory_gb = memory.available / (1024**3)\n",
    "    memory_usage_pct = memory.percent\n",
    "\n",
    "    # CPU analysis\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "\n",
    "    # Disk analysis\n",
    "    disk = psutil.disk_usage('/')\n",
    "    disk_free_gb = disk.free / (1024**3)\n",
    "\n",
    "    log(f\"   Memory: {total_memory_gb:.1f} GB total, {available_memory_gb:.1f} GB available ({memory_usage_pct:.1f}% used)\")\n",
    "    log(f\"   CPU: {cpu_count} cores, {cpu_usage:.1f}% usage\")\n",
    "    log(f\"   Disk: {disk_free_gb:.1f} GB free\")\n",
    "\n",
    "    # Determine optimal chunk size based on available memory\n",
    "    # Conservative estimation: assume each session needs ~1KB for processing\n",
    "    estimated_sessions_per_gb = 1000000  # 1M sessions per GB (conservative)\n",
    "    target_memory_gb = available_memory_gb * (config.TARGET_MEMORY_USAGE_PCT / 100)\n",
    "    optimal_chunk_size = int(target_memory_gb * estimated_sessions_per_gb)\n",
    "\n",
    "    # Clamp to reasonable bounds\n",
    "    optimal_chunk_size = max(config.MIN_CHUNK_SIZE, min(optimal_chunk_size, config.MAX_CHUNK_SIZE))\n",
    "\n",
    "    log(f\"   Optimal chunk size: {optimal_chunk_size:,} sessions\")\n",
    "\n",
    "    resource_info = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"total_memory_gb\": total_memory_gb,\n",
    "        \"available_memory_gb\": available_memory_gb,\n",
    "        \"memory_usage_pct\": memory_usage_pct,\n",
    "        \"cpu_count\": cpu_count,\n",
    "        \"cpu_usage\": cpu_usage,\n",
    "        \"disk_free_gb\": disk_free_gb,\n",
    "        \"optimal_chunk_size\": optimal_chunk_size,\n",
    "        \"target_memory_usage_pct\": config.TARGET_MEMORY_USAGE_PCT\n",
    "    }\n",
    "\n",
    "    log(\"System resource analysis completed!\")\n",
    "    return resource_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ikX6aj2XiY"
   },
   "source": [
    "## INPUT VALIDATION AND DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zncypSY2Zmq"
   },
   "outputs": [],
   "source": [
    "def validate_and_load_training_data():\n",
    "    \"\"\"\n",
    "    Validate input files and load training data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_features, validation_results)\n",
    "    \"\"\"\n",
    "    log(\"Validating input files...\")\n",
    "\n",
    "    # Required input files\n",
    "    required_files = {\n",
    "        \"train_features.parquet\": \"Processed training data from Part 1\"\n",
    "    }\n",
    "\n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_files.append(f\" {filename} - {description}\")\n",
    "        else:\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "            log(f\" {filename} - {file_size:.1f} MB\")\n",
    "\n",
    "    if missing_files:\n",
    "        log(\" MISSING REQUIRED INPUT FILES:\")\n",
    "        for missing in missing_files:\n",
    "            log(f\"   {missing}\")\n",
    "        log(\"\\n TO FIX THIS:\")\n",
    "        log(\"   Run Part 1 (Data Processing & Feature Engineering) to generate train_features.parquet\")\n",
    "        raise FileNotFoundError(\"Required input files are missing!\")\n",
    "\n",
    "    log(\"All required input files found!\")\n",
    "\n",
    "    # Load training data\n",
    "    log(\"\\n Loading training data...\")\n",
    "\n",
    "    try:\n",
    "        log(\"   Loading train_features.parquet...\")\n",
    "        train_features = pl.read_parquet(f\"{config.OUTPUT_PATH}/train_features.parquet\")\n",
    "        log(f\"    Training data loaded: {train_features.shape} ({train_features.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "        # Basic data validation\n",
    "        log(\"   Validating data structure...\")\n",
    "\n",
    "        required_columns = [\"session\", \"aid\", \"ts\", \"type\"]\n",
    "        missing_columns = [col for col in required_columns if col not in train_features.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Check data types\n",
    "        log(f\"    Data types: {train_features.dtypes}\")\n",
    "\n",
    "        # Basic statistics\n",
    "        total_events = len(train_features)\n",
    "        unique_sessions = train_features.select(\"session\").n_unique()\n",
    "        unique_items = train_features.select(\"aid\").n_unique()\n",
    "\n",
    "        log(f\"    Dataset overview:\")\n",
    "        log(f\"      Total events: {total_events:,}\")\n",
    "        log(f\"      Unique sessions: {unique_sessions:,}\")\n",
    "        log(f\"      Unique items: {unique_items:,}\")\n",
    "        log(f\"      Avg events per session: {total_events / unique_sessions:.1f}\")\n",
    "\n",
    "        # Timestamp range\n",
    "        ts_stats = train_features.select([\n",
    "            pl.col(\"ts\").min().alias(\"min_ts\"),\n",
    "            pl.col(\"ts\").max().alias(\"max_ts\")\n",
    "        ])\n",
    "\n",
    "        min_ts, max_ts = ts_stats.to_numpy()[0]\n",
    "        timespan_days = (max_ts - min_ts) / (1000 * 60 * 60 * 24)  # Convert ms to days\n",
    "\n",
    "        log(f\"      Timespan: {timespan_days:.1f} days\")\n",
    "\n",
    "        # Event type distribution\n",
    "        type_dist = train_features.group_by(\"type\").agg([\n",
    "            pl.count().alias(\"count\")\n",
    "        ]).sort(\"count\", descending=True)\n",
    "\n",
    "        log(f\"    Event type distribution:\")\n",
    "        for row in type_dist.iter_rows():\n",
    "            event_type, count = row\n",
    "            percentage = count / total_events * 100\n",
    "            log(f\"      {event_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Create validation results\n",
    "        validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_events\": total_events,\n",
    "            \"unique_sessions\": unique_sessions,\n",
    "            \"unique_items\": unique_items,\n",
    "            \"timespan_days\": timespan_days,\n",
    "            \"min_timestamp\": int(min_ts),\n",
    "            \"max_timestamp\": int(max_ts),\n",
    "            \"avg_events_per_session\": total_events / unique_sessions,\n",
    "            \"event_type_distribution\": {row[0]: int(row[1]) for row in type_dist.iter_rows()}\n",
    "        }\n",
    "\n",
    "        log(\"Training data validation completed!\")\n",
    "        return train_features, validation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error loading training data: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QYrSvT-2cup"
   },
   "source": [
    "## COMPREHENSIVE SESSION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg38EZ4F2epr"
   },
   "outputs": [],
   "source": [
    "def analyze_session_patterns(train_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of session patterns for co-visitation optimization\n",
    "\n",
    "    Args:\n",
    "        train_df: Training data\n",
    "\n",
    "    Returns:\n",
    "        dict: Session analysis results\n",
    "    \"\"\"\n",
    "    log(\"Analyzing session patterns...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Basic session statistics\n",
    "    log(\"   Computing basic session statistics...\")\n",
    "\n",
    "    session_stats = train_df.group_by(\"session\").agg([\n",
    "        pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "        pl.col(\"aid\").n_unique().alias(\"unique_items\"),\n",
    "        pl.col(\"type\").n_unique().alias(\"unique_types\"),\n",
    "        pl.col(\"ts\").min().alias(\"session_start\"),\n",
    "        pl.col(\"ts\").max().alias(\"session_end\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"clicks\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"carts\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"orders\")\n",
    "    ]).with_columns([\n",
    "        ((pl.col(\"session_end\") - pl.col(\"session_start\")) / 1000).alias(\"session_duration_sec\")\n",
    "    ])\n",
    "\n",
    "    # Session length distribution\n",
    "    length_stats = session_stats.select([\n",
    "        pl.col(\"session_length\").min().alias(\"min_length\"),\n",
    "        pl.col(\"session_length\").max().alias(\"max_length\"),\n",
    "        pl.col(\"session_length\").mean().alias(\"avg_length\"),\n",
    "        pl.col(\"session_length\").median().alias(\"median_length\"),\n",
    "        pl.col(\"session_length\").std().alias(\"std_length\")\n",
    "    ])\n",
    "\n",
    "    log(f\"    Session length statistics:\")\n",
    "    for row in length_stats.iter_rows():\n",
    "        min_len, max_len, avg_len, med_len, std_len = row\n",
    "        log(f\"      Min: {min_len}, Max: {max_len}, Avg: {avg_len:.1f}, Median: {med_len:.1f}, Std: {std_len:.1f}\")\n",
    "\n",
    "    # Session duration analysis\n",
    "    duration_stats = session_stats.select([\n",
    "        pl.col(\"session_duration_sec\").min().alias(\"min_duration\"),\n",
    "        pl.col(\"session_duration_sec\").max().alias(\"max_duration\"),\n",
    "        pl.col(\"session_duration_sec\").mean().alias(\"avg_duration\"),\n",
    "        pl.col(\"session_duration_sec\").median().alias(\"median_duration\")\n",
    "    ])\n",
    "\n",
    "    log(f\"     Session duration statistics (seconds):\")\n",
    "    for row in duration_stats.iter_rows():\n",
    "        min_dur, max_dur, avg_dur, med_dur = row\n",
    "        log(f\"      Min: {min_dur:.0f}, Max: {max_dur:.0f}, Avg: {avg_dur:.0f}, Median: {med_dur:.0f}\")\n",
    "\n",
    "    # Event type patterns within sessions\n",
    "    log(\"    Analyzing event type patterns...\")\n",
    "\n",
    "    session_type_patterns = session_stats.with_columns([\n",
    "        (pl.col(\"clicks\") > 0).alias(\"has_clicks\"),\n",
    "        (pl.col(\"carts\") > 0).alias(\"has_carts\"),\n",
    "        (pl.col(\"orders\") > 0).alias(\"has_orders\")\n",
    "    ])\n",
    "\n",
    "    pattern_counts = session_type_patterns.select([\n",
    "        pl.col(\"has_clicks\").sum().alias(\"sessions_with_clicks\"),\n",
    "        pl.col(\"has_carts\").sum().alias(\"sessions_with_carts\"),\n",
    "        pl.col(\"has_orders\").sum().alias(\"sessions_with_orders\"),\n",
    "        (pl.col(\"has_clicks\") & pl.col(\"has_carts\")).sum().alias(\"click_and_cart\"),\n",
    "        (pl.col(\"has_clicks\") & pl.col(\"has_orders\")).sum().alias(\"click_and_order\"),\n",
    "        (pl.col(\"has_carts\") & pl.col(\"has_orders\")).sum().alias(\"cart_and_order\"),\n",
    "        (pl.col(\"has_clicks\") & pl.col(\"has_carts\") & pl.col(\"has_orders\")).sum().alias(\"all_three_types\"),\n",
    "        pl.count().alias(\"total_sessions\")\n",
    "    ])\n",
    "\n",
    "    log(f\"    Session type patterns:\")\n",
    "    for row in pattern_counts.iter_rows():\n",
    "        clicks, carts, orders, click_cart, click_order, cart_order, all_three, total = row\n",
    "        log(f\"      Sessions with clicks: {clicks:,} ({clicks/total*100:.1f}%)\")\n",
    "        log(f\"      Sessions with carts: {carts:,} ({carts/total*100:.1f}%)\")\n",
    "        log(f\"      Sessions with orders: {orders:,} ({orders/total*100:.1f}%)\")\n",
    "        log(f\"      Click + Cart: {click_cart:,} ({click_cart/total*100:.1f}%)\")\n",
    "        log(f\"      Click + Order: {click_order:,} ({click_order/total*100:.1f}%)\")\n",
    "        log(f\"      Cart + Order: {cart_order:,} ({cart_order/total*100:.1f}%)\")\n",
    "        log(f\"      All three types: {all_three:,} ({all_three/total*100:.1f}%)\")\n",
    "\n",
    "    # Session length distribution buckets (FIXED VERSION)\n",
    "    log(\"    Session length distribution buckets...\")\n",
    "\n",
    "    # Create length buckets using a different approach to avoid the column error\n",
    "    length_bucket_data = []\n",
    "    for row in session_stats.select(\"session_length\").iter_rows():\n",
    "        session_length = row[0]\n",
    "        if session_length == 1:\n",
    "            bucket = \"1_item\"\n",
    "        elif session_length <= 5:\n",
    "            bucket = \"2-5_items\"\n",
    "        elif session_length <= 10:\n",
    "            bucket = \"6-10_items\"\n",
    "        elif session_length <= 20:\n",
    "            bucket = \"11-20_items\"\n",
    "        elif session_length <= 50:\n",
    "            bucket = \"21-50_items\"\n",
    "        else:\n",
    "            bucket = \"50+_items\"\n",
    "        length_bucket_data.append(bucket)\n",
    "\n",
    "    # Create DataFrame with buckets\n",
    "    bucket_series = pl.Series(\"length_bucket\", length_bucket_data)\n",
    "    bucket_counts = bucket_series.value_counts().sort(\"count\", descending=True)\n",
    "\n",
    "    log(f\"    Session length distribution:\")\n",
    "    total_sessions = len(length_bucket_data)\n",
    "    for row in bucket_counts.iter_rows():\n",
    "        bucket, count = row\n",
    "        percentage = count / total_sessions * 100\n",
    "        log(f\"      {bucket}: {count:,} sessions ({percentage:.1f}%)\")\n",
    "\n",
    "    # Potential co-visitation opportunities\n",
    "    log(\"    Analyzing co-visitation opportunities...\")\n",
    "\n",
    "    # Sessions with multiple items (potential for co-visitation)\n",
    "    multi_item_sessions = session_stats.filter(pl.col(\"session_length\") > 1).height\n",
    "    multi_item_pct = multi_item_sessions / total_sessions * 100\n",
    "\n",
    "    # Sessions with multiple unique items\n",
    "    multi_unique_sessions = session_stats.filter(pl.col(\"unique_items\") > 1).height\n",
    "    multi_unique_pct = multi_unique_sessions / total_sessions * 100\n",
    "\n",
    "    log(f\"      Sessions with >1 event: {multi_item_sessions:,} ({multi_item_pct:.1f}%)\")\n",
    "    log(f\"      Sessions with >1 unique item: {multi_unique_sessions:,} ({multi_unique_pct:.1f}%)\")\n",
    "\n",
    "    analysis_time = time.time() - start_time\n",
    "\n",
    "    # Compile analysis results\n",
    "    analysis_results = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"analysis_time_seconds\": analysis_time,\n",
    "        \"total_sessions\": int(total_sessions),\n",
    "        \"session_length_stats\": {\n",
    "            \"min\": int(length_stats.select(\"min_length\").item()),\n",
    "            \"max\": int(length_stats.select(\"max_length\").item()),\n",
    "            \"mean\": float(length_stats.select(\"avg_length\").item()),\n",
    "            \"median\": float(length_stats.select(\"median_length\").item()),\n",
    "            \"std\": float(length_stats.select(\"std_length\").item())\n",
    "        },\n",
    "        \"session_duration_stats\": {\n",
    "            \"min_seconds\": float(duration_stats.select(\"min_duration\").item()),\n",
    "            \"max_seconds\": float(duration_stats.select(\"max_duration\").item()),\n",
    "            \"mean_seconds\": float(duration_stats.select(\"avg_duration\").item()),\n",
    "            \"median_seconds\": float(duration_stats.select(\"median_duration\").item())\n",
    "        },\n",
    "        \"event_type_patterns\": {\n",
    "            \"sessions_with_clicks\": int(pattern_counts.select(\"sessions_with_clicks\").item()),\n",
    "            \"sessions_with_carts\": int(pattern_counts.select(\"sessions_with_carts\").item()),\n",
    "            \"sessions_with_orders\": int(pattern_counts.select(\"sessions_with_orders\").item()),\n",
    "            \"click_and_cart\": int(pattern_counts.select(\"click_and_cart\").item()),\n",
    "            \"click_and_order\": int(pattern_counts.select(\"click_and_order\").item()),\n",
    "            \"cart_and_order\": int(pattern_counts.select(\"cart_and_order\").item()),\n",
    "            \"all_three_types\": int(pattern_counts.select(\"all_three_types\").item())\n",
    "        },\n",
    "        \"length_distribution\": {row[0]: int(row[1]) for row in bucket_counts.iter_rows()},\n",
    "        \"covisitation_opportunities\": {\n",
    "            \"multi_item_sessions\": multi_item_sessions,\n",
    "            \"multi_item_percentage\": multi_item_pct,\n",
    "            \"multi_unique_sessions\": multi_unique_sessions,\n",
    "            \"multi_unique_percentage\": multi_unique_pct\n",
    "        }\n",
    "    }\n",
    "\n",
    "    log(f\" Session analysis completed in {analysis_time:.1f} seconds!\")\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s-t_ld82iFo"
   },
   "source": [
    "## TEMPORAL PATTERN ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cX_pH2jb2veo"
   },
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(train_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze temporal patterns for co-visitation timing optimization\n",
    "\n",
    "    Args:\n",
    "        train_df: Training data\n",
    "\n",
    "    Returns:\n",
    "        dict: Temporal analysis results\n",
    "    \"\"\"\n",
    "    log(\"Analyzing temporal patterns...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Sample data for temporal analysis (to save memory)\n",
    "    sample_size = min(1000000, len(train_df))\n",
    "    if len(train_df) > sample_size:\n",
    "        log(f\"   Sampling {sample_size:,} events for temporal analysis...\")\n",
    "        sample_df = train_df.sample(sample_size, seed=42)\n",
    "    else:\n",
    "        sample_df = train_df\n",
    "\n",
    "    # Convert timestamps to datetime for analysis\n",
    "    log(\"   Converting timestamps for temporal analysis...\")\n",
    "\n",
    "    try:\n",
    "        sample_pd = sample_df.select([\"session\", \"aid\", \"ts\", \"type\"]).to_pandas()\n",
    "        sample_pd['datetime'] = pd.to_datetime(sample_pd['ts'], unit='ms', errors='coerce')\n",
    "        sample_pd = sample_pd.dropna(subset=['datetime'])\n",
    "\n",
    "        if len(sample_pd) == 0:\n",
    "            log(\"   No valid timestamps for temporal analysis\")\n",
    "            return {\"error\": \"No valid timestamps\"}\n",
    "\n",
    "        # Extract temporal features\n",
    "        sample_pd['hour'] = sample_pd['datetime'].dt.hour\n",
    "        sample_pd['day_of_week'] = sample_pd['datetime'].dt.dayofweek\n",
    "        sample_pd['date'] = sample_pd['datetime'].dt.date\n",
    "\n",
    "        # Hourly patterns\n",
    "        log(\"   Analyzing hourly patterns...\")\n",
    "        hourly_dist = sample_pd['hour'].value_counts().sort_index()\n",
    "\n",
    "        log(f\"   Activity by hour (sample):\")\n",
    "        peak_hours = hourly_dist.nlargest(3)\n",
    "        for hour, count in peak_hours.items():\n",
    "            log(f\"      Hour {hour}: {count:,} events\")\n",
    "\n",
    "        # Daily patterns\n",
    "        log(\"   Analyzing daily patterns...\")\n",
    "        daily_dist = sample_pd['day_of_week'].value_counts().sort_index()\n",
    "\n",
    "        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        log(f\"   Activity by day of week (sample):\")\n",
    "        for day_idx, count in daily_dist.items():\n",
    "            log(f\"      {days[day_idx]}: {count:,} events\")\n",
    "\n",
    "        # Time gaps within sessions\n",
    "        log(\"   Analyzing time gaps within sessions...\")\n",
    "\n",
    "        session_time_gaps = []\n",
    "        for session_id in sample_pd['session'].unique()[:1000]:\n",
    "            session_data = sample_pd[sample_pd['session'] == session_id].sort_values('ts')\n",
    "            if len(session_data) > 1:\n",
    "                time_diffs = session_data['ts'].diff().dropna()\n",
    "                session_time_gaps.extend(time_diffs.tolist())\n",
    "\n",
    "        # FIXED: Calculate statistics properly\n",
    "        if session_time_gaps:\n",
    "            gap_series = pd.Series(session_time_gaps)\n",
    "            gap_stats = gap_series.describe()\n",
    "            p95 = gap_series.quantile(0.95)  # Calculate 95th percentile separately\n",
    "\n",
    "            log(f\"   Time gaps within sessions (ms):\")\n",
    "            log(f\"      Mean: {gap_stats['mean']:.0f} ms ({gap_stats['mean']/1000:.1f} sec)\")\n",
    "            log(f\"      Median: {gap_stats['50%']:.0f} ms ({gap_stats['50%']/1000:.1f} sec)\")\n",
    "            log(f\"      95th percentile: {p95:.0f} ms ({p95/1000:.1f} sec)\")\n",
    "\n",
    "        analysis_time = time.time() - start_time\n",
    "\n",
    "        # Compile temporal analysis results\n",
    "        temporal_results = {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"analysis_time_seconds\": analysis_time,\n",
    "            \"sample_size\": len(sample_pd),\n",
    "            \"hourly_distribution\": hourly_dist.to_dict(),\n",
    "            \"daily_distribution\": daily_dist.to_dict(),\n",
    "            \"peak_hours\": peak_hours.to_dict(),\n",
    "            \"time_gap_stats\": {\n",
    "                \"mean_ms\": float(gap_stats['mean']) if session_time_gaps else 0,\n",
    "                \"median_ms\": float(gap_stats['50%']) if session_time_gaps else 0,\n",
    "                \"p95_ms\": float(p95) if session_time_gaps else 0  # FIXED: Use p95 variable\n",
    "            } if session_time_gaps else {}\n",
    "        }\n",
    "\n",
    "        log(f\"Temporal analysis completed in {analysis_time:.1f} seconds!\")\n",
    "        return temporal_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error in temporal analysis: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIO2r8GK2zKJ"
   },
   "source": [
    "## CHUNKING STRATEGY OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgiWKOPc21ji"
   },
   "outputs": [],
   "source": [
    "def optimize_chunking_strategy(train_df: pl.DataFrame,\n",
    "                              session_analysis: Dict,\n",
    "                              resource_info: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Determine optimal chunking strategy for memory-efficient matrix generation\n",
    "\n",
    "    Args:\n",
    "        train_df: Training data\n",
    "        session_analysis: Session analysis results\n",
    "        resource_info: System resource information\n",
    "\n",
    "    Returns:\n",
    "        dict: Chunking strategy configuration\n",
    "    \"\"\"\n",
    "    log(\"Optimizing chunking strategy...\")\n",
    "\n",
    "    total_sessions = session_analysis[\"total_sessions\"]\n",
    "    avg_session_length = session_analysis[\"session_length_stats\"][\"mean\"]\n",
    "    available_memory_gb = resource_info[\"available_memory_gb\"]\n",
    "    base_chunk_size = resource_info[\"optimal_chunk_size\"]\n",
    "\n",
    "    log(f\"   Input parameters:\")\n",
    "    log(f\"      Total sessions: {total_sessions:,}\")\n",
    "    log(f\"      Avg session length: {avg_session_length:.1f}\")\n",
    "    log(f\"      Available memory: {available_memory_gb:.1f} GB\")\n",
    "    log(f\"      Base chunk size: {base_chunk_size:,}\")\n",
    "\n",
    "    # Estimate memory requirements per session for different matrix types\n",
    "    # These are conservative estimates based on typical co-visitation patterns\n",
    "\n",
    "    memory_per_session = {\n",
    "        \"click_to_click\": avg_session_length * avg_session_length * 0.1,  # Conservative estimate\n",
    "        \"click_to_buy\": avg_session_length * avg_session_length * 0.05,   # Fewer conversions\n",
    "        \"buy_to_buy\": avg_session_length * 0.2                            # Even fewer buy events\n",
    "    }\n",
    "\n",
    "    # Calculate optimal chunk sizes for each matrix type\n",
    "    chunk_sizes = {}\n",
    "\n",
    "    for matrix_type, mem_per_session in memory_per_session.items():\n",
    "        # Conservative memory allocation (use 50% of available memory)\n",
    "        target_memory_bytes = available_memory_gb * 0.5 * (1024**3)\n",
    "\n",
    "        # Estimate sessions that can fit in target memory\n",
    "        if mem_per_session > 0:\n",
    "            estimated_sessions = int(target_memory_bytes / mem_per_session)\n",
    "            chunk_size = max(config.MIN_CHUNK_SIZE, min(estimated_sessions, config.MAX_CHUNK_SIZE))\n",
    "        else:\n",
    "            chunk_size = base_chunk_size\n",
    "\n",
    "        chunk_sizes[matrix_type] = chunk_size\n",
    "\n",
    "        log(f\"   {matrix_type}: {chunk_size:,} sessions per chunk\")\n",
    "\n",
    "    # Calculate number of chunks needed\n",
    "    chunks_needed = {}\n",
    "    processing_time_estimates = {}\n",
    "\n",
    "    for matrix_type, chunk_size in chunk_sizes.items():\n",
    "        num_chunks = (total_sessions + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "        chunks_needed[matrix_type] = num_chunks\n",
    "\n",
    "        # Estimate processing time (rough estimate: 1-5 seconds per chunk)\n",
    "        estimated_time_minutes = num_chunks * 3 / 60  # 3 seconds per chunk average\n",
    "        processing_time_estimates[matrix_type] = estimated_time_minutes\n",
    "\n",
    "        log(f\"   {matrix_type}: {num_chunks} chunks, ~{estimated_time_minutes:.1f} minutes\")\n",
    "\n",
    "    # Overall strategy recommendations\n",
    "    log(\"   Strategy recommendations:\")\n",
    "\n",
    "    # Memory pressure assessment\n",
    "    total_estimated_time = sum(processing_time_estimates.values())\n",
    "    memory_pressure = \"HIGH\" if available_memory_gb < 16 else \"MEDIUM\" if available_memory_gb < 32 else \"LOW\"\n",
    "\n",
    "    log(f\"      Memory pressure: {memory_pressure}\")\n",
    "    log(f\"      Total estimated time: {total_estimated_time:.1f} minutes\")\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    if memory_pressure == \"HIGH\":\n",
    "        recommendations.extend([\n",
    "            \"Use smaller chunk sizes to avoid memory issues\",\n",
    "            \"Process matrices sequentially, not in parallel\",\n",
    "            \"Enable aggressive garbage collection between chunks\"\n",
    "        ])\n",
    "    elif memory_pressure == \"MEDIUM\":\n",
    "        recommendations.extend([\n",
    "            \"Standard chunk sizes should work well\",\n",
    "            \"Monitor memory usage during processing\",\n",
    "            \"Consider processing click-to-click matrix first (largest)\"\n",
    "        ])\n",
    "    else:\n",
    "        recommendations.extend([\n",
    "            \"Can use larger chunk sizes for better efficiency\",\n",
    "            \"Parallel processing may be possible\",\n",
    "            \"Memory should not be a limiting factor\"\n",
    "        ])\n",
    "\n",
    "    if total_estimated_time > 60:  # > 1 hour\n",
    "        recommendations.append(\"Consider running during off-peak hours due to long processing time\")\n",
    "\n",
    "    for rec in recommendations:\n",
    "        log(f\"        {rec}\")\n",
    "\n",
    "    # Compile chunking strategy\n",
    "    chunking_strategy = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_sessions\": total_sessions,\n",
    "        \"available_memory_gb\": available_memory_gb,\n",
    "        \"memory_pressure\": memory_pressure,\n",
    "        \"chunk_sizes\": chunk_sizes,\n",
    "        \"chunks_needed\": chunks_needed,\n",
    "        \"processing_time_estimates\": processing_time_estimates,\n",
    "        \"total_estimated_time_minutes\": total_estimated_time,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"memory_estimates_per_session\": memory_per_session\n",
    "    }\n",
    "\n",
    "    log(\"Chunking strategy optimization completed!\")\n",
    "    return chunking_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDpDONzp24cv"
   },
   "source": [
    "## DATA OPTIMIZATION FOR MATRIX GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE2UthX3AvTm"
   },
   "outputs": [],
   "source": [
    "def optimize_data_for_covisitation(train_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimize data representation for efficient co-visitation matrix generation\n",
    "    Memory-efficient version that processes data in chunks\n",
    "\n",
    "    Args:\n",
    "        train_df: Original training data\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Optimized data\n",
    "    \"\"\"\n",
    "    log(\"Optimizing data for co-visitation matrix generation...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    original_size = train_df.estimated_size('mb')\n",
    "\n",
    "    log(f\"   Original data: {train_df.shape}, {original_size:.1f} MB\")\n",
    "\n",
    "    # Step 1: Basic optimizations that don't require window functions\n",
    "    log(\"   Applying basic optimizations...\")\n",
    "\n",
    "    # Select only essential columns and optimize data types\n",
    "    essential_columns = [\"session\", \"aid\", \"ts\", \"type\"]\n",
    "    optimized_df = train_df.select(essential_columns).with_columns([\n",
    "        pl.col(\"session\").cast(pl.UInt32),\n",
    "        pl.col(\"aid\").cast(pl.UInt32),\n",
    "        pl.col(\"ts\").cast(pl.UInt64),\n",
    "        pl.col(\"type\").cast(pl.Categorical)\n",
    "    ])\n",
    "\n",
    "    # Sort data for optimal access patterns\n",
    "    log(\"   Sorting data for optimal access patterns...\")\n",
    "    optimized_df = optimized_df.sort([\"session\", \"ts\"])\n",
    "\n",
    "    # Step 2: Add memory-efficient derived columns\n",
    "    log(\"   Adding essential derived columns...\")\n",
    "\n",
    "    # Add event type codes (memory efficient)\n",
    "    optimized_df = optimized_df.with_columns([\n",
    "        pl.when(pl.col(\"type\") == \"clicks\").then(0)\n",
    "        .when(pl.col(\"type\") == \"carts\").then(1)\n",
    "        .otherwise(2).cast(pl.UInt8).alias(\"type_code\")\n",
    "    ])\n",
    "\n",
    "    # Step 3: Process session-level information in chunks to avoid memory issues\n",
    "    log(\"   Processing session information in memory-efficient chunks...\")\n",
    "\n",
    "    # Get unique sessions\n",
    "    unique_sessions = optimized_df.select(\"session\").unique().sort(\"session\")\n",
    "    total_sessions = len(unique_sessions)\n",
    "    chunk_size = 100000  # Process 100K sessions at a time\n",
    "\n",
    "    log(f\"   Processing {total_sessions:,} sessions in chunks of {chunk_size:,}...\")\n",
    "\n",
    "    # Pre-calculate session stats more efficiently\n",
    "    session_stats = optimized_df.group_by(\"session\").agg([\n",
    "        pl.count().alias(\"session_length\")\n",
    "    ])\n",
    "\n",
    "    # Merge session length back efficiently\n",
    "    optimized_df = optimized_df.join(session_stats, on=\"session\", how=\"left\")\n",
    "\n",
    "    # Add simple position within session (more memory efficient than ranking)\n",
    "    log(\"   Adding session positions efficiently...\")\n",
    "\n",
    "    # Use a simpler approach for session position\n",
    "    optimized_df = optimized_df.with_columns([\n",
    "        pl.int_range(pl.len()).over(\"session\").cast(pl.UInt16).alias(\"session_position\")\n",
    "    ])\n",
    "\n",
    "    # Cast session_length to appropriate type\n",
    "    optimized_df = optimized_df.with_columns([\n",
    "        pl.col(\"session_length\").cast(pl.UInt16)\n",
    "    ])\n",
    "\n",
    "    optimization_time = time.time() - start_time\n",
    "    optimized_size = optimized_df.estimated_size('mb')\n",
    "    size_change = optimized_size - original_size\n",
    "\n",
    "    log(f\"   Data optimization completed:\")\n",
    "    log(f\"      Time: {optimization_time:.1f} seconds\")\n",
    "    log(f\"      Size: {optimized_size:.1f} MB (change: {size_change:+.1f} MB)\")\n",
    "    log(f\"      Columns: {list(optimized_df.columns)}\")\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    return optimized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SapTqsd93BWW"
   },
   "source": [
    "## EVENT TYPE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jH-XYujtA4Xp"
   },
   "outputs": [],
   "source": [
    "def analyze_event_types(train_df: pl.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed analysis of event type patterns and transitions\n",
    "\n",
    "    Args:\n",
    "        train_df: Training data\n",
    "\n",
    "    Returns:\n",
    "        dict: Event type analysis results\n",
    "    \"\"\"\n",
    "    log(\"Analyzing event type patterns and transitions...\")\n",
    "\n",
    "    # Event type distribution\n",
    "    type_dist = train_df.group_by(\"type\").agg([\n",
    "        pl.count().alias(\"count\"),\n",
    "        pl.col(\"session\").n_unique().alias(\"unique_sessions\")\n",
    "    ]).sort(\"count\", descending=True)\n",
    "\n",
    "    total_events = len(train_df)\n",
    "    total_sessions = train_df.select(\"session\").n_unique()\n",
    "\n",
    "    log(\"   Event type distribution:\")\n",
    "    event_stats = {}\n",
    "    for row in type_dist.iter_rows():\n",
    "        event_type, count, sessions = row\n",
    "        event_pct = count / total_events * 100\n",
    "        session_pct = sessions / total_sessions * 100\n",
    "        event_stats[event_type] = {\n",
    "            \"count\": count,\n",
    "            \"percentage\": event_pct,\n",
    "            \"unique_sessions\": sessions,\n",
    "            \"session_percentage\": session_pct\n",
    "        }\n",
    "        log(f\"      {event_type}: {count:,} events ({event_pct:.1f}%), {sessions:,} sessions ({session_pct:.1f}%)\")\n",
    "\n",
    "    # Event transitions within sessions (sample analysis)\n",
    "    log(\"   Analyzing event transitions...\")\n",
    "\n",
    "    try:\n",
    "        sample_sessions = train_df.select(\"session\").unique().sample(10000, seed=42)[\"session\"].to_list()\n",
    "        sample_data = train_df.filter(pl.col(\"session\").is_in(sample_sessions)).sort([\"session\", \"ts\"])\n",
    "\n",
    "        # Calculate transitions\n",
    "        transitions = {}\n",
    "        sample_pd = sample_data.to_pandas()\n",
    "\n",
    "        for session_id in sample_sessions[:1000]:\n",
    "            session_events = sample_pd[sample_pd['session'] == session_id]['type'].tolist()\n",
    "            if len(session_events) > 1:\n",
    "                for i in range(len(session_events) - 1):\n",
    "                    transition = f\"{session_events[i]} -> {session_events[i+1]}\"\n",
    "                    transitions[transition] = transitions.get(transition, 0) + 1\n",
    "\n",
    "        # Sort transitions by frequency\n",
    "        sorted_transitions = sorted(transitions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        log(\"   Most common event transitions:\")\n",
    "        for i, (transition, count) in enumerate(sorted_transitions[:10], 1):\n",
    "            log(f\"      {i}. {transition}: {count:,} times\")\n",
    "\n",
    "        transition_data = dict(sorted_transitions[:20])\n",
    "    except Exception as e:\n",
    "        log(f\"   Error analyzing transitions: {e}\")\n",
    "        transition_data = {}\n",
    "\n",
    "    event_analysis = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_events\": total_events,\n",
    "        \"total_sessions\": total_sessions,\n",
    "        \"event_type_stats\": event_stats,\n",
    "        \"top_transitions\": transition_data\n",
    "    }\n",
    "\n",
    "    log(\"Event type analysis completed!\")\n",
    "    return event_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlMbxHZ03HgA"
   },
   "source": [
    "## SAVE OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxZUz7CQA7hS"
   },
   "outputs": [],
   "source": [
    "def save_preparation_outputs(covisit_data: pl.DataFrame,\n",
    "                           session_analysis: Dict,\n",
    "                           temporal_analysis: Dict,\n",
    "                           chunking_strategy: Dict,\n",
    "                           event_analysis: Dict,\n",
    "                           validation_results: Dict,\n",
    "                           resource_info: Dict):\n",
    "    \"\"\"\n",
    "    Save all outputs from data preparation\n",
    "\n",
    "    Args:\n",
    "        covisit_data: Optimized data for co-visitation\n",
    "        session_analysis: Session analysis results\n",
    "        temporal_analysis: Temporal analysis results\n",
    "        chunking_strategy: Chunking strategy configuration\n",
    "        event_analysis: Event type analysis results\n",
    "        validation_results: Input validation results\n",
    "        resource_info: System resource information\n",
    "    \"\"\"\n",
    "    log(\"Saving data preparation outputs...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Save optimized data (main output)\n",
    "        data_path = f\"{config.OUTPUT_PATH}/covisit_data_prepared.parquet\"\n",
    "        covisit_data.write_parquet(data_path)\n",
    "        file_size = os.path.getsize(data_path) / (1024*1024)\n",
    "        log(f\"   covisit_data_prepared.parquet saved ({file_size:.1f} MB)\")\n",
    "\n",
    "        # 2. Save session analysis\n",
    "        session_path = f\"{config.OUTPUT_PATH}/session_analysis.json\"\n",
    "        with open(session_path, \"w\") as f:\n",
    "            json.dump(session_analysis, f, indent=2)\n",
    "        log(f\"   session_analysis.json saved\")\n",
    "\n",
    "        # 3. Save chunking strategy\n",
    "        chunking_path = f\"{config.OUTPUT_PATH}/chunking_strategy.json\"\n",
    "        with open(chunking_path, \"w\") as f:\n",
    "            json.dump(chunking_strategy, f, indent=2)\n",
    "        log(f\"   chunking_strategy.json saved\")\n",
    "\n",
    "        # 4. Save temporal analysis\n",
    "        temporal_path = f\"{config.OUTPUT_PATH}/temporal_analysis.json\"\n",
    "        with open(temporal_path, \"w\") as f:\n",
    "            json.dump(temporal_analysis, f, indent=2)\n",
    "        log(f\"   temporal_analysis.json saved\")\n",
    "\n",
    "        # 5. Save event type analysis\n",
    "        event_path = f\"{config.OUTPUT_PATH}/event_type_analysis.json\"\n",
    "        with open(event_path, \"w\") as f:\n",
    "            json.dump(event_analysis, f, indent=2)\n",
    "        log(f\"   event_type_analysis.json saved\")\n",
    "\n",
    "        # 6. Save comprehensive summary\n",
    "        summary = {\n",
    "            \"notebook\": \"Part 2A1: Data Preparation & Session Analysis\",\n",
    "            \"completion_timestamp\": datetime.now().isoformat(),\n",
    "            \"system_resources\": resource_info,\n",
    "            \"input_validation\": validation_results,\n",
    "            \"session_analysis_summary\": {\n",
    "                \"total_sessions\": session_analysis[\"total_sessions\"],\n",
    "                \"avg_session_length\": session_analysis[\"session_length_stats\"][\"mean\"],\n",
    "                \"covisitation_opportunities\": session_analysis[\"covisitation_opportunities\"]\n",
    "            },\n",
    "            \"chunking_strategy_summary\": {\n",
    "                \"memory_pressure\": chunking_strategy[\"memory_pressure\"],\n",
    "                \"estimated_total_time\": chunking_strategy[\"total_estimated_time_minutes\"],\n",
    "                \"recommended_chunk_sizes\": chunking_strategy[\"chunk_sizes\"]\n",
    "            },\n",
    "            \"optimization_results\": {\n",
    "                \"data_size_mb\": file_size,\n",
    "                \"columns_added\": [\"session_position\", \"session_length\", \"type_code\"],\n",
    "                \"sorting_applied\": \"session, ts\"\n",
    "            },\n",
    "            \"next_steps\": [\n",
    "                \"Run Part 2A2: Click-to-Click Matrix Generation\",\n",
    "                \"Use chunking_strategy.json for optimal memory management\",\n",
    "                \"Use covisit_data_prepared.parquet as input for matrix generation\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        summary_path = f\"{config.OUTPUT_PATH}/part_2a1_summary.json\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        log(f\"   part_2a1_summary.json saved\")\n",
    "\n",
    "        log(\"All preparation outputs saved successfully!\")\n",
    "\n",
    "        return {\n",
    "            \"data_path\": data_path,\n",
    "            \"session_path\": session_path,\n",
    "            \"chunking_path\": chunking_path,\n",
    "            \"temporal_path\": temporal_path,\n",
    "            \"event_path\": event_path,\n",
    "            \"summary_path\": summary_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error saving outputs: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IccT03u5285b"
   },
   "source": [
    "## MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5DwIaUlA_zK",
    "outputId": "a770b3a4-db18-4c9e-9948-771d8d7f6040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 14:52:59] Step 1/7: Analyzing system resources...\n",
      "[2025-08-07 14:52:59] Analyzing system resources...\n",
      "[2025-08-07 14:53:00]    Memory: 51.0 GB total, 49.5 GB available (3.0% used)\n",
      "[2025-08-07 14:53:00]    CPU: 8 cores, 21.7% usage\n",
      "[2025-08-07 14:53:00]    Disk: 183.2 GB free\n",
      "[2025-08-07 14:53:00]    Optimal chunk size: 50,000 sessions\n",
      "[2025-08-07 14:53:00] System resource analysis completed!\n",
      "[2025-08-07 14:53:00] Step 2/7: Loading and validating training data...\n",
      "[2025-08-07 14:53:00] Validating input files...\n",
      "[2025-08-07 14:53:00]  train_features.parquet - 3893.1 MB\n",
      "[2025-08-07 14:53:00] All required input files found!\n",
      "[2025-08-07 14:53:00] \n",
      " Loading training data...\n",
      "[2025-08-07 14:53:00]    Loading train_features.parquet...\n",
      "[2025-08-07 14:53:33]     Training data loaded: (216384937, 22) (20636.1 MB)\n",
      "[2025-08-07 14:53:33]    Validating data structure...\n",
      "[2025-08-07 14:53:33]     Data types: [UInt32, UInt32, UInt64, Categorical(ordering='physical'), UInt32, UInt32, UInt64, UInt64, Float32, UInt32, UInt32, UInt32, UInt32, UInt32, UInt32, UInt32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "[2025-08-07 14:53:44]     Dataset overview:\n",
      "[2025-08-07 14:53:44]       Total events: 216,384,937\n",
      "[2025-08-07 14:53:44]       Unique sessions: 12,899,779\n",
      "[2025-08-07 14:53:44]       Unique items: 1,855,603\n",
      "[2025-08-07 14:53:44]       Avg events per session: 16.8\n",
      "[2025-08-07 14:53:44]       Timespan: 28.0 days\n",
      "[2025-08-07 14:53:45]     Event type distribution:\n",
      "[2025-08-07 14:53:45]       clicks: 194,625,054 (89.9%)\n",
      "[2025-08-07 14:53:45]       carts: 16,887,925 (7.8%)\n",
      "[2025-08-07 14:53:45]       orders: 4,871,958 (2.3%)\n",
      "[2025-08-07 14:53:45] Training data validation completed!\n",
      "[2025-08-07 14:53:45] Step 3/7: Performing session analysis...\n",
      "[2025-08-07 14:53:45] Analyzing session patterns...\n",
      "[2025-08-07 14:53:45]    Computing basic session statistics...\n",
      "[2025-08-07 14:54:26]     Session length statistics:\n",
      "[2025-08-07 14:54:26]       Min: 1, Max: 500, Avg: 16.8, Median: 6.0, Std: 33.5\n",
      "[2025-08-07 14:54:26]      Session duration statistics (seconds):\n",
      "[2025-08-07 14:54:26]       Min: 0, Max: 2419191, Avg: 592539, Median: 185618\n",
      "[2025-08-07 14:54:26]     Analyzing event type patterns...\n",
      "[2025-08-07 14:54:26]     Session type patterns:\n",
      "[2025-08-07 14:54:27]       Sessions with clicks: 12,899,779 (100.0%)\n",
      "[2025-08-07 14:54:27]       Sessions with carts: 3,810,706 (29.5%)\n",
      "[2025-08-07 14:54:27]       Sessions with orders: 1,626,338 (12.6%)\n",
      "[2025-08-07 14:54:27]       Click + Cart: 3,810,706 (29.5%)\n",
      "[2025-08-07 14:54:27]       Click + Order: 1,626,338 (12.6%)\n",
      "[2025-08-07 14:54:27]       Cart + Order: 1,590,375 (12.3%)\n",
      "[2025-08-07 14:54:27]       All three types: 1,590,375 (12.3%)\n",
      "[2025-08-07 14:54:27]     Session length distribution buckets...\n",
      "[2025-08-07 14:54:31]     Session length distribution:\n",
      "[2025-08-07 14:54:31]       2-5_items: 6,148,886 sessions (47.7%)\n",
      "[2025-08-07 14:54:31]       6-10_items: 2,430,369 sessions (18.8%)\n",
      "[2025-08-07 14:54:31]       11-20_items: 1,847,741 sessions (14.3%)\n",
      "[2025-08-07 14:54:31]       21-50_items: 1,523,532 sessions (11.8%)\n",
      "[2025-08-07 14:54:31]       50+_items: 946,619 sessions (7.3%)\n",
      "[2025-08-07 14:54:31]       1_item: 2,632 sessions (0.0%)\n",
      "[2025-08-07 14:54:31]     Analyzing co-visitation opportunities...\n",
      "[2025-08-07 14:54:31]       Sessions with >1 event: 12,897,147 (100.0%)\n",
      "[2025-08-07 14:54:31]       Sessions with >1 unique item: 11,880,807 (92.1%)\n",
      "[2025-08-07 14:54:31]  Session analysis completed in 46.1 seconds!\n",
      "[2025-08-07 14:54:31]    Memory cleanup after session analysis\n",
      "[2025-08-07 14:54:31] Step 4/7: Analyzing temporal patterns...\n",
      "[2025-08-07 14:54:31] Analyzing temporal patterns...\n",
      "[2025-08-07 14:54:31]    Sampling 1,000,000 events for temporal analysis...\n",
      "[2025-08-07 14:54:35]    Converting timestamps for temporal analysis...\n",
      "[2025-08-07 14:54:36]    Analyzing hourly patterns...\n",
      "[2025-08-07 14:54:36]    Activity by hour (sample):\n",
      "[2025-08-07 14:54:36]       Hour 19: 76,061 events\n",
      "[2025-08-07 14:54:36]       Hour 18: 71,895 events\n",
      "[2025-08-07 14:54:36]       Hour 20: 68,568 events\n",
      "[2025-08-07 14:54:36]    Analyzing daily patterns...\n",
      "[2025-08-07 14:54:36]    Activity by day of week (sample):\n",
      "[2025-08-07 14:54:36]       Monday: 140,375 events\n",
      "[2025-08-07 14:54:36]       Tuesday: 139,870 events\n",
      "[2025-08-07 14:54:36]       Wednesday: 138,521 events\n",
      "[2025-08-07 14:54:36]       Thursday: 132,038 events\n",
      "[2025-08-07 14:54:36]       Friday: 135,131 events\n",
      "[2025-08-07 14:54:36]       Saturday: 140,646 events\n",
      "[2025-08-07 14:54:36]       Sunday: 173,419 events\n",
      "[2025-08-07 14:54:36]    Analyzing time gaps within sessions...\n",
      "[2025-08-07 14:54:38]    Time gaps within sessions (ms):\n",
      "[2025-08-07 14:54:38]       Mean: 402809630 ms (402809.6 sec)\n",
      "[2025-08-07 14:54:38]       Median: 185070468 ms (185070.5 sec)\n",
      "[2025-08-07 14:54:38]       95th percentile: 1465227009 ms (1465227.0 sec)\n",
      "[2025-08-07 14:54:38] Temporal analysis completed in 6.8 seconds!\n",
      "[2025-08-07 14:54:38]    Memory cleanup after temporal analysis\n",
      "[2025-08-07 14:54:38] Step 5/7: Analyzing event types...\n",
      "[2025-08-07 14:54:38] Analyzing event type patterns and transitions...\n",
      "[2025-08-07 14:55:01]    Event type distribution:\n",
      "[2025-08-07 14:55:01]       clicks: 194,625,054 events (89.9%), 12,899,779 sessions (100.0%)\n",
      "[2025-08-07 14:55:01]       carts: 16,887,925 events (7.8%), 3,810,706 sessions (29.5%)\n",
      "[2025-08-07 14:55:01]       orders: 4,871,958 events (2.3%), 1,626,338 sessions (12.6%)\n",
      "[2025-08-07 14:55:01]    Analyzing event transitions...\n",
      "[2025-08-07 14:55:14]    Most common event transitions:\n",
      "[2025-08-07 14:55:14]       1. clicks -> clicks: 13,186 times\n",
      "[2025-08-07 14:55:14]       2. clicks -> carts: 1,121 times\n",
      "[2025-08-07 14:55:14]       3. carts -> clicks: 974 times\n",
      "[2025-08-07 14:55:14]       4. orders -> orders: 227 times\n",
      "[2025-08-07 14:55:14]       5. orders -> clicks: 175 times\n",
      "[2025-08-07 14:55:14]       6. carts -> carts: 142 times\n",
      "[2025-08-07 14:55:14]       7. clicks -> orders: 115 times\n",
      "[2025-08-07 14:55:14]       8. carts -> orders: 114 times\n",
      "[2025-08-07 14:55:14]       9. orders -> carts: 6 times\n",
      "[2025-08-07 14:55:14] Event type analysis completed!\n",
      "[2025-08-07 14:55:14]    Memory cleanup after event type analysis\n",
      "[2025-08-07 14:55:14] Step 6/7: Optimizing chunking strategy...\n",
      "[2025-08-07 14:55:14] Optimizing chunking strategy...\n",
      "[2025-08-07 14:55:14]    Input parameters:\n",
      "[2025-08-07 14:55:14]       Total sessions: 12,899,779\n",
      "[2025-08-07 14:55:14]       Avg session length: 16.8\n",
      "[2025-08-07 14:55:14]       Available memory: 49.5 GB\n",
      "[2025-08-07 14:55:14]       Base chunk size: 50,000\n",
      "[2025-08-07 14:55:14]    click_to_click: 50,000 sessions per chunk\n",
      "[2025-08-07 14:55:14]    click_to_buy: 50,000 sessions per chunk\n",
      "[2025-08-07 14:55:14]    buy_to_buy: 50,000 sessions per chunk\n",
      "[2025-08-07 14:55:14]    click_to_click: 258 chunks, ~12.9 minutes\n",
      "[2025-08-07 14:55:14]    click_to_buy: 258 chunks, ~12.9 minutes\n",
      "[2025-08-07 14:55:14]    buy_to_buy: 258 chunks, ~12.9 minutes\n",
      "[2025-08-07 14:55:14]    Strategy recommendations:\n",
      "[2025-08-07 14:55:14]       Memory pressure: LOW\n",
      "[2025-08-07 14:55:14]       Total estimated time: 38.7 minutes\n",
      "[2025-08-07 14:55:14]         Can use larger chunk sizes for better efficiency\n",
      "[2025-08-07 14:55:14]         Parallel processing may be possible\n",
      "[2025-08-07 14:55:14]         Memory should not be a limiting factor\n",
      "[2025-08-07 14:55:14] Chunking strategy optimization completed!\n",
      "[2025-08-07 14:55:14] Step 7/7: Optimizing data for co-visitation processing...\n",
      "[2025-08-07 14:55:14]    This is the memory-critical step - monitoring RAM usage...\n",
      "[2025-08-07 14:55:14]    Memory before optimization: 45.2% used, 27.9 GB available\n",
      "[2025-08-07 14:55:14] Optimizing data for co-visitation matrix generation...\n",
      "[2025-08-07 14:55:14]    Original data: (216384937, 22), 20636.1 MB\n",
      "[2025-08-07 14:55:14]    Applying basic optimizations...\n",
      "[2025-08-07 14:55:14]    Sorting data for optimal access patterns...\n",
      "[2025-08-07 14:55:22]    Adding essential derived columns...\n",
      "[2025-08-07 14:55:24]    Processing session information in memory-efficient chunks...\n",
      "[2025-08-07 14:55:25]    Processing 12,899,779 sessions in chunks of 100,000...\n",
      "[2025-08-07 14:55:27]    Adding session positions efficiently...\n",
      "[2025-08-07 14:55:58]    Data optimization completed:\n",
      "[2025-08-07 14:55:58]       Time: 44.2 seconds\n",
      "[2025-08-07 14:55:58]       Size: 5159.0 MB (change: -15477.1 MB)\n",
      "[2025-08-07 14:55:58]       Columns: ['session', 'aid', 'ts', 'type', 'type_code', 'session_length', 'session_position']\n",
      "[2025-08-07 14:55:58]    Clearing original training data from memory...\n",
      "[2025-08-07 14:55:59]    Memory after optimization: 38.1% used, 31.5 GB available\n",
      "[2025-08-07 14:55:59] Saving all outputs...\n",
      "[2025-08-07 14:55:59] Saving data preparation outputs...\n",
      "[2025-08-07 15:00:38]    covisit_data_prepared.parquet saved (1605.4 MB)\n",
      "[2025-08-07 15:00:38]    session_analysis.json saved\n",
      "[2025-08-07 15:00:38]    chunking_strategy.json saved\n",
      "[2025-08-07 15:00:38]    temporal_analysis.json saved\n",
      "[2025-08-07 15:00:38]    event_type_analysis.json saved\n",
      "[2025-08-07 15:00:38]    part_2a1_summary.json saved\n",
      "[2025-08-07 15:00:38] All preparation outputs saved successfully!\n",
      "[2025-08-07 15:00:38] \n",
      "================================================================================\n",
      "[2025-08-07 15:00:38] PART 2A1 COMPLETED: DATA PREPARATION & SESSION ANALYSIS\n",
      "[2025-08-07 15:00:38] ================================================================================\n",
      "[2025-08-07 15:00:38] \n",
      " KEY INSIGHTS:\n",
      "[2025-08-07 15:00:38] Total sessions: 12,899,779\n",
      "[2025-08-07 15:00:38] Avg session length: 16.8 events\n",
      "[2025-08-07 15:00:38] Co-visitation opportunities: 92.1% sessions\n",
      "[2025-08-07 15:00:38] Memory pressure: LOW\n",
      "[2025-08-07 15:00:38] Estimated total processing time: 38.7 minutes\n",
      "[2025-08-07 15:00:38] \n",
      " OUTPUT FILES GENERATED:\n",
      "[2025-08-07 15:00:38]    covisit_data_prepared.parquet (1605.4 MB)\n",
      "[2025-08-07 15:00:38]    session_analysis.json\n",
      "[2025-08-07 15:00:38]    chunking_strategy.json\n",
      "[2025-08-07 15:00:38]    temporal_analysis.json\n",
      "[2025-08-07 15:00:38]    event_type_analysis.json\n",
      "[2025-08-07 15:00:38]    part_2a1_summary.json\n",
      "[2025-08-07 15:00:38] All files saved to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-07 15:00:38] \n",
      " OPTIMAL CHUNKING STRATEGY:\n",
      "[2025-08-07 15:00:38]    click_to_click: 50,000 sessions/chunk, 258 chunks, ~12.9 min\n",
      "[2025-08-07 15:00:38]    click_to_buy: 50,000 sessions/chunk, 258 chunks, ~12.9 min\n",
      "[2025-08-07 15:00:38]    buy_to_buy: 50,000 sessions/chunk, 258 chunks, ~12.9 min\n",
      "[2025-08-07 15:00:38] \n",
      " RECOMMENDATIONS:\n",
      "[2025-08-07 15:00:38]    Can use larger chunk sizes for better efficiency\n",
      "[2025-08-07 15:00:38]    Parallel processing may be possible\n",
      "[2025-08-07 15:00:38]    Memory should not be a limiting factor\n",
      "[2025-08-07 15:00:39] \n",
      " Final memory usage: 4.0% used, 49.0 GB available\n",
      "[2025-08-07 15:00:39]  Part 2A1 finished successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Step 1: Analyze system resources\n",
    "    log(\"Step 1/7: Analyzing system resources...\")\n",
    "    resource_info = analyze_system_resources()\n",
    "\n",
    "    # Step 2: Load and validate training data\n",
    "    log(\"Step 2/7: Loading and validating training data...\")\n",
    "    train_features, validation_results = validate_and_load_training_data()\n",
    "\n",
    "    # Step 3: Perform comprehensive session analysis\n",
    "    log(\"Step 3/7: Performing session analysis...\")\n",
    "    session_analysis = analyze_session_patterns(train_features)\n",
    "\n",
    "    # Clean up memory after session analysis\n",
    "    gc.collect()\n",
    "    log(\"   Memory cleanup after session analysis\")\n",
    "\n",
    "    # Step 4: Analyze temporal patterns (with sampling to save memory)\n",
    "    log(\"Step 4/7: Analyzing temporal patterns...\")\n",
    "    temporal_analysis = analyze_temporal_patterns(train_features)\n",
    "\n",
    "    # Clean up memory after temporal analysis\n",
    "    gc.collect()\n",
    "    log(\"   Memory cleanup after temporal analysis\")\n",
    "\n",
    "    # Step 5: Perform event type analysis\n",
    "    log(\"Step 5/7: Analyzing event types...\")\n",
    "    event_type_analysis = analyze_event_types(train_features)\n",
    "\n",
    "    # Clean up memory after event analysis\n",
    "    gc.collect()\n",
    "    log(\"   Memory cleanup after event type analysis\")\n",
    "\n",
    "    # Step 6: Optimize chunking strategy\n",
    "    log(\"Step 6/7: Optimizing chunking strategy...\")\n",
    "    chunking_strategy = optimize_chunking_strategy(train_features, session_analysis, resource_info)\n",
    "\n",
    "    # Step 7: Optimize data for co-visitation processing (MEMORY CRITICAL STEP)\n",
    "    log(\"Step 7/7: Optimizing data for co-visitation processing...\")\n",
    "    log(\"   This is the memory-critical step - monitoring RAM usage...\")\n",
    "\n",
    "    # Check memory before optimization\n",
    "    memory = psutil.virtual_memory()\n",
    "    log(f\"   Memory before optimization: {memory.percent:.1f}% used, {memory.available/(1024**3):.1f} GB available\")\n",
    "\n",
    "    # Optimize data with memory-efficient version\n",
    "    covisit_data_prepared = optimize_data_for_covisitation(train_features)\n",
    "\n",
    "    # Clear the original training data from memory immediately\n",
    "    log(\"   Clearing original training data from memory...\")\n",
    "    del train_features\n",
    "    gc.collect()\n",
    "\n",
    "    # Check memory after optimization\n",
    "    memory = psutil.virtual_memory()\n",
    "    log(f\"   Memory after optimization: {memory.percent:.1f}% used, {memory.available/(1024**3):.1f} GB available\")\n",
    "\n",
    "    # Save all outputs\n",
    "    log(\"Saving all outputs...\")\n",
    "    output_paths = save_preparation_outputs(\n",
    "        covisit_data_prepared,\n",
    "        session_analysis,\n",
    "        temporal_analysis,\n",
    "        chunking_strategy,\n",
    "        event_type_analysis,\n",
    "        validation_results,\n",
    "        resource_info\n",
    "    )\n",
    "\n",
    "    ## FINAL SUMMARY\n",
    "    log(\"\\n\" + \"=\"*80)\n",
    "    log(\"PART 2A1 COMPLETED: DATA PREPARATION & SESSION ANALYSIS\")\n",
    "    log(\"=\"*80)\n",
    "\n",
    "    log(f\"\\n KEY INSIGHTS:\")\n",
    "    log(f\"Total sessions: {session_analysis['total_sessions']:,}\")\n",
    "    log(f\"Avg session length: {session_analysis['session_length_stats']['mean']:.1f} events\")\n",
    "    log(f\"Co-visitation opportunities: {session_analysis['covisitation_opportunities']['multi_unique_percentage']:.1f}% sessions\")\n",
    "    log(f\"Memory pressure: {chunking_strategy['memory_pressure']}\")\n",
    "    log(f\"Estimated total processing time: {chunking_strategy['total_estimated_time_minutes']:.1f} minutes\")\n",
    "\n",
    "    log(f\"\\n OUTPUT FILES GENERATED:\")\n",
    "    for description, path in output_paths.items():\n",
    "        filename = os.path.basename(path)\n",
    "        if path.endswith('.parquet'):\n",
    "            file_size = os.path.getsize(path) / (1024*1024)\n",
    "            log(f\"   {filename} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            log(f\"   {filename}\")\n",
    "    log(f\"All files saved to: {config.OUTPUT_PATH}\")\n",
    "\n",
    "    log(f\"\\n OPTIMAL CHUNKING STRATEGY:\")\n",
    "    for matrix_type, chunk_size in chunking_strategy[\"chunk_sizes\"].items():\n",
    "        chunks = chunking_strategy[\"chunks_needed\"][matrix_type]\n",
    "        time_est = chunking_strategy[\"processing_time_estimates\"][matrix_type]\n",
    "        log(f\"   {matrix_type}: {chunk_size:,} sessions/chunk, {chunks} chunks, ~{time_est:.1f} min\")\n",
    "\n",
    "    log(f\"\\n RECOMMENDATIONS:\")\n",
    "    for rec in chunking_strategy[\"recommendations\"]:\n",
    "        log(f\"   {rec}\")\n",
    "\n",
    "    # Final memory cleanup\n",
    "    del covisit_data_prepared\n",
    "    gc.collect()\n",
    "\n",
    "    final_memory = psutil.virtual_memory()\n",
    "    log(f\"\\n Final memory usage: {final_memory.percent:.1f}% used, {final_memory.available/(1024**3):.1f} GB available\")\n",
    "    log(f\" Part 2A1 finished successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\" ERROR: {e}\")\n",
    "    import traceback\n",
    "    log(f\"Full traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    # Emergency memory cleanup\n",
    "    try:\n",
    "        if 'train_features' in locals():\n",
    "            del train_features\n",
    "            log(\"   Emergency cleanup: train_features deleted\")\n",
    "        if 'covisit_data_prepared' in locals():\n",
    "            del covisit_data_prepared\n",
    "            log(\"   Emergency cleanup: covisit_data_prepared deleted\")\n",
    "        gc.collect()\n",
    "        log(\"   Emergency memory cleanup completed\")\n",
    "\n",
    "        # Show final memory status\n",
    "        final_memory = psutil.virtual_memory()\n",
    "        log(f\"   Memory after cleanup: {final_memory.percent:.1f}% used, {final_memory.available/(1024**3):.1f} GB available\")\n",
    "    except Exception as cleanup_error:\n",
    "        log(f\"   Error during emergency cleanup: {cleanup_error}\")\n",
    "\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
