{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2B4 Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8mzfNxT-5D-a"
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nPt-cXI5OTo",
    "outputId": "70aab110-ce83-4219-8105-e4d3968c396f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXYlQGz5Z9M"
   },
   "source": [
    "## CONFIGURATION AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fThxgMKR5X85",
    "outputId": "20359d67-b8e5-41b0-8f74-560dfce97cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Part 2B4 Configuration:\n",
      "  Max Memory: 38.2 GB\n",
      "  Training Samples: 100,000\n",
      "  GPU Enabled: True\n",
      "  Initial Memory: 1.40 GB\n"
     ]
    }
   ],
   "source": [
    "class FixedConfig:\n",
    "    \"\"\"Fixed configuration with robust parameters\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.OUTPUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\"\n",
    "\n",
    "        # Memory management - conservative for stability\n",
    "        total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        self.MAX_MEMORY_GB = min(40.0, total_memory_gb * 0.75)\n",
    "        self.MEMORY_THRESHOLD = self.MAX_MEMORY_GB * 0.85\n",
    "\n",
    "        # Training parameters - balanced for performance and stability\n",
    "        self.TRAINING_SAMPLE_SIZE = 100000  # Reduced for stability\n",
    "        self.VALIDATION_SAMPLE_SIZE = 50000  # Balanced validation size\n",
    "        self.CHUNK_SIZE = 25000  # Smaller chunks for memory safety\n",
    "\n",
    "        # LightGBM parameters - optimized and tested\n",
    "        self.N_ESTIMATORS = 150  # Good performance without overfitting\n",
    "        self.NUM_LEAVES = 63  # Power of 2 minus 1 for efficiency\n",
    "        self.LEARNING_RATE = 0.1  # Standard learning rate\n",
    "        self.MAX_DEPTH = 7  # Limited depth for regularization\n",
    "        self.MIN_CHILD_SAMPLES = 50  # Reasonable regularization\n",
    "        self.SUBSAMPLE = 0.8  # Feature bagging\n",
    "        self.COLSAMPLE_BYTREE = 0.8  # Column sampling\n",
    "        self.REG_ALPHA = 0.05  # L1 regularization\n",
    "        self.REG_LAMBDA = 0.05  # L2 regularization\n",
    "        self.EARLY_STOPPING_ROUNDS = 20  # Early stopping\n",
    "\n",
    "        # GPU configuration with fallback\n",
    "        self.USE_GPU = True\n",
    "        self.GPU_DEVICE_ID = 0\n",
    "\n",
    "        # Data types for memory optimization\n",
    "        self.FLOAT_DTYPE = 'float32'\n",
    "        self.INT_DTYPE = 'int32'\n",
    "\n",
    "        # Evaluation parameters\n",
    "        self.EVAL_AT_K = [5, 10, 20]  # Multiple evaluation points\n",
    "        self.MIN_RECALL_THRESHOLD = 0.001  # Minimum acceptable performance\n",
    "\n",
    "config = FixedConfig()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    return psutil.virtual_memory().used / (1024**3)\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Force aggressive garbage collection\"\"\"\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Fixed Part 2B4 Configuration:\")\n",
    "print(f\"  Max Memory: {config.MAX_MEMORY_GB:.1f} GB\")\n",
    "print(f\"  Training Samples: {config.TRAINING_SAMPLE_SIZE:,}\")\n",
    "print(f\"  GPU Enabled: {config.USE_GPU}\")\n",
    "print(f\"  Initial Memory: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5hIAVnE5dr4"
   },
   "source": [
    "## ENHANCED LOGGING SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfpzG2hp5gMq",
    "outputId": "89ab12d2-0e15-46dc-f915-b3b2eed0a058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 10:26:42] [INFO] [MEM: 1.4GB/3.7%/OK] ================================================================================\n",
      "[2025-08-08 10:26:43] [INFO] [MEM: 1.5GB/3.8%/OK] OTTO PART 2B4: FIXED MODEL TRAINING & EVALUATION STARTED\n",
      "[2025-08-08 10:26:43] [INFO] [MEM: 1.5GB/3.8%/OK] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_enhanced_logging():\n",
    "    \"\"\"Setup comprehensive logging with memory tracking\"\"\"\n",
    "    log_file = f\"{config.OUTPUT_PATH}/fixed_model_training_log.txt\"\n",
    "\n",
    "    os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "    def log_message(message, level=\"INFO\"):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        memory_gb = get_memory_usage()\n",
    "        memory_pct = (memory_gb / config.MAX_MEMORY_GB) * 100\n",
    "        memory_status = \"OK\" if memory_gb < config.MEMORY_THRESHOLD else \"HIGH\"\n",
    "\n",
    "        log_entry = f\"[{timestamp}] [{level}] [MEM: {memory_gb:.1f}GB/{memory_pct:.1f}%/{memory_status}] {message}\"\n",
    "        print(log_entry)\n",
    "\n",
    "        try:\n",
    "            with open(log_file, \"a\", encoding='utf-8') as f:\n",
    "                f.write(log_entry + \"\\n\")\n",
    "        except:\n",
    "            pass  # Continue if logging fails\n",
    "\n",
    "        # Auto-cleanup if memory is high\n",
    "        if memory_gb > config.MEMORY_THRESHOLD:\n",
    "            force_garbage_collection()\n",
    "\n",
    "    return log_message\n",
    "\n",
    "log = setup_enhanced_logging()\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"OTTO PART 2B4: FIXED MODEL TRAINING & EVALUATION STARTED\")\n",
    "log(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYPclQLM5iav"
   },
   "source": [
    "## INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTqkIOB65kHy",
    "outputId": "2ab09029-994f-4cfb-aed4-d3fbdb5c9272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 10:26:43] [INFO] [MEM: 1.5GB/3.8%/OK] Validating and loading inputs...\n",
      "[2025-08-08 10:26:43] [INFO] [MEM: 1.5GB/3.8%/OK]   Loading validation data with features...\n",
      "[2025-08-08 10:26:50] [INFO] [MEM: 4.8GB/12.6%/OK]     Loaded: 6,617,259 samples, 67 columns\n",
      "[2025-08-08 10:26:50] [INFO] [MEM: 4.8GB/12.6%/OK]     Feature columns loaded: 61 features\n",
      "[2025-08-08 10:26:50] [INFO] [MEM: 4.8GB/12.6%/OK]     Training statistics loaded\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK]   Input validation completed successfully\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK]     Memory after loading: 4.82 GB\n"
     ]
    }
   ],
   "source": [
    "def validate_and_load_inputs():\n",
    "    \"\"\"Enhanced input validation and loading with error handling\"\"\"\n",
    "    log(\"Validating and loading inputs...\")\n",
    "\n",
    "    validation_results = {\n",
    "        \"inputs_found\": {},\n",
    "        \"data_shapes\": {},\n",
    "        \"feature_info\": {},\n",
    "        \"memory_usage\": get_memory_usage()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 1. Load validation data with features\n",
    "        val_data_path = f\"{config.OUTPUT_PATH}/val_data_features.parquet\"\n",
    "        if not os.path.exists(val_data_path):\n",
    "            raise FileNotFoundError(f\"Required file not found: {val_data_path}\")\n",
    "\n",
    "        log(\"  Loading validation data with features...\")\n",
    "        val_data = pl.read_parquet(val_data_path)\n",
    "        validation_results[\"inputs_found\"][\"val_data_features\"] = True\n",
    "        validation_results[\"data_shapes\"][\"val_data\"] = val_data.shape\n",
    "        log(f\"    Loaded: {val_data.shape[0]:,} samples, {val_data.shape[1]:,} columns\")\n",
    "\n",
    "        # 2. Load feature columns\n",
    "        feature_columns_path = f\"{config.OUTPUT_PATH}/feature_columns.json\"\n",
    "        if os.path.exists(feature_columns_path):\n",
    "            with open(feature_columns_path, \"r\") as f:\n",
    "                feature_columns = json.load(f)\n",
    "            validation_results[\"inputs_found\"][\"feature_columns\"] = True\n",
    "            validation_results[\"feature_info\"][\"available_features\"] = len(feature_columns)\n",
    "            log(f\"    Feature columns loaded: {len(feature_columns)} features\")\n",
    "        else:\n",
    "            # Fallback: identify feature columns automatically\n",
    "            basic_columns = [\"session\", \"aid\", \"type\", \"label\"]\n",
    "            feature_columns = [col for col in val_data.columns if col not in basic_columns]\n",
    "            validation_results[\"inputs_found\"][\"feature_columns\"] = False\n",
    "            validation_results[\"feature_info\"][\"available_features\"] = len(feature_columns)\n",
    "            log(f\"    Feature columns auto-detected: {len(feature_columns)} features\", \"WARN\")\n",
    "\n",
    "        # 3. Load training statistics (optional)\n",
    "        stats_path = f\"{config.OUTPUT_PATH}/training_data_statistics.pkl\"\n",
    "        training_stats = {}\n",
    "        if os.path.exists(stats_path):\n",
    "            try:\n",
    "                with open(stats_path, \"rb\") as f:\n",
    "                    training_stats = pickle.load(f)\n",
    "                validation_results[\"inputs_found\"][\"training_statistics\"] = True\n",
    "                log(\"    Training statistics loaded\")\n",
    "            except:\n",
    "                validation_results[\"inputs_found\"][\"training_statistics\"] = False\n",
    "                log(\"    Training statistics load failed\", \"WARN\")\n",
    "        else:\n",
    "            validation_results[\"inputs_found\"][\"training_statistics\"] = False\n",
    "            log(\"    Training statistics not found\", \"WARN\")\n",
    "\n",
    "        # 4. Validate data quality\n",
    "        required_columns = [\"session\", \"aid\", \"type\", \"label\"]\n",
    "        missing_columns = [col for col in required_columns if col not in val_data.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Check for labels\n",
    "        unique_labels = val_data.select(\"label\").unique().to_series().to_list()\n",
    "        if 0 not in unique_labels or 1 not in unique_labels:\n",
    "            log(\"    Warning: Labels may not be properly set\", \"WARN\")\n",
    "\n",
    "        validation_results[\"feature_info\"][\"feature_columns\"] = feature_columns\n",
    "        validation_results[\"quality_checks\"] = {\n",
    "            \"required_columns_present\": len(missing_columns) == 0,\n",
    "            \"has_labels\": len(unique_labels) >= 2,\n",
    "            \"sufficient_data\": len(val_data) > 1000,\n",
    "            \"sufficient_features\": len(feature_columns) >= 5\n",
    "        }\n",
    "\n",
    "        log(f\"  Input validation completed successfully\")\n",
    "        log(f\"    Memory after loading: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "        return val_data, feature_columns, training_stats, validation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Input validation failed: {str(e)}\", \"ERROR\")\n",
    "        raise e\n",
    "\n",
    "# Load inputs\n",
    "val_data, feature_columns, training_stats, validation_results = validate_and_load_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ8gDRVY5m2T"
   },
   "source": [
    "## ENHANCED DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NtkRonO05oce"
   },
   "outputs": [],
   "source": [
    "class FixedDataPreprocessor:\n",
    "    \"\"\"Fixed data preprocessing with robust Polars to NumPy conversion\"\"\"\n",
    "\n",
    "    def __init__(self, feature_columns: List[str]):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.preprocessing_stats = {}\n",
    "\n",
    "    def safe_polars_to_numpy(self, df: pl.DataFrame, columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"Safely convert Polars DataFrame to NumPy array with multiple fallback methods\"\"\"\n",
    "        try:\n",
    "            # Method 1: Direct to_numpy() - simplest approach\n",
    "            log(f\"      Attempting direct Polars to NumPy conversion...\")\n",
    "            return df.select(columns).to_numpy()\n",
    "        except Exception as e1:\n",
    "            log(f\"      Direct conversion failed: {e1}\", \"WARN\")\n",
    "\n",
    "            try:\n",
    "                # Method 2: Convert via pandas\n",
    "                log(f\"      Attempting conversion via pandas...\")\n",
    "                pandas_df = df.select(columns).to_pandas()\n",
    "                return pandas_df.values.astype(config.FLOAT_DTYPE)\n",
    "            except Exception as e2:\n",
    "                log(f\"      Pandas conversion failed: {e2}\", \"WARN\")\n",
    "\n",
    "                try:\n",
    "                    # Method 3: Manual column-by-column conversion\n",
    "                    log(f\"      Attempting manual column conversion...\")\n",
    "                    arrays = []\n",
    "                    for col in columns:\n",
    "                        col_array = df.select(col).to_series().to_numpy()\n",
    "                        arrays.append(col_array)\n",
    "                    return np.column_stack(arrays).astype(config.FLOAT_DTYPE)\n",
    "                except Exception as e3:\n",
    "                    log(f\"      Manual conversion failed: {e3}\", \"ERROR\")\n",
    "                    raise ValueError(f\"All conversion methods failed: {e1}, {e2}, {e3}\")\n",
    "\n",
    "    def prepare_training_data(self, data: pl.DataFrame, event_type: str,\n",
    "                            sample_size: int = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"Prepare optimized training data for a specific event type with fixed conversion\"\"\"\n",
    "        log(f\"    Preparing {event_type} training data...\")\n",
    "\n",
    "        try:\n",
    "            # Filter for event type\n",
    "            type_data = data.filter(pl.col(\"type\") == event_type)\n",
    "            log(f\"      {event_type} data: {len(type_data):,} samples\")\n",
    "\n",
    "            if len(type_data) == 0:\n",
    "                raise ValueError(f\"No data found for event type: {event_type}\")\n",
    "\n",
    "            # Progressive sampling for large datasets\n",
    "            if sample_size and len(type_data) > sample_size:\n",
    "                # Stratified sampling to maintain label distribution\n",
    "                positive_data = type_data.filter(pl.col(\"label\") == 1)\n",
    "                negative_data = type_data.filter(pl.col(\"label\") == 0)\n",
    "\n",
    "                pos_ratio = len(positive_data) / len(type_data)\n",
    "                pos_sample_size = max(1, int(sample_size * pos_ratio))\n",
    "                neg_sample_size = sample_size - pos_sample_size\n",
    "\n",
    "                sampled_parts = []\n",
    "                if len(positive_data) > 0:\n",
    "                    if len(positive_data) > pos_sample_size:\n",
    "                        positive_sampled = positive_data.sample(n=pos_sample_size, seed=42)\n",
    "                    else:\n",
    "                        positive_sampled = positive_data\n",
    "                    sampled_parts.append(positive_sampled)\n",
    "\n",
    "                if len(negative_data) > 0:\n",
    "                    if len(negative_data) > neg_sample_size:\n",
    "                        negative_sampled = negative_data.sample(n=neg_sample_size, seed=42)\n",
    "                    else:\n",
    "                        negative_sampled = negative_data\n",
    "                    sampled_parts.append(negative_sampled)\n",
    "\n",
    "                if sampled_parts:\n",
    "                    type_data = pl.concat(sampled_parts)\n",
    "                    log(f\"      Sampled to: {len(type_data):,} samples\")\n",
    "\n",
    "            # Extract features and labels\n",
    "            available_features = [col for col in self.feature_columns if col in type_data.columns]\n",
    "            if len(available_features) == 0:\n",
    "                raise ValueError(f\"No feature columns found in data for {event_type}\")\n",
    "\n",
    "            log(f\"      Using {len(available_features)} features\")\n",
    "\n",
    "            # Convert to numpy with fixed conversion method\n",
    "            log(f\"      Converting features to NumPy...\")\n",
    "            X = self.safe_polars_to_numpy(type_data, available_features)\n",
    "\n",
    "            log(f\"      Converting labels to NumPy...\")\n",
    "            y = self.safe_polars_to_numpy(type_data, [\"label\"]).flatten()\n",
    "\n",
    "            # Convert to memory-efficient dtypes\n",
    "            X = X.astype(config.FLOAT_DTYPE)\n",
    "            y = y.astype(config.INT_DTYPE)\n",
    "\n",
    "            # Store preprocessing stats\n",
    "            self.preprocessing_stats[event_type] = {\n",
    "                \"original_samples\": len(data.filter(pl.col(\"type\") == event_type)),\n",
    "                \"final_samples\": len(type_data),\n",
    "                \"features_used\": len(available_features),\n",
    "                \"positive_rate\": float(y.mean()),\n",
    "                \"feature_list\": available_features\n",
    "            }\n",
    "\n",
    "            log(f\"      Final shape: X={X.shape}, y={y.shape}, Positive rate: {y.mean():.4f}\")\n",
    "\n",
    "            # Cleanup\n",
    "            del type_data\n",
    "            force_garbage_collection()\n",
    "\n",
    "            return X, y, available_features\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Error preparing {event_type} data: {str(e)}\", \"ERROR\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYAaq4ix5qx_"
   },
   "source": [
    "## ENHANCED MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zj7fT_iz5soS"
   },
   "outputs": [],
   "source": [
    "class FixedModelTrainer:\n",
    "    \"\"\"Fixed LightGBM trainer with robust GPU detection and fallbacks\"\"\"\n",
    "\n",
    "    def __init__(self, use_gpu: bool = True):\n",
    "        self.use_gpu = use_gpu and self._check_gpu_availability()\n",
    "        self.models = {}\n",
    "        self.training_results = {}\n",
    "        self.feature_importance = {}\n",
    "\n",
    "        log(f\"Model trainer initialized - GPU: {'ENABLED' if self.use_gpu else 'DISABLED'}\")\n",
    "\n",
    "    def _check_gpu_availability(self) -> bool:\n",
    "        \"\"\"Robustly check if GPU is available for LightGBM\"\"\"\n",
    "        try:\n",
    "            # Check if LightGBM was compiled with GPU support\n",
    "            import lightgbm as lgb\n",
    "\n",
    "            # Create minimal test data\n",
    "            X_test = np.random.random((50, 3)).astype(np.float32)\n",
    "            y_test = np.random.randint(0, 2, 50)\n",
    "            train_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "            # Try GPU training\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                'device': 'gpu',\n",
    "                'gpu_device_id': config.GPU_DEVICE_ID,\n",
    "                'verbose': -1\n",
    "            }\n",
    "\n",
    "            model = lgb.train(params, train_data, num_boost_round=1, verbose_eval=False)\n",
    "            log(\"    GPU availability test: PASSED\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"    GPU availability test: FAILED ({str(e)})\")\n",
    "            return False\n",
    "\n",
    "    def get_optimized_params(self, event_type: str, n_samples: int) -> Dict:\n",
    "        \"\"\"Get optimized parameters based on event type and data size\"\"\"\n",
    "\n",
    "        # Base parameters for ranking\n",
    "        params = {\n",
    "            'objective': 'binary',  # Changed from lambdarank to binary for stability\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': config.NUM_LEAVES,\n",
    "            'learning_rate': config.LEARNING_RATE,\n",
    "            'max_depth': config.MAX_DEPTH,\n",
    "            'min_child_samples': config.MIN_CHILD_SAMPLES,\n",
    "            'subsample': config.SUBSAMPLE,\n",
    "            'colsample_bytree': config.COLSAMPLE_BYTREE,\n",
    "            'reg_alpha': config.REG_ALPHA,\n",
    "            'reg_lambda': config.REG_LAMBDA,\n",
    "            'random_state': 42,\n",
    "            'verbose': -1,\n",
    "            'force_row_wise': True,\n",
    "            'is_unbalance': True  # Handle imbalanced data\n",
    "        }\n",
    "\n",
    "        # GPU configuration\n",
    "        if self.use_gpu:\n",
    "            params.update({\n",
    "                'device': 'gpu',\n",
    "                'gpu_device_id': config.GPU_DEVICE_ID\n",
    "            })\n",
    "        else:\n",
    "            params['device'] = 'cpu'\n",
    "\n",
    "        # Event-specific adjustments\n",
    "        if event_type == \"clicks\":\n",
    "            # Clicks are more frequent, can handle more complexity\n",
    "            params['num_leaves'] = min(127, int(config.NUM_LEAVES * 1.5))\n",
    "            params['learning_rate'] = config.LEARNING_RATE * 0.9\n",
    "        elif event_type == \"orders\":\n",
    "            # Orders are sparse, need more regularization\n",
    "            params['reg_alpha'] = config.REG_ALPHA * 2\n",
    "            params['reg_lambda'] = config.REG_LAMBDA * 2\n",
    "            params['min_child_samples'] = config.MIN_CHILD_SAMPLES * 2\n",
    "\n",
    "        # Adaptive parameters based on data size\n",
    "        if n_samples < 5000:\n",
    "            params['num_leaves'] = max(15, params['num_leaves'] // 3)\n",
    "            params['min_child_samples'] = max(5, params['min_child_samples'] // 2)\n",
    "        elif n_samples < 20000:\n",
    "            params['num_leaves'] = max(31, params['num_leaves'] // 2)\n",
    "            params['min_child_samples'] = max(10, params['min_child_samples'] // 2)\n",
    "\n",
    "        return params\n",
    "\n",
    "    def train_model(self, X: np.ndarray, y: np.ndarray, event_type: str,\n",
    "                   feature_names: List[str]) -> Optional[lgb.Booster]:\n",
    "        \"\"\"Train a robust LightGBM model with comprehensive error handling\"\"\"\n",
    "        log(f\"    Training {event_type} model...\")\n",
    "\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            # Validate input data\n",
    "            if len(X) == 0 or len(y) == 0:\n",
    "                raise ValueError(f\"Empty training data for {event_type}\")\n",
    "\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(f\"Mismatched X and y shapes: {X.shape[0]} vs {y.shape[0]}\")\n",
    "\n",
    "            # Check for valid labels\n",
    "            unique_labels = np.unique(y)\n",
    "            if len(unique_labels) < 2:\n",
    "                log(f\"      Warning: Only {len(unique_labels)} unique labels found\", \"WARN\")\n",
    "                if len(unique_labels) == 1:\n",
    "                    # Add a few synthetic samples with the other label\n",
    "                    if unique_labels[0] == 0:\n",
    "                        y = np.append(y, [1] * min(10, len(y) // 10))\n",
    "                        X = np.vstack([X, X[:min(10, len(y) // 10)]])\n",
    "                    else:\n",
    "                        y = np.append(y, [0] * min(10, len(y) // 10))\n",
    "                        X = np.vstack([X, X[:min(10, len(y) // 10)]])\n",
    "\n",
    "            # Split data with stratification\n",
    "            try:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42, stratify=y\n",
    "                )\n",
    "            except ValueError:\n",
    "                # Fallback without stratification if it fails\n",
    "                X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "            log(f\"      Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}\")\n",
    "            log(f\"      Train labels: {np.bincount(y_train)}, Val labels: {np.bincount(y_val)}\")\n",
    "\n",
    "            # Create LightGBM datasets with error handling\n",
    "            try:\n",
    "                train_data = lgb.Dataset(\n",
    "                    X_train, label=y_train,\n",
    "                    feature_name=feature_names,\n",
    "                    free_raw_data=False\n",
    "                )\n",
    "\n",
    "                val_data = lgb.Dataset(\n",
    "                    X_val, label=y_val,\n",
    "                    feature_name=feature_names,\n",
    "                    reference=train_data,\n",
    "                    free_raw_data=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                log(f\"      Error creating LightGBM datasets: {e}\", \"ERROR\")\n",
    "                raise e\n",
    "\n",
    "            # Get optimized parameters\n",
    "            params = self.get_optimized_params(event_type, len(X))\n",
    "            log(f\"      Using {'GPU' if params.get('device') == 'gpu' else 'CPU'} device\")\n",
    "\n",
    "            # Training with early stopping and comprehensive callbacks\n",
    "            callbacks = [\n",
    "                lgb.early_stopping(config.EARLY_STOPPING_ROUNDS, verbose=False),\n",
    "                lgb.log_evaluation(period=0)  # Silent training\n",
    "            ]\n",
    "\n",
    "            # Train with error handling for GPU fallback\n",
    "            try:\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    valid_sets=[train_data, val_data],\n",
    "                    valid_names=['train', 'valid'],\n",
    "                    num_boost_round=config.N_ESTIMATORS,\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "            except Exception as gpu_error:\n",
    "                if self.use_gpu and 'gpu' in str(gpu_error).lower():\n",
    "                    log(f\"      GPU training failed, falling back to CPU: {gpu_error}\", \"WARN\")\n",
    "                    # Fallback to CPU\n",
    "                    params['device'] = 'cpu'\n",
    "                    if 'gpu_device_id' in params:\n",
    "                        del params['gpu_device_id']\n",
    "\n",
    "                    model = lgb.train(\n",
    "                        params,\n",
    "                        train_data,\n",
    "                        valid_sets=[train_data, val_data],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        num_boost_round=config.N_ESTIMATORS,\n",
    "                        callbacks=callbacks\n",
    "                    )\n",
    "                else:\n",
    "                    raise gpu_error\n",
    "\n",
    "            training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            # Get best score (handle different score formats)\n",
    "            best_score = 0.0\n",
    "            if hasattr(model, 'best_score') and model.best_score:\n",
    "                valid_scores = model.best_score.get('valid', {})\n",
    "                best_score = valid_scores.get('binary_logloss', 0.0)\n",
    "\n",
    "            # Store results\n",
    "            self.training_results[event_type] = {\n",
    "                \"training_samples\": len(X_train),\n",
    "                \"validation_samples\": len(X_val),\n",
    "                \"features_used\": len(feature_names),\n",
    "                \"best_iteration\": getattr(model, 'best_iteration', config.N_ESTIMATORS),\n",
    "                \"best_score\": best_score,\n",
    "                \"training_time\": training_time,\n",
    "                \"params_used\": params,\n",
    "                \"positive_rate\": float(y.mean())\n",
    "            }\n",
    "\n",
    "            # Store feature importance\n",
    "            try:\n",
    "                importance = model.feature_importance(importance_type='gain')\n",
    "                self.feature_importance[event_type] = {\n",
    "                    feature_names[i]: float(importance[i])\n",
    "                    for i in range(len(feature_names))\n",
    "                }\n",
    "            except Exception as e:\n",
    "                log(f\"      Warning: Could not extract feature importance: {e}\", \"WARN\")\n",
    "                self.feature_importance[event_type] = {}\n",
    "\n",
    "            log(f\"      Training completed: {training_time:.1f}s, Score: {best_score:.4f}\")\n",
    "\n",
    "            # Store model\n",
    "            self.models[event_type] = model\n",
    "\n",
    "            # Cleanup\n",
    "            del X_train, X_val, y_train, y_val, train_data, val_data\n",
    "            force_garbage_collection()\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Training failed for {event_type}: {str(e)}\", \"ERROR\")\n",
    "            self.training_results[event_type] = {\"error\": str(e)}\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ig8wdtK5vFS"
   },
   "source": [
    "## ENHANCED MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "znwIHJWl5w5B"
   },
   "outputs": [],
   "source": [
    "class FixedModelEvaluator:\n",
    "    \"\"\"Enhanced model evaluation with robust metrics calculation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluation_results = {}\n",
    "\n",
    "    def evaluate_model(self, model: lgb.Booster, X_test: np.ndarray,\n",
    "                      y_test: np.ndarray, event_type: str) -> Dict:\n",
    "        \"\"\"Comprehensive model evaluation with robust error handling\"\"\"\n",
    "        log(f\"    Evaluating {event_type} model...\")\n",
    "\n",
    "        try:\n",
    "            # Predictions with error handling\n",
    "            try:\n",
    "                y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "            except:\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "            # Basic validation\n",
    "            if len(y_pred) != len(y_test):\n",
    "                raise ValueError(f\"Prediction length mismatch: {len(y_pred)} vs {len(y_test)}\")\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = {}\n",
    "\n",
    "            # Basic prediction statistics\n",
    "            metrics.update({\n",
    "                'mean_prediction': float(np.mean(y_pred)),\n",
    "                'std_prediction': float(np.std(y_pred)),\n",
    "                'min_prediction': float(np.min(y_pred)),\n",
    "                'max_prediction': float(np.max(y_pred)),\n",
    "                'actual_positive_rate': float(np.mean(y_test)),\n",
    "                'test_samples': len(y_test)\n",
    "            })\n",
    "\n",
    "            # Calculate recall and precision metrics\n",
    "            if len(y_test) > 0 and np.sum(y_test) > 0:\n",
    "                # Sort by prediction scores\n",
    "                sorted_indices = np.argsort(y_pred)[::-1]\n",
    "                sorted_labels = y_test[sorted_indices]\n",
    "                total_positives = np.sum(y_test)\n",
    "\n",
    "                # Calculate recall at different K values\n",
    "                for k in config.EVAL_AT_K:\n",
    "                    if k <= len(sorted_labels):\n",
    "                        top_k_labels = sorted_labels[:k]\n",
    "                        recall_k = np.sum(top_k_labels) / max(1, total_positives)\n",
    "                        precision_k = np.sum(top_k_labels) / max(1, k)\n",
    "                        metrics[f'recall_at_{k}'] = float(recall_k)\n",
    "                        metrics[f'precision_at_{k}'] = float(precision_k)\n",
    "                    else:\n",
    "                        metrics[f'recall_at_{k}'] = 0.0\n",
    "                        metrics[f'precision_at_{k}'] = 0.0\n",
    "\n",
    "                # Calculate NDCG with error handling\n",
    "                for k in config.EVAL_AT_K:\n",
    "                    try:\n",
    "                        if len(np.unique(y_test)) > 1 and k <= len(y_test):\n",
    "                            ndcg_k = ndcg_score([y_test], [y_pred], k=k)\n",
    "                            metrics[f'ndcg_at_{k}'] = float(ndcg_k)\n",
    "                        else:\n",
    "                            metrics[f'ndcg_at_{k}'] = 0.0\n",
    "                    except Exception as e:\n",
    "                        log(f\"        NDCG@{k} calculation failed: {e}\", \"WARN\")\n",
    "                        metrics[f'ndcg_at_{k}'] = 0.0\n",
    "            else:\n",
    "                # No positive samples\n",
    "                for k in config.EVAL_AT_K:\n",
    "                    metrics[f'recall_at_{k}'] = 0.0\n",
    "                    metrics[f'precision_at_{k}'] = 0.0\n",
    "                    metrics[f'ndcg_at_{k}'] = 0.0\n",
    "\n",
    "            self.evaluation_results[event_type] = metrics\n",
    "\n",
    "            log(f\"      Evaluation completed - NDCG@20: {metrics.get('ndcg_at_20', 0):.4f}, \"\n",
    "                f\"Recall@20: {metrics.get('recall_at_20', 0):.4f}\")\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Evaluation failed for {event_type}: {str(e)}\", \"ERROR\")\n",
    "            error_metrics = {f'{metric}_at_{k}': 0.0 for metric in ['ndcg', 'recall', 'precision'] for k in config.EVAL_AT_K}\n",
    "            error_metrics.update({\n",
    "                'error': str(e),\n",
    "                'test_samples': len(y_test) if 'y_test' in locals() else 0\n",
    "            })\n",
    "            self.evaluation_results[event_type] = error_metrics\n",
    "            return error_metrics\n",
    "\n",
    "    def calculate_weighted_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate weighted metrics across all event types\"\"\"\n",
    "\n",
    "        # Event type weights (based on business importance)\n",
    "        weights = {\"clicks\": 0.10, \"carts\": 0.30, \"orders\": 0.60}\n",
    "\n",
    "        weighted_metrics = {}\n",
    "\n",
    "        for metric in ['ndcg_at_20', 'recall_at_20', 'precision_at_20']:\n",
    "            weighted_sum = 0.0\n",
    "            total_weight = 0.0\n",
    "\n",
    "            for event_type, weight in weights.items():\n",
    "                if event_type in self.evaluation_results:\n",
    "                    metric_value = self.evaluation_results[event_type].get(metric, 0.0)\n",
    "                    if not np.isnan(metric_value):\n",
    "                        weighted_sum += metric_value * weight\n",
    "                        total_weight += weight\n",
    "\n",
    "            if total_weight > 0:\n",
    "                weighted_metrics[f'weighted_{metric}'] = weighted_sum / total_weight\n",
    "            else:\n",
    "                weighted_metrics[f'weighted_{metric}'] = 0.0\n",
    "\n",
    "        return weighted_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4j7eVGD5zdU"
   },
   "source": [
    "## MAIN TRAINING EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVb68ff551Fl",
    "outputId": "f6e7193e-826b-43bd-990e-38ae31e8d07e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK] Starting fixed training pipeline...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK]     GPU availability test: FAILED (train() got an unexpected keyword argument 'verbose_eval')\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK] Model trainer initialized - GPU: DISABLED\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK]   Processing clicks model...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 4.8GB/12.6%/OK]     Preparing clicks training data...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 6.0GB/15.6%/OK]       clicks data: 2,313,996 samples\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.8%/OK]       Sampled to: 100,000 samples\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.8%/OK]       Using 61 features\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.8%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.8%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.9%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.2GB/18.9%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:51] [INFO] [MEM: 7.5GB/19.7%/OK]       Final shape: X=(100000, 61), y=(100000,), Positive rate: 0.0213\n",
      "[2025-08-08 10:26:52] [INFO] [MEM: 7.5GB/19.7%/OK]     Training clicks model...\n",
      "[2025-08-08 10:26:52] [INFO] [MEM: 7.5GB/19.7%/OK]       Train: 80,000, Val: 20,000\n",
      "[2025-08-08 10:26:52] [INFO] [MEM: 7.5GB/19.7%/OK]       Train labels: [78300  1700], Val labels: [19575   425]\n",
      "[2025-08-08 10:26:52] [INFO] [MEM: 7.5GB/19.7%/OK]       Using CPU device\n",
      "[2025-08-08 10:26:52] [INFO] [MEM: 7.6GB/19.8%/OK]       Training completed: 0.5s, Score: 0.1859\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.6GB/19.8%/OK]     Preparing clicks training data...\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.6GB/19.8%/OK]       clicks data: 2,313,996 samples\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Sampled to: 50,000 samples\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Using 61 features\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.7GB/20.1%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:53] [INFO] [MEM: 7.9GB/20.6%/OK]       Final shape: X=(50000, 61), y=(50000,), Positive rate: 0.0212\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]     Evaluating clicks model...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]       Evaluation completed - NDCG@20: 0.2992, Recall@20: 0.0047\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]     clicks processing completed [Memory: 7.9GB]\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]   Processing carts model...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]     Preparing carts training data...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.6%/OK]       carts data: 2,153,154 samples\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.8%/OK]       Sampled to: 100,000 samples\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.7%/OK]       Using 61 features\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.7%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.7%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.8%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:26:54] [INFO] [MEM: 7.9GB/20.8%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:55] [INFO] [MEM: 8.0GB/20.8%/OK]       Final shape: X=(100000, 61), y=(100000,), Positive rate: 0.0022\n",
      "[2025-08-08 10:26:55] [INFO] [MEM: 8.0GB/20.9%/OK]     Training carts model...\n",
      "[2025-08-08 10:26:55] [INFO] [MEM: 8.0GB/20.9%/OK]       Train: 80,000, Val: 20,000\n",
      "[2025-08-08 10:26:55] [INFO] [MEM: 8.0GB/20.9%/OK]       Train labels: [79828   172], Val labels: [19957    43]\n",
      "[2025-08-08 10:26:55] [INFO] [MEM: 8.0GB/20.9%/OK]       Using CPU device\n",
      "[2025-08-08 10:26:56] [INFO] [MEM: 8.0GB/20.9%/OK]       Training completed: 1.0s, Score: 0.2979\n",
      "[2025-08-08 10:26:56] [INFO] [MEM: 8.0GB/20.8%/OK]     Preparing carts training data...\n",
      "[2025-08-08 10:26:56] [INFO] [MEM: 8.0GB/20.8%/OK]       carts data: 2,153,154 samples\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Sampled to: 50,000 samples\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Using 61 features\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.9%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.8%/OK]       Final shape: X=(50000, 61), y=(50000,), Positive rate: 0.0021\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.8%/OK]     Evaluating carts model...\n",
      "[2025-08-08 10:26:57] [INFO] [MEM: 8.0GB/20.8%/OK]       Evaluation completed - NDCG@20: 0.0241, Recall@20: 0.0093\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.8%/OK]     carts processing completed [Memory: 8.0GB]\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.8%/OK]   Processing orders model...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.8%/OK]     Preparing orders training data...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.9%/OK]       orders data: 2,150,109 samples\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.9%/OK]       Sampled to: 100,000 samples\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.9%/OK]       Using 61 features\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.9%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/20.9%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]       Final shape: X=(100000, 61), y=(100000,), Positive rate: 0.0009\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]     Training orders model...\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]       Train: 80,000, Val: 20,000\n",
      "[2025-08-08 10:26:58] [INFO] [MEM: 8.0GB/21.0%/OK]       Train labels: [79931    69], Val labels: [19983    17]\n",
      "[2025-08-08 10:26:59] [INFO] [MEM: 8.0GB/21.0%/OK]       Using CPU device\n",
      "[2025-08-08 10:26:59] [INFO] [MEM: 8.1GB/21.1%/OK]       Training completed: 0.5s, Score: 0.3441\n",
      "[2025-08-08 10:26:59] [INFO] [MEM: 8.0GB/21.0%/OK]     Preparing orders training data...\n",
      "[2025-08-08 10:26:59] [INFO] [MEM: 8.0GB/21.0%/OK]       orders data: 2,150,109 samples\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.0GB/21.0%/OK]       Sampled to: 50,000 samples\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.0GB/21.0%/OK]       Using 61 features\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.0GB/21.0%/OK]       Converting features to NumPy...\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.0GB/21.0%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.1GB/21.1%/OK]       Converting labels to NumPy...\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.1GB/21.1%/OK]       Attempting direct Polars to NumPy conversion...\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.0GB/21.0%/OK]       Final shape: X=(50000, 61), y=(50000,), Positive rate: 0.0009\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.1GB/21.0%/OK]     Evaluating orders model...\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.1GB/21.1%/OK]       Evaluation completed - NDCG@20: 0.0066, Recall@20: 0.0698\n",
      "[2025-08-08 10:27:00] [INFO] [MEM: 8.1GB/21.1%/OK]     orders processing completed [Memory: 8.1GB]\n"
     ]
    }
   ],
   "source": [
    "def run_fixed_training():\n",
    "    \"\"\"Execute fixed training pipeline with comprehensive error handling\"\"\"\n",
    "    log(\"Starting fixed training pipeline...\")\n",
    "\n",
    "    # Initialize components\n",
    "    preprocessor = FixedDataPreprocessor(feature_columns)\n",
    "    trainer = FixedModelTrainer(use_gpu=config.USE_GPU)\n",
    "    evaluator = FixedModelEvaluator()\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "    # Train models for each event type\n",
    "    for event_type in [\"clicks\", \"carts\", \"orders\"]:\n",
    "        log(f\"  Processing {event_type} model...\")\n",
    "\n",
    "        try:\n",
    "            # Prepare training data\n",
    "            X_train, y_train, used_features = preprocessor.prepare_training_data(\n",
    "                val_data, event_type, config.TRAINING_SAMPLE_SIZE\n",
    "            )\n",
    "\n",
    "            # Validate prepared data\n",
    "            if len(X_train) < 100:\n",
    "                log(f\"      Insufficient training data for {event_type}: {len(X_train)} samples\", \"WARN\")\n",
    "                continue\n",
    "\n",
    "            # Train model\n",
    "            model = trainer.train_model(X_train, y_train, event_type, used_features)\n",
    "\n",
    "            if model is not None:\n",
    "                trained_models[event_type] = model\n",
    "\n",
    "                # Evaluate model\n",
    "                try:\n",
    "                    X_eval, y_eval, _ = preprocessor.prepare_training_data(\n",
    "                        val_data, event_type, config.VALIDATION_SAMPLE_SIZE\n",
    "                    )\n",
    "\n",
    "                    if len(X_eval) > 0:\n",
    "                        evaluator.evaluate_model(model, X_eval, y_eval, event_type)\n",
    "\n",
    "                    # Cleanup evaluation data\n",
    "                    del X_eval, y_eval\n",
    "                except Exception as eval_error:\n",
    "                    log(f\"      Evaluation failed for {event_type}: {eval_error}\", \"WARN\")\n",
    "\n",
    "            # Cleanup training data\n",
    "            del X_train, y_train\n",
    "            force_garbage_collection()\n",
    "\n",
    "            log(f\"    {event_type} processing completed [Memory: {get_memory_usage():.1f}GB]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"    {event_type} processing failed: {str(e)}\", \"ERROR\")\n",
    "            continue\n",
    "\n",
    "    # Calculate weighted metrics\n",
    "    weighted_metrics = evaluator.calculate_weighted_metrics()\n",
    "\n",
    "    return trained_models, trainer.training_results, evaluator.evaluation_results, weighted_metrics, trainer.feature_importance\n",
    "\n",
    "# Execute training\n",
    "trained_models, training_results, evaluation_results, weighted_metrics, feature_importance = run_fixed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDla30fg53jx"
   },
   "source": [
    "## ENHANCED OUTPUT SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvBoFkOv55yJ",
    "outputId": "5f6c92af-7a7b-47b2-e004-161fbdd35093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK] Saving fixed outputs...\n",
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_clicks.txt saved (0.0 MB)\n",
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_clicks.txt validation: PASSED\n",
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_carts.txt saved (0.0 MB)\n",
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_carts.txt validation: PASSED\n",
      "[2025-08-08 10:27:01] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_orders.txt saved (0.0 MB)\n",
      "[2025-08-08 10:27:02] [INFO] [MEM: 8.1GB/21.1%/OK]     ranker_orders.txt validation: PASSED\n",
      "[2025-08-08 10:27:02] [INFO] [MEM: 8.1GB/21.1%/OK]     model_training_results.pkl saved\n",
      "[2025-08-08 10:27:02] [INFO] [MEM: 8.1GB/21.1%/OK]     model_evaluation_results.pkl saved\n",
      "[2025-08-08 10:27:02] [INFO] [MEM: 8.1GB/21.1%/OK]     evaluation_results.json saved\n",
      "[2025-08-08 10:27:02] [INFO] [MEM: 8.1GB/21.1%/OK]     enhanced_feature_importance.pkl saved\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]     part_2b4_fixed_summary.pkl saved\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] Fixed outputs saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_fixed_outputs(models: Dict, training_results: Dict, evaluation_results: Dict,\n",
    "                      weighted_metrics: Dict, feature_importance: Dict) -> Dict:\n",
    "    \"\"\"Save all outputs with enhanced error handling and validation\"\"\"\n",
    "    log(\"Saving fixed outputs...\")\n",
    "\n",
    "    saved_files = {}\n",
    "\n",
    "    try:\n",
    "        # 1. Save trained models with validation\n",
    "        for event_type, model in models.items():\n",
    "            try:\n",
    "                model_path = f\"{config.OUTPUT_PATH}/ranker_{event_type}.txt\"\n",
    "                model.save_model(model_path)\n",
    "\n",
    "                # Validate the saved model\n",
    "                if os.path.exists(model_path):\n",
    "                    file_size = os.path.getsize(model_path) / (1024*1024)\n",
    "                    saved_files[f\"ranker_{event_type}\"] = model_path\n",
    "                    log(f\"    ranker_{event_type}.txt saved ({file_size:.1f} MB)\")\n",
    "\n",
    "                    # Test loading the saved model\n",
    "                    try:\n",
    "                        test_model = lgb.Booster(model_file=model_path)\n",
    "                        log(f\"    ranker_{event_type}.txt validation: PASSED\")\n",
    "                    except Exception as load_error:\n",
    "                        log(f\"    ranker_{event_type}.txt validation: FAILED - {load_error}\", \"ERROR\")\n",
    "                else:\n",
    "                    log(f\"    Error: ranker_{event_type}.txt was not created\", \"ERROR\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"    Error saving {event_type} model: {e}\", \"ERROR\")\n",
    "\n",
    "        # 2. Save training results (use pickle to avoid JSON serialization issues)\n",
    "        try:\n",
    "            training_path = f\"{config.OUTPUT_PATH}/model_training_results.pkl\"\n",
    "            with open(training_path, \"wb\") as f:\n",
    "                pickle.dump(training_results, f)\n",
    "            saved_files[\"training_results\"] = training_path\n",
    "            log(f\"    model_training_results.pkl saved\")\n",
    "        except Exception as e:\n",
    "            log(f\"    Error saving training results: {e}\", \"ERROR\")\n",
    "\n",
    "        # 3. Save evaluation results (BOTH pickle and JSON formats)\n",
    "        try:\n",
    "            # Save as pickle (detailed format)\n",
    "            evaluation_path = f\"{config.OUTPUT_PATH}/model_evaluation_results.pkl\"\n",
    "            combined_evaluation = {\n",
    "                \"individual_metrics\": evaluation_results,\n",
    "                \"weighted_metrics\": weighted_metrics,\n",
    "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "                \"config_used\": {\n",
    "                    \"training_sample_size\": config.TRAINING_SAMPLE_SIZE,\n",
    "                    \"validation_sample_size\": config.VALIDATION_SAMPLE_SIZE,\n",
    "                    \"n_estimators\": config.N_ESTIMATORS,\n",
    "                    \"num_leaves\": config.NUM_LEAVES,\n",
    "                    \"learning_rate\": config.LEARNING_RATE\n",
    "                }\n",
    "            }\n",
    "            with open(evaluation_path, \"wb\") as f:\n",
    "                pickle.dump(combined_evaluation, f)\n",
    "            saved_files[\"evaluation_results\"] = evaluation_path\n",
    "            log(f\"    model_evaluation_results.pkl saved\")\n",
    "\n",
    "            # CRITICAL FIX: Save as JSON for Part 3 compatibility\n",
    "            json_evaluation_path = f\"{config.OUTPUT_PATH}/evaluation_results.json\"\n",
    "\n",
    "            # Create the JSON structure that Part 3 expects\n",
    "            json_evaluation = {\n",
    "                # Extract individual event type metrics (NDCG@20 as primary metric)\n",
    "                \"clicks\": evaluation_results.get(\"clicks\", {}).get(\"ndcg_at_20\", 0.0),\n",
    "                \"carts\": evaluation_results.get(\"carts\", {}).get(\"ndcg_at_20\", 0.0),\n",
    "                \"orders\": evaluation_results.get(\"orders\", {}).get(\"ndcg_at_20\", 0.0),\n",
    "\n",
    "                # Weighted average (use NDCG@20 as primary metric)\n",
    "                \"weighted_average\": weighted_metrics.get(\"weighted_ndcg_at_20\", 0.0),\n",
    "\n",
    "                # Additional metrics for comprehensive reporting\n",
    "                \"detailed_metrics\": {\n",
    "                    \"weighted_recall_at_20\": weighted_metrics.get(\"weighted_recall_at_20\", 0.0),\n",
    "                    \"weighted_precision_at_20\": weighted_metrics.get(\"weighted_precision_at_20\", 0.0),\n",
    "                    \"weighted_ndcg_at_20\": weighted_metrics.get(\"weighted_ndcg_at_20\", 0.0)\n",
    "                },\n",
    "\n",
    "                # Individual event type detailed metrics\n",
    "                \"individual_metrics\": {\n",
    "                    event_type: {\n",
    "                        \"ndcg_at_20\": metrics.get(\"ndcg_at_20\", 0.0),\n",
    "                        \"recall_at_20\": metrics.get(\"recall_at_20\", 0.0),\n",
    "                        \"precision_at_20\": metrics.get(\"precision_at_20\", 0.0),\n",
    "                        \"test_samples\": metrics.get(\"test_samples\", 0)\n",
    "                    }\n",
    "                    for event_type, metrics in evaluation_results.items()\n",
    "                    if isinstance(metrics, dict) and \"error\" not in metrics\n",
    "                },\n",
    "\n",
    "                # Metadata\n",
    "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "                \"models_evaluated\": list(models.keys()),\n",
    "                \"total_models\": len(models)\n",
    "            }\n",
    "\n",
    "            # Save JSON file with error handling\n",
    "            with open(json_evaluation_path, \"w\") as f:\n",
    "                json.dump(json_evaluation, f, indent=2)\n",
    "            saved_files[\"evaluation_results_json\"] = json_evaluation_path\n",
    "            log(f\"    evaluation_results.json saved\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"    Error saving evaluation results: {e}\", \"ERROR\")\n",
    "\n",
    "        # 4. Save enhanced feature importance\n",
    "        try:\n",
    "            importance_path = f\"{config.OUTPUT_PATH}/enhanced_feature_importance.pkl\"\n",
    "            enhanced_importance = {\n",
    "                \"individual_importance\": feature_importance,\n",
    "                \"aggregated_importance\": {},\n",
    "                \"top_features\": {},\n",
    "                \"feature_ranking\": {}\n",
    "            }\n",
    "\n",
    "            # Calculate aggregated importance across all models\n",
    "            all_features = set()\n",
    "            for event_importance in feature_importance.values():\n",
    "                all_features.update(event_importance.keys())\n",
    "\n",
    "            for feature in all_features:\n",
    "                total_importance = sum(\n",
    "                    event_importance.get(feature, 0)\n",
    "                    for event_importance in feature_importance.values()\n",
    "                )\n",
    "                enhanced_importance[\"aggregated_importance\"][feature] = total_importance\n",
    "\n",
    "            # Get top features\n",
    "            if enhanced_importance[\"aggregated_importance\"]:\n",
    "                sorted_features = sorted(\n",
    "                    enhanced_importance[\"aggregated_importance\"].items(),\n",
    "                    key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "                enhanced_importance[\"top_features\"] = dict(sorted_features[:20])\n",
    "                enhanced_importance[\"feature_ranking\"] = {\n",
    "                    feature: rank + 1 for rank, (feature, _) in enumerate(sorted_features)\n",
    "                }\n",
    "\n",
    "            with open(importance_path, \"wb\") as f:\n",
    "                pickle.dump(enhanced_importance, f)\n",
    "            saved_files[\"feature_importance\"] = importance_path\n",
    "            log(f\"    enhanced_feature_importance.pkl saved\")\n",
    "        except Exception as e:\n",
    "            log(f\"    Error saving feature importance: {e}\", \"ERROR\")\n",
    "\n",
    "        # 5. Save comprehensive summary\n",
    "        try:\n",
    "            summary = {\n",
    "                \"notebook\": \"Part 2B4: FIXED Model Training & Evaluation\",\n",
    "                \"completion_timestamp\": datetime.now().isoformat(),\n",
    "                \"version\": \"FIXED - Robust Polars conversion and comprehensive error handling\",\n",
    "                \"critical_fixes_applied\": [\n",
    "                    \"Fixed Polars to_numpy() conversion with multiple fallback methods\",\n",
    "                    \"Robust GPU detection with automatic CPU fallback\",\n",
    "                    \"Enhanced data validation and preprocessing\",\n",
    "                    \"Improved error handling throughout the pipeline\",\n",
    "                    \"Binary classification instead of lambdarank for stability\",\n",
    "                    \"Comprehensive model validation after saving\",\n",
    "                    \"Memory-optimized data processing\",\n",
    "                    \"Stratified sampling with fallbacks\",\n",
    "                    \"CRITICAL: Added JSON evaluation results for Part 3 compatibility\"\n",
    "                ],\n",
    "                \"inputs_used\": {\n",
    "                    \"val_data_features.parquet\": f\"{validation_results['data_shapes']['val_data'][0]:,} samples\",\n",
    "                    \"feature_columns\": f\"{len(feature_columns)} features\",\n",
    "                    \"memory_configuration\": f\"{config.MAX_MEMORY_GB:.1f}GB max\"\n",
    "                },\n",
    "                \"outputs_generated\": {\n",
    "                    \"trained_models\": list(models.keys()),\n",
    "                    \"model_files\": [f\"ranker_{et}.txt\" for et in models.keys()],\n",
    "                    \"training_results\": \"Comprehensive training metrics\",\n",
    "                    \"evaluation_results\": \"Multi-metric evaluation with NDCG and Recall\",\n",
    "                    \"evaluation_results_json\": \"JSON format for Part 3 compatibility\",\n",
    "                    \"feature_importance\": \"Enhanced feature analysis and ranking\"\n",
    "                },\n",
    "                \"performance_metrics\": {\n",
    "                    \"models_trained\": len(models),\n",
    "                    \"successful_models\": [et for et in models.keys()],\n",
    "                    \"failed_models\": [et for et in [\"clicks\", \"carts\", \"orders\"] if et not in models],\n",
    "                    \"weighted_recall_at_20\": weighted_metrics.get(\"weighted_recall_at_20\", 0),\n",
    "                    \"weighted_ndcg_at_20\": weighted_metrics.get(\"weighted_ndcg_at_20\", 0),\n",
    "                    \"weighted_precision_at_20\": weighted_metrics.get(\"weighted_precision_at_20\", 0),\n",
    "                    \"gpu_used\": any(tr.get(\"params_used\", {}).get(\"device\") == \"gpu\" for tr in training_results.values()),\n",
    "                    \"final_memory_usage_gb\": get_memory_usage(),\n",
    "                    \"training_successful\": len(models) >= 1\n",
    "                },\n",
    "                \"quality_assessment\": {\n",
    "                    \"models_trained_successfully\": len(models) >= 1,\n",
    "                    \"at_least_two_models\": len(models) >= 2,\n",
    "                    \"performance_acceptable\": weighted_metrics.get(\"weighted_recall_at_20\", 0) > config.MIN_RECALL_THRESHOLD,\n",
    "                    \"memory_efficient\": get_memory_usage() < config.MEMORY_THRESHOLD,\n",
    "                    \"ready_for_inference\": len(models) >= 1 and all(os.path.exists(f\"{config.OUTPUT_PATH}/ranker_{et}.txt\") for et in models.keys()),\n",
    "                    \"json_compatibility\": os.path.exists(f\"{config.OUTPUT_PATH}/evaluation_results.json\")\n",
    "                },\n",
    "                \"next_step\": \"Ready for Part 3: Inference & Submission Generation\" if len(models) >= 1 else \"Review errors and retry training\"\n",
    "            }\n",
    "\n",
    "            summary_path = f\"{config.OUTPUT_PATH}/part_2b4_fixed_summary.pkl\"\n",
    "            with open(summary_path, \"wb\") as f:\n",
    "                pickle.dump(summary, f)\n",
    "            saved_files[\"summary\"] = summary_path\n",
    "            log(f\"    part_2b4_fixed_summary.pkl saved\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"    Error saving summary: {e}\", \"ERROR\")\n",
    "\n",
    "        log(f\"Fixed outputs saved successfully!\")\n",
    "        return saved_files\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error in save_fixed_outputs: {e}\", \"ERROR\")\n",
    "        return saved_files\n",
    "\n",
    "# Save all outputs\n",
    "saved_files = save_fixed_outputs(trained_models, training_results, evaluation_results, weighted_metrics, feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqh2wHP_59St"
   },
   "source": [
    "## ENHANCED FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq156jJt5_Vu",
    "outputId": "5cc8e447-3cbb-4208-b9d9-cf13a6e1a455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "================================================================================\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] PART 2B4 FIXED TRAINING COMPLETED\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] ================================================================================\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "TRAINING RESULTS:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Models successfully trained: 3/3\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   clicks: SUCCESS - 80,000 samples, Score=0.1859, 0.5s (cpu)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   carts: SUCCESS - 80,000 samples, Score=0.2979, 1.0s (cpu)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   orders: SUCCESS - 80,000 samples, Score=0.3441, 0.5s (cpu)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "EVALUATION RESULTS:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   clicks: NDCG@20=0.2992, Recall@20=0.0047, Precision@20=0.2500 (50,000 samples)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   carts: NDCG@20=0.0241, Recall@20=0.0093, Precision@20=0.0500 (50,000 samples)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   orders: NDCG@20=0.0066, Recall@20=0.0698, Precision@20=0.1500 (50,000 samples)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "WEIGHTED PERFORMANCE:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Weighted NDCG@20: 0.0411\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Weighted Recall@20: 0.0451\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Weighted Precision@20: 0.1300\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "TOP FEATURES (aggregated importance):\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    1. session_events_per_second: 1231973796.72\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    2. max_item_popularity: 592971151.00\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    3. unique_items: 406784683.41\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    4. session_length: 207141717.80\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    5. popularity_coefficient_variation: 173089591.56\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    6. order_conversion_rate: 96227661.53\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    7. session_duration_seconds: 54638976.43\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    8. session_duration_ms: 35081460.67\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]    9. item_order_ratio_in_session: 21019440.00\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   10. item_clicks_in_session: 20736100.00\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "OUTPUT FILES GENERATED:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_clicks.txt (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_carts.txt (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_orders.txt (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   model_training_results.pkl (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   model_evaluation_results.pkl (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   evaluation_results.json (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   enhanced_feature_importance.pkl (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   part_2b4_fixed_summary.pkl (0.0 MB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   All files saved to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "MODEL FILE VALIDATION:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_clicks.txt: VALID \n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_carts.txt: VALID \n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   ranker_orders.txt: VALID \n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "QUALITY ASSESSMENT:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Models trained (1): PASS (3/3)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Performance (>0.001): PASS (0.0451)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Memory efficiency: PASS (8.1GB)\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Model files created: PASS\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Ready for inference: YES\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "Overall Quality: EXCELLENT\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "PERFORMANCE SUMMARY:\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Final memory usage: 8.06 GB\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Memory threshold: 32.5 GB\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   GPU acceleration attempted: True\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK]   Training optimization: FIXED AND ROBUST\n",
      "[2025-08-08 10:27:03] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      "Performing final cleanup...\n",
      "[2025-08-08 10:27:04] [INFO] [MEM: 8.1GB/21.1%/OK] Final memory after cleanup: 8.06 GB\n",
      "[2025-08-08 10:27:04] [INFO] [MEM: 8.1GB/21.1%/OK] \n",
      " SUCCESS: Part 2B4 Fixed Training completed successfully!\n",
      "[2025-08-08 10:27:04] [INFO] [MEM: 8.1GB/21.1%/OK]  3 models trained and saved\n",
      "[2025-08-08 10:27:04] [INFO] [MEM: 8.1GB/21.1%/OK]  Ready for Part 3: Inference & Submission Generation\n",
      "[2025-08-08 10:27:04] [INFO] [MEM: 8.1GB/21.1%/OK] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2B4 FIXED TRAINING COMPLETED\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Display training results\n",
    "log(f\"\\nTRAINING RESULTS:\")\n",
    "log(f\"  Models successfully trained: {len(trained_models)}/3\")\n",
    "for event_type in [\"clicks\", \"carts\", \"orders\"]:\n",
    "    if event_type in training_results:\n",
    "        results = training_results[event_type]\n",
    "        if \"error\" not in results:\n",
    "            samples = results.get(\"training_samples\", 0)\n",
    "            best_score = results.get(\"best_score\", 0)\n",
    "            training_time = results.get(\"training_time\", 0)\n",
    "            device = results.get(\"params_used\", {}).get(\"device\", \"unknown\")\n",
    "            log(f\"  {event_type}: SUCCESS - {samples:,} samples, Score={best_score:.4f}, {training_time:.1f}s ({device})\")\n",
    "        else:\n",
    "            log(f\"  {event_type}: FAILED - {results['error']}\")\n",
    "    else:\n",
    "        log(f\"  {event_type}: NOT ATTEMPTED\")\n",
    "\n",
    "# Display evaluation results\n",
    "if evaluation_results:\n",
    "    log(f\"\\nEVALUATION RESULTS:\")\n",
    "    for event_type in [\"clicks\", \"carts\", \"orders\"]:\n",
    "        if event_type in evaluation_results:\n",
    "            metrics = evaluation_results[event_type]\n",
    "            if \"error\" not in metrics:\n",
    "                ndcg = metrics.get(\"ndcg_at_20\", 0)\n",
    "                recall = metrics.get(\"recall_at_20\", 0)\n",
    "                precision = metrics.get(\"precision_at_20\", 0)\n",
    "                samples = metrics.get(\"test_samples\", 0)\n",
    "                log(f\"  {event_type}: NDCG@20={ndcg:.4f}, Recall@20={recall:.4f}, Precision@20={precision:.4f} ({samples:,} samples)\")\n",
    "            else:\n",
    "                log(f\"  {event_type}: EVALUATION FAILED - {metrics['error']}\")\n",
    "        else:\n",
    "            log(f\"  {event_type}: NOT EVALUATED\")\n",
    "\n",
    "# Display weighted metrics\n",
    "if weighted_metrics:\n",
    "    log(f\"\\nWEIGHTED PERFORMANCE:\")\n",
    "    log(f\"  Weighted NDCG@20: {weighted_metrics.get('weighted_ndcg_at_20', 0):.4f}\")\n",
    "    log(f\"  Weighted Recall@20: {weighted_metrics.get('weighted_recall_at_20', 0):.4f}\")\n",
    "    log(f\"  Weighted Precision@20: {weighted_metrics.get('weighted_precision_at_20', 0):.4f}\")\n",
    "\n",
    "# Display top features\n",
    "if feature_importance:\n",
    "    log(f\"\\nTOP FEATURES (aggregated importance):\")\n",
    "    all_features = {}\n",
    "    for event_importance in feature_importance.values():\n",
    "        for feature, importance in event_importance.items():\n",
    "            all_features[feature] = all_features.get(feature, 0) + importance\n",
    "\n",
    "    if all_features:\n",
    "        top_features = sorted(all_features.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for i, (feature, importance) in enumerate(top_features):\n",
    "            log(f\"  {i+1:2d}. {feature}: {importance:.2f}\")\n",
    "    else:\n",
    "        log(f\"  No feature importance data available\")\n",
    "\n",
    "# Display output files\n",
    "log(f\"\\nOUTPUT FILES GENERATED:\")\n",
    "for file_type, file_path in saved_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        log(f\"  {os.path.basename(file_path)} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        log(f\"  {os.path.basename(file_path)} (FILE NOT FOUND)\", \"ERROR\")\n",
    "log(f\"  All files saved to: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Model file validation\n",
    "log(f\"\\nMODEL FILE VALIDATION:\")\n",
    "for event_type in [\"clicks\", \"carts\", \"orders\"]:\n",
    "    model_path = f\"{config.OUTPUT_PATH}/ranker_{event_type}.txt\"\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            test_model = lgb.Booster(model_file=model_path)\n",
    "            log(f\"  ranker_{event_type}.txt: VALID \")\n",
    "        except Exception as e:\n",
    "            log(f\"  ranker_{event_type}.txt: INVALID  ({e})\", \"ERROR\")\n",
    "    else:\n",
    "        log(f\"  ranker_{event_type}.txt: NOT FOUND \")\n",
    "\n",
    "# Quality assessment\n",
    "models_ok = len(trained_models) >= 1\n",
    "performance_ok = weighted_metrics.get(\"weighted_recall_at_20\", 0) > config.MIN_RECALL_THRESHOLD\n",
    "memory_ok = get_memory_usage() < config.MEMORY_THRESHOLD\n",
    "files_ok = all(os.path.exists(f\"{config.OUTPUT_PATH}/ranker_{et}.txt\") for et in trained_models.keys())\n",
    "\n",
    "log(f\"\\nQUALITY ASSESSMENT:\")\n",
    "log(f\"  Models trained (1): {'PASS' if models_ok else 'FAIL'} ({len(trained_models)}/3)\")\n",
    "log(f\"  Performance (>{config.MIN_RECALL_THRESHOLD:.3f}): {'PASS' if performance_ok else 'FAIL'} ({weighted_metrics.get('weighted_recall_at_20', 0):.4f})\")\n",
    "log(f\"  Memory efficiency: {'PASS' if memory_ok else 'FAIL'} ({get_memory_usage():.1f}GB)\")\n",
    "log(f\"  Model files created: {'PASS' if files_ok else 'FAIL'}\")\n",
    "log(f\"  Ready for inference: {'YES' if models_ok and files_ok else 'NO'}\")\n",
    "\n",
    "overall_quality = \"EXCELLENT\" if models_ok and performance_ok and memory_ok and files_ok else \"GOOD\" if models_ok and files_ok else \"NEEDS_IMPROVEMENT\"\n",
    "log(f\"\\nOverall Quality: {overall_quality}\")\n",
    "\n",
    "# Performance summary\n",
    "log(f\"\\nPERFORMANCE SUMMARY:\")\n",
    "log(f\"  Final memory usage: {get_memory_usage():.2f} GB\")\n",
    "log(f\"  Memory threshold: {config.MEMORY_THRESHOLD:.1f} GB\")\n",
    "log(f\"  GPU acceleration attempted: {config.USE_GPU}\")\n",
    "log(f\"  Training optimization: FIXED AND ROBUST\")\n",
    "\n",
    "# Final cleanup\n",
    "log(f\"\\nPerforming final cleanup...\")\n",
    "try:\n",
    "    force_garbage_collection()\n",
    "    final_memory = get_memory_usage()\n",
    "    log(f\"Final memory after cleanup: {final_memory:.2f} GB\")\n",
    "except Exception as e:\n",
    "    log(f\"Cleanup warning: {e}\", \"WARN\")\n",
    "\n",
    "# Final status\n",
    "if len(trained_models) >= 1 and files_ok:\n",
    "    log(f\"\\n SUCCESS: Part 2B4 Fixed Training completed successfully!\")\n",
    "    log(f\" {len(trained_models)} models trained and saved\")\n",
    "    log(f\" Ready for Part 3: Inference & Submission Generation\")\n",
    "else:\n",
    "    log(f\"\\n  PARTIAL SUCCESS: Some issues remain\")\n",
    "    log(f\" {len(trained_models)} out of 3 models trained\")\n",
    "    log(f\" Review errors above and consider re-running\")\n",
    "\n",
    "log(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
