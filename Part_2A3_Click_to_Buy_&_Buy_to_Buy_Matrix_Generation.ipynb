{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m1IaRM08W5l"
   },
   "source": [
    "# Part 2A3 Click to Buy & Buy to Buy Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgVt3j3f8urf",
    "outputId": "6f947111-ad94-4562-c068-4beeb72fed2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars==0.20.31 in /usr/local/lib/python3.11/dist-packages (0.20.31)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install polars==0.20.31\n",
    "!pip install psutil\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QspyLnBo8yxr",
    "outputId": "3b8ae79d-58f2-4e6f-e867-5095b752f706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EOLKoWt_839G"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output'\n",
    "\n",
    "    # Matrix generation parameters\n",
    "    MAX_CANDIDATES_PER_ITEM = 40      # Maximum candidates to store per source item\n",
    "    CLICK_TO_BUY_TIME_WINDOW_DAYS = 14  # Time window for click-to-buy relationships\n",
    "    BUY_TO_BUY_TIME_WINDOW_DAYS = 7   # Time window for buy-to-buy relationships\n",
    "\n",
    "    # Memory management\n",
    "    MEMORY_CHECK_INTERVAL = 20        # Check memory every N chunks\n",
    "    CHUNK_SIZE_MULTIPLIER = 2         # Can use larger chunks than click-to-click\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZwn3oxQ84Yt"
   },
   "source": [
    "## LOGGING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1tSoGwt86pM",
    "outputId": "0a9ffc2e-c22d-4f77-b921-ddc9a2a4b698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:41:43] ================================================================================\n",
      "[2025-08-07 18:41:43] OTTO PART 2A3: CLICK-TO-BUY & BUY-TO-BUY MATRIX GENERATION STARTED\n",
      "[2025-08-07 18:41:43] ================================================================================\n",
      "[2025-08-07 18:41:43] Initial memory status: LOW\n"
     ]
    }
   ],
   "source": [
    "def setup_logging_and_monitoring():\n",
    "    \"\"\"Setup comprehensive logging and memory monitoring with emergency handling\"\"\"\n",
    "    log_file = f\"{config.OUTPUT_PATH}/buy_matrices_generation_log.txt\"\n",
    "    memory_log = []\n",
    "\n",
    "    def log_message(message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        print(log_entry)\n",
    "\n",
    "        # Also write to file\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_entry + \"\\n\")\n",
    "\n",
    "    def check_memory_usage():\n",
    "        \"\"\"Check current memory usage and log critical levels\"\"\"\n",
    "        memory = psutil.virtual_memory()\n",
    "        memory_pct = memory.percent\n",
    "        available_gb = memory.available / (1024**3)\n",
    "\n",
    "        memory_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"memory_percent\": memory_pct,\n",
    "            \"available_gb\": available_gb,\n",
    "            \"used_gb\": memory.used / (1024**3)\n",
    "        }\n",
    "        memory_log.append(memory_entry)\n",
    "\n",
    "        if memory_pct > 90:\n",
    "            log_message(f\"CRITICAL MEMORY WARNING: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"CRITICAL\"\n",
    "        elif memory_pct > 80:\n",
    "            log_message(f\"HIGH MEMORY USAGE: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"HIGH\"\n",
    "        elif memory_pct > 60:\n",
    "            log_message(f\"Memory usage: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"NORMAL\"\n",
    "        else:\n",
    "            return \"LOW\"\n",
    "\n",
    "    return log_message, check_memory_usage, memory_log\n",
    "\n",
    "log, check_memory, memory_log = setup_logging_and_monitoring()\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"OTTO PART 2A3: CLICK-TO-BUY & BUY-TO-BUY MATRIX GENERATION STARTED\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Initial memory check\n",
    "initial_memory_status = check_memory()\n",
    "log(f\"Initial memory status: {initial_memory_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYaKkT4kCFIh"
   },
   "source": [
    "## INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVzVwyCDCHHx",
    "outputId": "8406338a-55ca-434b-c550-58b85ac57b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:41:43]  Validating and loading inputs...\n",
      "[2025-08-07 18:41:43]  covisit_data_prepared.parquet - 1605.4 MB\n",
      "[2025-08-07 18:41:43]  chunking_strategy.json - 0.0 MB\n",
      "[2025-08-07 18:41:43]  session_analysis.json - 0.0 MB\n",
      "[2025-08-07 18:41:43]  All required input files found!\n",
      "[2025-08-07 18:41:43] \n",
      " Loading input data...\n",
      "[2025-08-07 18:41:43]    Loading optimized training data...\n",
      "[2025-08-07 18:41:52]     Prepared data: (216384937, 7) (5159.0 MB)\n",
      "[2025-08-07 18:41:52]    Loading chunking strategy...\n",
      "[2025-08-07 18:41:52]     Chunking strategy loaded\n",
      "[2025-08-07 18:41:52]    Loading session analysis...\n",
      "[2025-08-07 18:41:52]     Session analysis loaded\n",
      "[2025-08-07 18:41:52]    Validating data for buy matrix generation...\n",
      "[2025-08-07 18:42:07]     Event distribution validation:\n",
      "[2025-08-07 18:42:07]       clicks: 194,625,054 events, 12,899,779 sessions\n",
      "[2025-08-07 18:42:07]       carts: 16,887,925 events, 3,810,706 sessions\n",
      "[2025-08-07 18:42:07]       orders: 4,871,958 events, 1,626,338 sessions\n",
      "[2025-08-07 18:42:16]     Conversion opportunities:\n",
      "[2025-08-07 18:42:16]       Sessions with multiple event types: 3,846,669\n",
      "[2025-08-07 18:42:16]       Potential click-to-cart conversions: 3,810,706\n",
      "[2025-08-07 18:42:16]       Potential click-to-order conversions: 1,626,338\n",
      "[2025-08-07 18:42:16]     Chunk sizes:\n",
      "[2025-08-07 18:42:16]       Click-to-buy: 100,000 sessions\n",
      "[2025-08-07 18:42:16]       Buy-to-buy: 150,000 sessions\n",
      "[2025-08-07 18:42:16]  Input validation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def validate_and_load_inputs():\n",
    "    \"\"\"\n",
    "    Validate and load all required inputs\n",
    "\n",
    "    Returns:\n",
    "        tuple: (prepared_data, chunking_strategy, session_analysis, validation_results)\n",
    "    \"\"\"\n",
    "    log(\" Validating and loading inputs...\")\n",
    "\n",
    "    # Required input files\n",
    "    required_files = {\n",
    "        \"covisit_data_prepared.parquet\": \"Optimized training data from Part 2A1\",\n",
    "        \"chunking_strategy.json\": \"Memory management configuration from Part 2A1\",\n",
    "        \"session_analysis.json\": \"Session analysis results from Part 2A1\"\n",
    "    }\n",
    "\n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_files.append(f\" {filename} - {description}\")\n",
    "        else:\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "            log(f\" {filename} - {file_size:.1f} MB\")\n",
    "\n",
    "    if missing_files:\n",
    "        log(\" MISSING REQUIRED INPUT FILES:\")\n",
    "        for missing in missing_files:\n",
    "            log(f\"   {missing}\")\n",
    "        raise FileNotFoundError(\"Required input files are missing!\")\n",
    "\n",
    "    log(\" All required input files found!\")\n",
    "\n",
    "    # Load data\n",
    "    log(\"\\n Loading input data...\")\n",
    "\n",
    "    try:\n",
    "        # Load prepared data\n",
    "        log(\"   Loading optimized training data...\")\n",
    "        prepared_data = pl.read_parquet(f\"{config.OUTPUT_PATH}/covisit_data_prepared.parquet\")\n",
    "        log(f\"    Prepared data: {prepared_data.shape} ({prepared_data.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "        # Load chunking strategy\n",
    "        log(\"   Loading chunking strategy...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/chunking_strategy.json\", \"r\") as f:\n",
    "            chunking_strategy = json.load(f)\n",
    "        log(f\"    Chunking strategy loaded\")\n",
    "\n",
    "        # Load session analysis\n",
    "        log(\"   Loading session analysis...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/session_analysis.json\", \"r\") as f:\n",
    "            session_analysis = json.load(f)\n",
    "        log(f\"    Session analysis loaded\")\n",
    "\n",
    "        # Validate data for buy matrix generation\n",
    "        log(\"   Validating data for buy matrix generation...\")\n",
    "\n",
    "        # Check event type distribution\n",
    "        event_dist = prepared_data.group_by(\"type\").agg([\n",
    "            pl.count().alias(\"count\"),\n",
    "            pl.col(\"session\").n_unique().alias(\"unique_sessions\")\n",
    "        ])\n",
    "\n",
    "        log(f\"    Event distribution validation:\")\n",
    "        event_stats = {}\n",
    "        for row in event_dist.iter_rows():\n",
    "            event_type, count, sessions = row\n",
    "            event_stats[event_type] = {\"count\": count, \"sessions\": sessions}\n",
    "            log(f\"      {event_type}: {count:,} events, {sessions:,} sessions\")\n",
    "\n",
    "        # Check for conversion opportunities\n",
    "        click_sessions = event_stats.get(\"clicks\", {}).get(\"sessions\", 0)\n",
    "        cart_sessions = event_stats.get(\"carts\", {}).get(\"sessions\", 0)\n",
    "        order_sessions = event_stats.get(\"orders\", {}).get(\"sessions\", 0)\n",
    "\n",
    "        # Sessions with multiple event types (conversion opportunities)\n",
    "        mixed_sessions = prepared_data.group_by(\"session\").agg([\n",
    "            pl.col(\"type\").n_unique().alias(\"unique_types\")\n",
    "        ]).filter(pl.col(\"unique_types\") > 1).height\n",
    "\n",
    "        log(f\"    Conversion opportunities:\")\n",
    "        log(f\"      Sessions with multiple event types: {mixed_sessions:,}\")\n",
    "        log(f\"      Potential click-to-cart conversions: {min(click_sessions, cart_sessions):,}\")\n",
    "        log(f\"      Potential click-to-order conversions: {min(click_sessions, order_sessions):,}\")\n",
    "\n",
    "        # Determine chunk sizes\n",
    "        base_chunk_size = chunking_strategy.get(\"chunk_sizes\", {}).get(\"click_to_buy\", 15000)\n",
    "        click_to_buy_chunk_size = int(base_chunk_size * config.CHUNK_SIZE_MULTIPLIER)\n",
    "        buy_to_buy_chunk_size = int(base_chunk_size * config.CHUNK_SIZE_MULTIPLIER * 1.5)  # Even larger for buy-to-buy\n",
    "\n",
    "        log(f\"    Chunk sizes:\")\n",
    "        log(f\"      Click-to-buy: {click_to_buy_chunk_size:,} sessions\")\n",
    "        log(f\"      Buy-to-buy: {buy_to_buy_chunk_size:,} sessions\")\n",
    "\n",
    "        validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_statistics\": event_stats,\n",
    "            \"mixed_sessions\": mixed_sessions,\n",
    "            \"chunk_sizes\": {\n",
    "                \"click_to_buy\": click_to_buy_chunk_size,\n",
    "                \"buy_to_buy\": buy_to_buy_chunk_size\n",
    "            },\n",
    "            \"time_windows\": {\n",
    "                \"click_to_buy_days\": config.CLICK_TO_BUY_TIME_WINDOW_DAYS,\n",
    "                \"buy_to_buy_days\": config.BUY_TO_BUY_TIME_WINDOW_DAYS\n",
    "            }\n",
    "        }\n",
    "\n",
    "        log(\" Input validation completed successfully!\")\n",
    "        return prepared_data, chunking_strategy, session_analysis, validation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\" Error loading input data: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Load and validate inputs\n",
    "prepared_data, chunking_strategy, session_analysis, validation_results = validate_and_load_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4IzMz3FCNVO"
   },
   "source": [
    "## CLICK-TO-BUY MATRIX GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mXKDY0moCP1s"
   },
   "outputs": [],
   "source": [
    "class ClickToBuyMatrixGenerator:\n",
    "    \"\"\"\n",
    "    Memory-efficient click-to-buy co-visitation matrix generator with progressive saving\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int, time_window_days: int):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.time_window_ms = time_window_days * 24 * 60 * 60 * 1000  # Convert to milliseconds\n",
    "        self.covisitation_counts = {}\n",
    "        self.temporal_stats = {\"valid_transitions\": 0, \"invalid_transitions\": 0, \"total_sessions\": 0}\n",
    "\n",
    "        # Memory management\n",
    "        self.processed_chunks = 0\n",
    "        self.start_time = time.time()\n",
    "        self.last_save_time = time.time()\n",
    "        self.max_pairs_in_memory = 2000000  # 2M pairs before cleanup\n",
    "        self.save_interval_chunks = 15      # Save every 15 chunks\n",
    "\n",
    "        log(\"  Initializing memory-efficient click-to-buy matrix generator...\")\n",
    "        log(f\"   Chunk size: {self.chunk_size:,} sessions\")\n",
    "        log(f\"   Time window: {time_window_days} days ({self.time_window_ms:,} ms)\")\n",
    "        log(f\"   Max pairs in memory: {self.max_pairs_in_memory:,}\")\n",
    "        log(f\"   Save interval: {self.save_interval_chunks} chunks\")\n",
    "\n",
    "    def estimate_memory_usage(self) -> float:\n",
    "        \"\"\"Estimate current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            total_pairs = sum(len(targets) for targets in self.covisitation_counts.values())\n",
    "            # Each pair takes ~50 bytes (item IDs + count + overhead)\n",
    "            estimated_mb = total_pairs * 50 / (1024 * 1024)\n",
    "            return estimated_mb\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def cleanup_memory(self, force_aggressive: bool = False):\n",
    "        \"\"\"Clean up memory by keeping only top candidates\"\"\"\n",
    "        log(f\"   Performing click-to-buy memory cleanup (aggressive={force_aggressive})...\")\n",
    "\n",
    "        before_size = len(self.covisitation_counts)\n",
    "        before_memory = self.estimate_memory_usage()\n",
    "\n",
    "        # Keep only top candidates per source item\n",
    "        max_candidates = config.MAX_CANDIDATES_PER_ITEM if not force_aggressive else 20\n",
    "\n",
    "        cleaned_counts = {}\n",
    "        for source_aid, targets in self.covisitation_counts.items():\n",
    "            if len(targets) > max_candidates:\n",
    "                sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                top_targets = dict(sorted_targets[:max_candidates])\n",
    "                cleaned_counts[source_aid] = top_targets\n",
    "            else:\n",
    "                cleaned_counts[source_aid] = targets\n",
    "\n",
    "        self.covisitation_counts = cleaned_counts\n",
    "        gc.collect()\n",
    "\n",
    "        after_memory = self.estimate_memory_usage()\n",
    "        log(f\"   CTB memory cleanup: {before_memory:.1f}MB → {after_memory:.1f}MB\")\n",
    "\n",
    "    def save_intermediate_results(self, force_save: bool = False):\n",
    "        \"\"\"Save intermediate results and clean up memory\"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        if force_save or (self.processed_chunks % self.save_interval_chunks == 0 and self.processed_chunks > 0):\n",
    "            log(f\"   Saving CTB intermediate results (chunk {self.processed_chunks})...\")\n",
    "\n",
    "            try:\n",
    "                temp_path = f\"{config.OUTPUT_PATH}/ctb_matrix_temp_chunk_{self.processed_chunks}.pkl\"\n",
    "\n",
    "                # Convert to final format before saving\n",
    "                temp_matrix = {}\n",
    "                for source_aid, targets in self.covisitation_counts.items():\n",
    "                    if targets:\n",
    "                        sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                        top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                        temp_matrix[source_aid] = top_targets\n",
    "\n",
    "                with open(temp_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"partial_matrix\": temp_matrix,\n",
    "                        \"processed_chunks\": self.processed_chunks,\n",
    "                        \"temporal_stats\": self.temporal_stats,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }, f)\n",
    "\n",
    "                file_size = os.path.getsize(temp_path) / (1024*1024)\n",
    "                log(f\"   CTB intermediate results saved: {temp_path} ({file_size:.1f} MB)\")\n",
    "                self.last_save_time = current_time\n",
    "\n",
    "                # Clear memory after saving\n",
    "                self.covisitation_counts.clear()\n",
    "                gc.collect()\n",
    "                log(f\"   CTB memory cleared after save\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"   Failed to save CTB intermediate results: {e}\")\n",
    "\n",
    "    def process_session_chunk(self, session_chunk: List[int], data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Process a chunk of sessions for click-to-buy relationships with memory limits\"\"\"\n",
    "        chunk_covisitations = {}\n",
    "\n",
    "        # Filter data for this chunk\n",
    "        chunk_data = data.filter(pl.col(\"session\").is_in(session_chunk))\n",
    "\n",
    "        if len(chunk_data) == 0:\n",
    "            return chunk_covisitations\n",
    "\n",
    "        # Process each session with limits\n",
    "        session_groups = chunk_data.group_by(\"session\").agg([\n",
    "            pl.col(\"aid\").alias(\"aids\"),\n",
    "            pl.col(\"ts\").alias(\"timestamps\"),\n",
    "            pl.col(\"type\").alias(\"types\")\n",
    "        ])\n",
    "\n",
    "        pairs_in_chunk = 0\n",
    "        max_pairs_per_chunk = 500000  # Limit pairs per chunk\n",
    "\n",
    "        for row in session_groups.iter_rows():\n",
    "            session_id, aids, timestamps, types = row\n",
    "            self.temporal_stats[\"total_sessions\"] += 1\n",
    "\n",
    "            if len(aids) < 2:\n",
    "                continue\n",
    "\n",
    "            # Limit session size to prevent memory explosion\n",
    "            if len(aids) > 50:  # Skip very long sessions\n",
    "                continue\n",
    "\n",
    "            # Create event list with timestamps\n",
    "            events = list(zip(aids, timestamps, types))\n",
    "            events.sort(key=lambda x: x[1])  # Sort by timestamp\n",
    "\n",
    "            # Find click-to-buy relationships with limits\n",
    "            for i in range(len(events)):\n",
    "                for j in range(i + 1, len(events)):\n",
    "                    if pairs_in_chunk >= max_pairs_per_chunk:\n",
    "                        break\n",
    "\n",
    "                    aid1, ts1, type1 = events[i]\n",
    "                    aid2, ts2, type2 = events[j]\n",
    "\n",
    "                    # Check for click-to-buy pattern\n",
    "                    if type1 == \"clicks\" and type2 in [\"carts\", \"orders\"]:\n",
    "                        time_diff = ts2 - ts1\n",
    "\n",
    "                        # Check temporal constraint\n",
    "                        if 0 <= time_diff <= self.time_window_ms:\n",
    "                            self.temporal_stats[\"valid_transitions\"] += 1\n",
    "\n",
    "                            if aid1 != aid2:  # Different items\n",
    "                                chunk_covisitations[(aid1, aid2)] = chunk_covisitations.get((aid1, aid2), 0) + 1\n",
    "                                pairs_in_chunk += 1\n",
    "                        else:\n",
    "                            self.temporal_stats[\"invalid_transitions\"] += 1\n",
    "\n",
    "                if pairs_in_chunk >= max_pairs_per_chunk:\n",
    "                    break\n",
    "\n",
    "        return chunk_covisitations\n",
    "\n",
    "    def merge_chunk_results(self, chunk_covisitations: Dict):\n",
    "        \"\"\"Merge chunk results with memory management\"\"\"\n",
    "        for (aid1, aid2), count in chunk_covisitations.items():\n",
    "            if aid1 not in self.covisitation_counts:\n",
    "                self.covisitation_counts[aid1] = {}\n",
    "            self.covisitation_counts[aid1][aid2] = self.covisitation_counts[aid1].get(aid2, 0) + count\n",
    "\n",
    "    def generate_matrix(self, data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Generate complete click-to-buy co-visitation matrix with memory management\"\"\"\n",
    "        log(\"  Starting memory-efficient click-to-buy matrix generation...\")\n",
    "\n",
    "        # Get sessions with both clicks and purchases\n",
    "        session_type_counts = data.group_by(\"session\").agg([\n",
    "            pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"clicks\"),\n",
    "            pl.col(\"type\").filter(pl.col(\"type\").is_in([\"carts\", \"orders\"])).count().alias(\"purchases\")\n",
    "        ]).filter((pl.col(\"clicks\") > 0) & (pl.col(\"purchases\") > 0))\n",
    "\n",
    "        conversion_sessions = session_type_counts[\"session\"].to_list()\n",
    "        total_sessions = len(conversion_sessions)\n",
    "\n",
    "        log(f\"    Processing {total_sessions:,} sessions with conversion opportunities\")\n",
    "        log(f\"    Using {self.chunk_size:,} sessions per chunk\")\n",
    "\n",
    "        num_chunks = (total_sessions + self.chunk_size - 1) // self.chunk_size\n",
    "        log(f\"    Total chunks to process: {num_chunks}\")\n",
    "\n",
    "        # Process sessions in chunks with memory management\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            # Memory check with emergency handling\n",
    "            if chunk_idx % config.MEMORY_CHECK_INTERVAL == 0:\n",
    "                memory_status = check_memory()\n",
    "                if memory_status == \"CRITICAL\":\n",
    "                    log(f\"CRITICAL MEMORY - EMERGENCY CTB SAVE\")\n",
    "                    self.save_intermediate_results(force_save=True)\n",
    "                    break\n",
    "\n",
    "            # Get session chunk\n",
    "            start_idx = chunk_idx * self.chunk_size\n",
    "            end_idx = min(start_idx + self.chunk_size, total_sessions)\n",
    "            session_chunk = conversion_sessions[start_idx:end_idx]\n",
    "\n",
    "            log(f\"    Processing CTB chunk {chunk_idx + 1}/{num_chunks} ({end_idx/total_sessions*100:.1f}%)\")\n",
    "\n",
    "            try:\n",
    "                # Process chunk\n",
    "                chunk_covisitations = self.process_session_chunk(session_chunk, data)\n",
    "                self.merge_chunk_results(chunk_covisitations)\n",
    "                self.processed_chunks += 1\n",
    "\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                pairs_found = len(chunk_covisitations)\n",
    "                current_memory = self.estimate_memory_usage()\n",
    "\n",
    "                log(f\"       CTB chunk {chunk_idx + 1}: {pairs_found:,} pairs, {chunk_time:.1f}s, ~{current_memory:.0f}MB\")\n",
    "\n",
    "                # Memory management\n",
    "                if (current_memory > 3000 or  # 3GB limit\n",
    "                    self.processed_chunks % self.save_interval_chunks == 0):\n",
    "                    self.save_intermediate_results()\n",
    "\n",
    "                # Cleanup\n",
    "                del chunk_covisitations\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"       Error processing CTB chunk {chunk_idx + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Final processing - collect all intermediate results\n",
    "        log(\"    Collecting CTB intermediate results...\")\n",
    "        final_matrix = self._merge_intermediate_results()\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "        log(f\"  Click-to-buy matrix generation completed!\")\n",
    "        log(f\"     Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "        log(f\"     Final matrix size: {len(final_matrix):,} source items\")\n",
    "\n",
    "        return final_matrix\n",
    "\n",
    "    def _merge_intermediate_results(self) -> Dict:\n",
    "        \"\"\"Merge all intermediate result files into final matrix\"\"\"\n",
    "        log(\"    Merging CTB intermediate results...\")\n",
    "\n",
    "        final_matrix = {}\n",
    "        temp_files = []\n",
    "\n",
    "        # Find all temporary files\n",
    "        import glob\n",
    "        temp_pattern = f\"{config.OUTPUT_PATH}/ctb_matrix_temp_chunk_*.pkl\"\n",
    "        temp_files = glob.glob(temp_pattern)\n",
    "\n",
    "        log(f\"    Found {len(temp_files)} CTB intermediate files to merge\")\n",
    "\n",
    "        # Merge current memory state\n",
    "        if self.covisitation_counts:\n",
    "            log(\"    Adding current CTB memory state...\")\n",
    "            for source_aid, targets in self.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    final_matrix[source_aid] = top_targets\n",
    "\n",
    "        # Merge intermediate files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                log(f\"    Merging {os.path.basename(temp_file)}...\")\n",
    "                with open(temp_file, \"rb\") as f:\n",
    "                    temp_data = pickle.load(f)\n",
    "                    temp_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "\n",
    "                    # Update temporal stats\n",
    "                    if \"temporal_stats\" in temp_data:\n",
    "                        temp_stats = temp_data[\"temporal_stats\"]\n",
    "                        for key in self.temporal_stats:\n",
    "                            if key in temp_stats:\n",
    "                                self.temporal_stats[key] += temp_stats[key]\n",
    "\n",
    "                # Merge into final matrix\n",
    "                for source_aid, candidates in temp_matrix.items():\n",
    "                    if source_aid in final_matrix:\n",
    "                        # Merge candidates\n",
    "                        existing_dict = dict(final_matrix[source_aid])\n",
    "                        for target_aid, score in candidates:\n",
    "                            existing_dict[target_aid] = existing_dict.get(target_aid, 0) + score\n",
    "\n",
    "                        # Keep top candidates\n",
    "                        sorted_candidates = sorted(existing_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                        final_matrix[source_aid] = sorted_candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    else:\n",
    "                        final_matrix[source_aid] = candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "\n",
    "                # Clean up temp file\n",
    "                os.remove(temp_file)\n",
    "                log(f\"    Cleaned up {os.path.basename(temp_file)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"    Error processing CTB {temp_file}: {e}\")\n",
    "\n",
    "        log(f\"    Final CTB matrix merged: {len(final_matrix):,} source items\")\n",
    "        return final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbPwGM4fCUmN"
   },
   "source": [
    "## BUY-TO-BUY MATRIX GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UHgB1OytCWyW"
   },
   "outputs": [],
   "source": [
    "class BuyToBuyMatrixGenerator:\n",
    "    \"\"\"\n",
    "    Memory-efficient buy-to-buy co-visitation matrix generator with progressive saving\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int, time_window_days: int):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.time_window_ms = time_window_days * 24 * 60 * 60 * 1000\n",
    "        self.covisitation_counts = {}\n",
    "\n",
    "        # Memory management\n",
    "        self.processed_chunks = 0\n",
    "        self.start_time = time.time()\n",
    "        self.last_save_time = time.time()\n",
    "        self.max_pairs_in_memory = 2000000  # 2M pairs before cleanup\n",
    "        self.save_interval_chunks = 15      # Save every 15 chunks\n",
    "\n",
    "        log(\"  Initializing memory-efficient buy-to-buy matrix generator...\")\n",
    "        log(f\"   Chunk size: {self.chunk_size:,} sessions\")\n",
    "        log(f\"   Time window: {time_window_days} days\")\n",
    "        log(f\"   Max pairs in memory: {self.max_pairs_in_memory:,}\")\n",
    "        log(f\"   Save interval: {self.save_interval_chunks} chunks\")\n",
    "\n",
    "    def estimate_memory_usage(self) -> float:\n",
    "        \"\"\"Estimate current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            total_pairs = sum(len(targets) for targets in self.covisitation_counts.values())\n",
    "            estimated_mb = total_pairs * 50 / (1024 * 1024)\n",
    "            return estimated_mb\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def cleanup_memory(self, force_aggressive: bool = False):\n",
    "        \"\"\"Clean up memory by keeping only top candidates\"\"\"\n",
    "        log(f\"   Performing buy-to-buy memory cleanup (aggressive={force_aggressive})...\")\n",
    "\n",
    "        before_memory = self.estimate_memory_usage()\n",
    "        max_candidates = config.MAX_CANDIDATES_PER_ITEM if not force_aggressive else 20\n",
    "\n",
    "        cleaned_counts = {}\n",
    "        for source_aid, targets in self.covisitation_counts.items():\n",
    "            if len(targets) > max_candidates:\n",
    "                sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                top_targets = dict(sorted_targets[:max_candidates])\n",
    "                cleaned_counts[source_aid] = top_targets\n",
    "            else:\n",
    "                cleaned_counts[source_aid] = targets\n",
    "\n",
    "        self.covisitation_counts = cleaned_counts\n",
    "        gc.collect()\n",
    "\n",
    "        after_memory = self.estimate_memory_usage()\n",
    "        log(f\"   BTB memory cleanup: {before_memory:.1f}MB → {after_memory:.1f}MB\")\n",
    "\n",
    "    def save_intermediate_results(self, force_save: bool = False):\n",
    "        \"\"\"Save intermediate results and clean up memory\"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        if force_save or (self.processed_chunks % self.save_interval_chunks == 0 and self.processed_chunks > 0):\n",
    "            log(f\"   Saving BTB intermediate results (chunk {self.processed_chunks})...\")\n",
    "\n",
    "            try:\n",
    "                temp_path = f\"{config.OUTPUT_PATH}/btb_matrix_temp_chunk_{self.processed_chunks}.pkl\"\n",
    "\n",
    "                # Convert to final format before saving\n",
    "                temp_matrix = {}\n",
    "                for source_aid, targets in self.covisitation_counts.items():\n",
    "                    if targets:\n",
    "                        sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                        top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                        temp_matrix[source_aid] = top_targets\n",
    "\n",
    "                with open(temp_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"partial_matrix\": temp_matrix,\n",
    "                        \"processed_chunks\": self.processed_chunks,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }, f)\n",
    "\n",
    "                file_size = os.path.getsize(temp_path) / (1024*1024)\n",
    "                log(f\"   BTB intermediate results saved: {temp_path} ({file_size:.1f} MB)\")\n",
    "                self.last_save_time = current_time\n",
    "\n",
    "                # Clear memory after saving\n",
    "                self.covisitation_counts.clear()\n",
    "                gc.collect()\n",
    "                log(f\"   BTB memory cleared after save\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"   Failed to save BTB intermediate results: {e}\")\n",
    "\n",
    "    def process_session_chunk(self, session_chunk: List[int], buy_data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Process a chunk of sessions for buy-to-buy relationships with memory limits\"\"\"\n",
    "        chunk_covisitations = {}\n",
    "\n",
    "        # Filter data for this chunk\n",
    "        chunk_data = buy_data.filter(pl.col(\"session\").is_in(session_chunk))\n",
    "\n",
    "        if len(chunk_data) == 0:\n",
    "            return chunk_covisitations\n",
    "\n",
    "        # Process each session with limits\n",
    "        session_groups = chunk_data.group_by(\"session\").agg([\n",
    "            pl.col(\"aid\").alias(\"aids\"),\n",
    "            pl.col(\"ts\").alias(\"timestamps\")\n",
    "        ])\n",
    "\n",
    "        pairs_in_chunk = 0\n",
    "        max_pairs_per_chunk = 500000  # Limit pairs per chunk\n",
    "\n",
    "        for row in session_groups.iter_rows():\n",
    "            session_id, aids, timestamps = row\n",
    "\n",
    "            if len(aids) < 2:  # Need at least 2 purchase events\n",
    "                continue\n",
    "\n",
    "            # Limit session size to prevent memory explosion\n",
    "            if len(aids) > 30:  # Skip very long purchase sessions\n",
    "                continue\n",
    "\n",
    "            # Create aid-timestamp pairs\n",
    "            events = list(zip(aids, timestamps))\n",
    "            events.sort(key=lambda x: x[1])  # Sort by timestamp\n",
    "\n",
    "            # Generate buy-to-buy pairs within time window with limits\n",
    "            for i in range(len(events)):\n",
    "                for j in range(i + 1, len(events)):\n",
    "                    if pairs_in_chunk >= max_pairs_per_chunk:\n",
    "                        break\n",
    "\n",
    "                    aid1, ts1 = events[i]\n",
    "                    aid2, ts2 = events[j]\n",
    "\n",
    "                    # Check time window\n",
    "                    if (ts2 - ts1) <= self.time_window_ms and aid1 != aid2:\n",
    "                        # Add both directions\n",
    "                        chunk_covisitations[(aid1, aid2)] = chunk_covisitations.get((aid1, aid2), 0) + 1\n",
    "                        chunk_covisitations[(aid2, aid1)] = chunk_covisitations.get((aid2, aid1), 0) + 1\n",
    "                        pairs_in_chunk += 2\n",
    "\n",
    "                if pairs_in_chunk >= max_pairs_per_chunk:\n",
    "                    break\n",
    "\n",
    "        return chunk_covisitations\n",
    "\n",
    "    def merge_chunk_results(self, chunk_covisitations: Dict):\n",
    "        \"\"\"Merge chunk results with memory management\"\"\"\n",
    "        for (aid1, aid2), count in chunk_covisitations.items():\n",
    "            if aid1 not in self.covisitation_counts:\n",
    "                self.covisitation_counts[aid1] = {}\n",
    "            self.covisitation_counts[aid1][aid2] = self.covisitation_counts[aid1].get(aid2, 0) + count\n",
    "\n",
    "    def generate_matrix(self, data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Generate complete buy-to-buy co-visitation matrix with memory management\"\"\"\n",
    "        log(\"  Starting memory-efficient buy-to-buy matrix generation...\")\n",
    "\n",
    "        # Filter for purchase events (carts and orders)\n",
    "        buy_data = data.filter(pl.col(\"type\").is_in([\"carts\", \"orders\"])).sort([\"session\", \"ts\"])\n",
    "\n",
    "        # Get sessions with multiple purchase events\n",
    "        purchase_session_counts = buy_data.group_by(\"session\").agg([\n",
    "            pl.count().alias(\"purchase_count\")\n",
    "        ]).filter(pl.col(\"purchase_count\") > 1)\n",
    "\n",
    "        multi_purchase_sessions = purchase_session_counts[\"session\"].to_list()\n",
    "        total_sessions = len(multi_purchase_sessions)\n",
    "\n",
    "        log(f\"    Processing {total_sessions:,} sessions with multiple purchases\")\n",
    "        log(f\"    Purchase events: {len(buy_data):,}\")\n",
    "\n",
    "        num_chunks = (total_sessions + self.chunk_size - 1) // self.chunk_size\n",
    "        log(f\"    Total chunks to process: {num_chunks}\")\n",
    "\n",
    "        # Process sessions in chunks with memory management\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            # Memory check with emergency handling\n",
    "            if chunk_idx % config.MEMORY_CHECK_INTERVAL == 0:\n",
    "                memory_status = check_memory()\n",
    "                if memory_status == \"CRITICAL\":\n",
    "                    log(f\"CRITICAL MEMORY - EMERGENCY BTB SAVE\")\n",
    "                    self.save_intermediate_results(force_save=True)\n",
    "                    break\n",
    "\n",
    "            # Get session chunk\n",
    "            start_idx = chunk_idx * self.chunk_size\n",
    "            end_idx = min(start_idx + self.chunk_size, total_sessions)\n",
    "            session_chunk = multi_purchase_sessions[start_idx:end_idx]\n",
    "\n",
    "            log(f\"    Processing BTB chunk {chunk_idx + 1}/{num_chunks} ({end_idx/total_sessions*100:.1f}%)\")\n",
    "\n",
    "            try:\n",
    "                # Process chunk\n",
    "                chunk_covisitations = self.process_session_chunk(session_chunk, buy_data)\n",
    "                self.merge_chunk_results(chunk_covisitations)\n",
    "                self.processed_chunks += 1\n",
    "\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                pairs_found = len(chunk_covisitations)\n",
    "                current_memory = self.estimate_memory_usage()\n",
    "\n",
    "                log(f\"       BTB chunk {chunk_idx + 1}: {pairs_found:,} pairs, {chunk_time:.1f}s, ~{current_memory:.0f}MB\")\n",
    "\n",
    "                # Memory management\n",
    "                if (current_memory > 3000 or  # 3GB limit\n",
    "                    self.processed_chunks % self.save_interval_chunks == 0):\n",
    "                    self.save_intermediate_results()\n",
    "\n",
    "                # Cleanup\n",
    "                del chunk_covisitations\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"       Error processing BTB chunk {chunk_idx + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Final processing - collect all intermediate results\n",
    "        log(\"    Collecting BTB intermediate results...\")\n",
    "        final_matrix = self._merge_intermediate_results()\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "        log(f\"  Buy-to-buy matrix generation completed!\")\n",
    "        log(f\"     Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "        log(f\"     Final matrix size: {len(final_matrix):,} source items\")\n",
    "\n",
    "        return final_matrix\n",
    "\n",
    "    def _merge_intermediate_results(self) -> Dict:\n",
    "        \"\"\"Merge all intermediate result files into final matrix\"\"\"\n",
    "        log(\"    Merging BTB intermediate results...\")\n",
    "\n",
    "        final_matrix = {}\n",
    "        temp_files = []\n",
    "\n",
    "        # Find all temporary files\n",
    "        import glob\n",
    "        temp_pattern = f\"{config.OUTPUT_PATH}/btb_matrix_temp_chunk_*.pkl\"\n",
    "        temp_files = glob.glob(temp_pattern)\n",
    "\n",
    "        log(f\"    Found {len(temp_files)} BTB intermediate files to merge\")\n",
    "\n",
    "        # Merge current memory state\n",
    "        if self.covisitation_counts:\n",
    "            log(\"    Adding current BTB memory state...\")\n",
    "            for source_aid, targets in self.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    final_matrix[source_aid] = top_targets\n",
    "\n",
    "        # Merge intermediate files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                log(f\"    Merging {os.path.basename(temp_file)}...\")\n",
    "                with open(temp_file, \"rb\") as f:\n",
    "                    temp_data = pickle.load(f)\n",
    "                    temp_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "\n",
    "                # Merge into final matrix\n",
    "                for source_aid, candidates in temp_matrix.items():\n",
    "                    if source_aid in final_matrix:\n",
    "                        # Merge candidates\n",
    "                        existing_dict = dict(final_matrix[source_aid])\n",
    "                        for target_aid, score in candidates:\n",
    "                            existing_dict[target_aid] = existing_dict.get(target_aid, 0) + score\n",
    "\n",
    "                        # Keep top candidates\n",
    "                        sorted_candidates = sorted(existing_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                        final_matrix[source_aid] = sorted_candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    else:\n",
    "                        final_matrix[source_aid] = candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "\n",
    "                # Clean up temp file\n",
    "                os.remove(temp_file)\n",
    "                log(f\"    Cleaned up {os.path.basename(temp_file)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"    Error processing BTB {temp_file}: {e}\")\n",
    "\n",
    "        log(f\"    Final BTB matrix merged: {len(final_matrix):,} source items\")\n",
    "        return final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlYm_QAaCamX"
   },
   "source": [
    "## MATRIX GENERATION EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an0Oq-ytCcgX",
    "outputId": "4339523e-f924-4ad2-f99d-60fe80a603d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:42:17] \n",
      " Preparing data for matrix generation...\n",
      "[2025-08-07 18:42:17] Initial memory status: LOW\n",
      "[2025-08-07 18:42:17] \n",
      " Initializing memory-efficient matrix generators...\n",
      "[2025-08-07 18:42:17]   Initializing memory-efficient click-to-buy matrix generator...\n",
      "[2025-08-07 18:42:17]    Chunk size: 100,000 sessions\n",
      "[2025-08-07 18:42:17]    Time window: 14 days (1,209,600,000 ms)\n",
      "[2025-08-07 18:42:17]    Max pairs in memory: 2,000,000\n",
      "[2025-08-07 18:42:17]    Save interval: 15 chunks\n",
      "[2025-08-07 18:42:17]   Initializing memory-efficient buy-to-buy matrix generator...\n",
      "[2025-08-07 18:42:17]    Chunk size: 150,000 sessions\n",
      "[2025-08-07 18:42:17]    Time window: 7 days\n",
      "[2025-08-07 18:42:17]    Max pairs in memory: 2,000,000\n",
      "[2025-08-07 18:42:17]    Save interval: 15 chunks\n",
      "[2025-08-07 18:42:17] \n",
      "============================================================\n",
      "[2025-08-07 18:42:17]  GENERATING CLICK-TO-BUY MATRIX\n",
      "[2025-08-07 18:42:17] ============================================================\n",
      "[2025-08-07 18:42:17] Pre-CTB generation memory: LOW\n",
      "[2025-08-07 18:42:17]   Starting memory-efficient click-to-buy matrix generation...\n",
      "[2025-08-07 18:42:23]     Processing 3,846,669 sessions with conversion opportunities\n",
      "[2025-08-07 18:42:23]     Using 100,000 sessions per chunk\n",
      "[2025-08-07 18:42:23]     Total chunks to process: 39\n",
      "[2025-08-07 18:42:23]     Processing CTB chunk 1/39 (2.6%)\n",
      "[2025-08-07 18:42:29]        CTB chunk 1: 280,444 pairs, 5.8s, ~13MB\n",
      "[2025-08-07 18:42:29]     Processing CTB chunk 2/39 (5.2%)\n",
      "[2025-08-07 18:42:34]        CTB chunk 2: 280,051 pairs, 5.0s, ~27MB\n",
      "[2025-08-07 18:42:34]     Processing CTB chunk 3/39 (7.8%)\n",
      "[2025-08-07 18:42:39]        CTB chunk 3: 280,989 pairs, 5.0s, ~40MB\n",
      "[2025-08-07 18:42:40]     Processing CTB chunk 4/39 (10.4%)\n",
      "[2025-08-07 18:42:45]        CTB chunk 4: 283,624 pairs, 5.5s, ~53MB\n",
      "[2025-08-07 18:42:45]     Processing CTB chunk 5/39 (13.0%)\n",
      "[2025-08-07 18:42:50]        CTB chunk 5: 283,823 pairs, 5.3s, ~66MB\n",
      "[2025-08-07 18:42:51]     Processing CTB chunk 6/39 (15.6%)\n",
      "[2025-08-07 18:42:56]        CTB chunk 6: 283,083 pairs, 5.7s, ~79MB\n",
      "[2025-08-07 18:42:57]     Processing CTB chunk 7/39 (18.2%)\n",
      "[2025-08-07 18:43:02]        CTB chunk 7: 282,976 pairs, 5.3s, ~92MB\n",
      "[2025-08-07 18:43:02]     Processing CTB chunk 8/39 (20.8%)\n",
      "[2025-08-07 18:43:07]        CTB chunk 8: 280,776 pairs, 5.1s, ~104MB\n",
      "[2025-08-07 18:43:07]     Processing CTB chunk 9/39 (23.4%)\n",
      "[2025-08-07 18:43:13]        CTB chunk 9: 281,942 pairs, 5.3s, ~117MB\n",
      "[2025-08-07 18:43:13]     Processing CTB chunk 10/39 (26.0%)\n",
      "[2025-08-07 18:43:19]        CTB chunk 10: 280,064 pairs, 5.7s, ~130MB\n",
      "[2025-08-07 18:43:19]     Processing CTB chunk 11/39 (28.6%)\n",
      "[2025-08-07 18:43:25]        CTB chunk 11: 283,659 pairs, 5.9s, ~142MB\n",
      "[2025-08-07 18:43:25]     Processing CTB chunk 12/39 (31.2%)\n",
      "[2025-08-07 18:43:30]        CTB chunk 12: 281,158 pairs, 5.4s, ~155MB\n",
      "[2025-08-07 18:43:30]     Processing CTB chunk 13/39 (33.8%)\n",
      "[2025-08-07 18:43:36]        CTB chunk 13: 282,968 pairs, 5.6s, ~168MB\n",
      "[2025-08-07 18:43:36]     Processing CTB chunk 14/39 (36.4%)\n",
      "[2025-08-07 18:43:42]        CTB chunk 14: 281,648 pairs, 5.7s, ~180MB\n",
      "[2025-08-07 18:43:42]     Processing CTB chunk 15/39 (39.0%)\n",
      "[2025-08-07 18:43:48]        CTB chunk 15: 284,998 pairs, 6.0s, ~193MB\n",
      "[2025-08-07 18:43:48]    Saving CTB intermediate results (chunk 15)...\n",
      "[2025-08-07 18:43:54]    CTB intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/ctb_matrix_temp_chunk_15.pkl (32.0 MB)\n",
      "[2025-08-07 18:43:55]    CTB memory cleared after save\n",
      "[2025-08-07 18:43:56]     Processing CTB chunk 16/39 (41.6%)\n",
      "[2025-08-07 18:44:01]        CTB chunk 16: 281,018 pairs, 5.6s, ~13MB\n",
      "[2025-08-07 18:44:01]     Processing CTB chunk 17/39 (44.2%)\n",
      "[2025-08-07 18:44:07]        CTB chunk 17: 279,612 pairs, 5.6s, ~27MB\n",
      "[2025-08-07 18:44:07]     Processing CTB chunk 18/39 (46.8%)\n",
      "[2025-08-07 18:44:13]        CTB chunk 18: 281,947 pairs, 5.6s, ~40MB\n",
      "[2025-08-07 18:44:13]     Processing CTB chunk 19/39 (49.4%)\n",
      "[2025-08-07 18:44:18]        CTB chunk 19: 281,474 pairs, 5.5s, ~53MB\n",
      "[2025-08-07 18:44:18]     Processing CTB chunk 20/39 (52.0%)\n",
      "[2025-08-07 18:44:23]        CTB chunk 20: 280,730 pairs, 4.9s, ~66MB\n",
      "[2025-08-07 18:44:24]     Processing CTB chunk 21/39 (54.6%)\n",
      "[2025-08-07 18:44:29]        CTB chunk 21: 280,112 pairs, 5.2s, ~79MB\n",
      "[2025-08-07 18:44:29]     Processing CTB chunk 22/39 (57.2%)\n",
      "[2025-08-07 18:44:34]        CTB chunk 22: 281,487 pairs, 5.4s, ~91MB\n",
      "[2025-08-07 18:44:34]     Processing CTB chunk 23/39 (59.8%)\n",
      "[2025-08-07 18:44:40]        CTB chunk 23: 282,446 pairs, 5.1s, ~104MB\n",
      "[2025-08-07 18:44:40]     Processing CTB chunk 24/39 (62.4%)\n",
      "[2025-08-07 18:44:46]        CTB chunk 24: 282,303 pairs, 5.8s, ~117MB\n",
      "[2025-08-07 18:44:46]     Processing CTB chunk 25/39 (65.0%)\n",
      "[2025-08-07 18:44:51]        CTB chunk 25: 280,785 pairs, 5.3s, ~129MB\n",
      "[2025-08-07 18:44:51]     Processing CTB chunk 26/39 (67.6%)\n",
      "[2025-08-07 18:44:57]        CTB chunk 26: 281,722 pairs, 5.9s, ~142MB\n",
      "[2025-08-07 18:44:57]     Processing CTB chunk 27/39 (70.2%)\n",
      "[2025-08-07 18:45:03]        CTB chunk 27: 281,658 pairs, 5.9s, ~155MB\n",
      "[2025-08-07 18:45:03]     Processing CTB chunk 28/39 (72.8%)\n",
      "[2025-08-07 18:45:10]        CTB chunk 28: 281,671 pairs, 6.1s, ~167MB\n",
      "[2025-08-07 18:45:10]     Processing CTB chunk 29/39 (75.4%)\n",
      "[2025-08-07 18:45:15]        CTB chunk 29: 282,536 pairs, 5.4s, ~180MB\n",
      "[2025-08-07 18:45:15]     Processing CTB chunk 30/39 (78.0%)\n",
      "[2025-08-07 18:45:21]        CTB chunk 30: 283,275 pairs, 6.1s, ~192MB\n",
      "[2025-08-07 18:45:21]    Saving CTB intermediate results (chunk 30)...\n",
      "[2025-08-07 18:45:28]    CTB intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/ctb_matrix_temp_chunk_30.pkl (31.9 MB)\n",
      "[2025-08-07 18:45:29]    CTB memory cleared after save\n",
      "[2025-08-07 18:45:29]     Processing CTB chunk 31/39 (80.6%)\n",
      "[2025-08-07 18:45:35]        CTB chunk 31: 281,149 pairs, 5.4s, ~13MB\n",
      "[2025-08-07 18:45:35]     Processing CTB chunk 32/39 (83.2%)\n",
      "[2025-08-07 18:45:40]        CTB chunk 32: 283,107 pairs, 5.2s, ~27MB\n",
      "[2025-08-07 18:45:40]     Processing CTB chunk 33/39 (85.8%)\n",
      "[2025-08-07 18:45:46]        CTB chunk 33: 282,173 pairs, 5.7s, ~40MB\n",
      "[2025-08-07 18:45:46]     Processing CTB chunk 34/39 (88.4%)\n",
      "[2025-08-07 18:45:52]        CTB chunk 34: 281,734 pairs, 5.6s, ~53MB\n",
      "[2025-08-07 18:45:52]     Processing CTB chunk 35/39 (91.0%)\n",
      "[2025-08-07 18:45:57]        CTB chunk 35: 282,874 pairs, 5.5s, ~66MB\n",
      "[2025-08-07 18:45:57]     Processing CTB chunk 36/39 (93.6%)\n",
      "[2025-08-07 18:46:03]        CTB chunk 36: 280,010 pairs, 5.4s, ~79MB\n",
      "[2025-08-07 18:46:03]     Processing CTB chunk 37/39 (96.2%)\n",
      "[2025-08-07 18:46:08]        CTB chunk 37: 283,393 pairs, 4.9s, ~92MB\n",
      "[2025-08-07 18:46:08]     Processing CTB chunk 38/39 (98.8%)\n",
      "[2025-08-07 18:46:14]        CTB chunk 38: 283,529 pairs, 5.6s, ~105MB\n",
      "[2025-08-07 18:46:14]     Processing CTB chunk 39/39 (100.0%)\n",
      "[2025-08-07 18:46:19]        CTB chunk 39: 281,248 pairs, 5.0s, ~117MB\n",
      "[2025-08-07 18:46:19]     Collecting CTB intermediate results...\n",
      "[2025-08-07 18:46:19]     Merging CTB intermediate results...\n",
      "[2025-08-07 18:46:19]     Found 2 CTB intermediate files to merge\n",
      "[2025-08-07 18:46:19]     Adding current CTB memory state...\n",
      "[2025-08-07 18:46:22]     Merging ctb_matrix_temp_chunk_15.pkl...\n",
      "[2025-08-07 18:46:30]     Cleaned up ctb_matrix_temp_chunk_15.pkl\n",
      "[2025-08-07 18:46:30]     Merging ctb_matrix_temp_chunk_30.pkl...\n",
      "[2025-08-07 18:46:40]     Cleaned up ctb_matrix_temp_chunk_30.pkl\n",
      "[2025-08-07 18:46:40]     Final CTB matrix merged: 841,226 source items\n",
      "[2025-08-07 18:46:40]   Click-to-buy matrix generation completed!\n",
      "[2025-08-07 18:46:40]      Total time: 263.3 seconds (4.4 minutes)\n",
      "[2025-08-07 18:46:40]      Final matrix size: 841,226 source items\n",
      "[2025-08-07 18:46:40]  Click-to-buy matrix generated successfully!\n",
      "[2025-08-07 18:46:40]    Source items: 841,226\n",
      "[2025-08-07 18:46:40]    Generation time: 263.3 seconds\n",
      "[2025-08-07 18:46:42]    CTB generator memory cleaned up\n",
      "[2025-08-07 18:46:42] Post-CTB generation memory: LOW\n",
      "[2025-08-07 18:46:42] \n",
      "============================================================\n",
      "[2025-08-07 18:46:42]  GENERATING BUY-TO-BUY MATRIX\n",
      "[2025-08-07 18:46:42] ============================================================\n",
      "[2025-08-07 18:46:42] Pre-BTB generation memory: LOW\n",
      "[2025-08-07 18:46:42]   Starting memory-efficient buy-to-buy matrix generation...\n",
      "[2025-08-07 18:46:45]     Processing 2,814,925 sessions with multiple purchases\n",
      "[2025-08-07 18:46:45]     Purchase events: 21,759,883\n",
      "[2025-08-07 18:46:45]     Total chunks to process: 19\n",
      "[2025-08-07 18:46:45]     Processing BTB chunk 1/19 (5.3%)\n",
      "[2025-08-07 18:46:46]        BTB chunk 1: 257,440 pairs, 1.0s, ~12MB\n",
      "[2025-08-07 18:46:48]     Processing BTB chunk 2/19 (10.7%)\n",
      "[2025-08-07 18:46:49]        BTB chunk 2: 235,800 pairs, 0.9s, ~23MB\n",
      "[2025-08-07 18:46:50]     Processing BTB chunk 3/19 (16.0%)\n",
      "[2025-08-07 18:46:51]        BTB chunk 3: 248,154 pairs, 0.9s, ~35MB\n",
      "[2025-08-07 18:46:53]     Processing BTB chunk 4/19 (21.3%)\n",
      "[2025-08-07 18:46:53]        BTB chunk 4: 233,220 pairs, 0.8s, ~46MB\n",
      "[2025-08-07 18:46:55]     Processing BTB chunk 5/19 (26.6%)\n",
      "[2025-08-07 18:46:56]        BTB chunk 5: 240,030 pairs, 0.9s, ~57MB\n",
      "[2025-08-07 18:46:57]     Processing BTB chunk 6/19 (32.0%)\n",
      "[2025-08-07 18:46:58]        BTB chunk 6: 238,572 pairs, 0.9s, ~68MB\n",
      "[2025-08-07 18:46:59]     Processing BTB chunk 7/19 (37.3%)\n",
      "[2025-08-07 18:47:00]        BTB chunk 7: 246,960 pairs, 0.9s, ~80MB\n",
      "[2025-08-07 18:47:02]     Processing BTB chunk 8/19 (42.6%)\n",
      "[2025-08-07 18:47:03]        BTB chunk 8: 258,012 pairs, 1.0s, ~92MB\n",
      "[2025-08-07 18:47:04]     Processing BTB chunk 9/19 (48.0%)\n",
      "[2025-08-07 18:47:05]        BTB chunk 9: 252,650 pairs, 1.0s, ~103MB\n",
      "[2025-08-07 18:47:07]     Processing BTB chunk 10/19 (53.3%)\n",
      "[2025-08-07 18:47:08]        BTB chunk 10: 234,206 pairs, 0.9s, ~114MB\n",
      "[2025-08-07 18:47:10]     Processing BTB chunk 11/19 (58.6%)\n",
      "[2025-08-07 18:47:11]        BTB chunk 11: 238,920 pairs, 1.0s, ~125MB\n",
      "[2025-08-07 18:47:13]     Processing BTB chunk 12/19 (63.9%)\n",
      "[2025-08-07 18:47:14]        BTB chunk 12: 234,130 pairs, 1.0s, ~136MB\n",
      "[2025-08-07 18:47:16]     Processing BTB chunk 13/19 (69.3%)\n",
      "[2025-08-07 18:47:17]        BTB chunk 13: 250,144 pairs, 1.0s, ~147MB\n",
      "[2025-08-07 18:47:19]     Processing BTB chunk 14/19 (74.6%)\n",
      "[2025-08-07 18:47:19]        BTB chunk 14: 258,192 pairs, 0.9s, ~159MB\n",
      "[2025-08-07 18:47:21]     Processing BTB chunk 15/19 (79.9%)\n",
      "[2025-08-07 18:47:22]        BTB chunk 15: 239,402 pairs, 0.9s, ~170MB\n",
      "[2025-08-07 18:47:22]    Saving BTB intermediate results (chunk 15)...\n",
      "[2025-08-07 18:47:26]    BTB intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/btb_matrix_temp_chunk_15.pkl (26.8 MB)\n",
      "[2025-08-07 18:47:29]    BTB memory cleared after save\n",
      "[2025-08-07 18:47:31]     Processing BTB chunk 16/19 (85.3%)\n",
      "[2025-08-07 18:47:32]        BTB chunk 16: 236,904 pairs, 0.8s, ~11MB\n",
      "[2025-08-07 18:47:34]     Processing BTB chunk 17/19 (90.6%)\n",
      "[2025-08-07 18:47:34]        BTB chunk 17: 226,526 pairs, 0.9s, ~22MB\n",
      "[2025-08-07 18:47:37]     Processing BTB chunk 18/19 (95.9%)\n",
      "[2025-08-07 18:47:37]        BTB chunk 18: 248,126 pairs, 0.9s, ~34MB\n",
      "[2025-08-07 18:47:39]     Processing BTB chunk 19/19 (100.0%)\n",
      "[2025-08-07 18:47:40]        BTB chunk 19: 234,600 pairs, 0.9s, ~45MB\n",
      "[2025-08-07 18:47:42]     Collecting BTB intermediate results...\n",
      "[2025-08-07 18:47:42]     Merging BTB intermediate results...\n",
      "[2025-08-07 18:47:42]     Found 1 BTB intermediate files to merge\n",
      "[2025-08-07 18:47:42]     Adding current BTB memory state...\n",
      "[2025-08-07 18:47:43]     Merging btb_matrix_temp_chunk_15.pkl...\n",
      "[2025-08-07 18:47:49]     Cleaned up btb_matrix_temp_chunk_15.pkl\n",
      "[2025-08-07 18:47:49]     Final BTB matrix merged: 311,156 source items\n",
      "[2025-08-07 18:47:49]   Buy-to-buy matrix generation completed!\n",
      "[2025-08-07 18:47:49]      Total time: 332.1 seconds (5.5 minutes)\n",
      "[2025-08-07 18:47:49]      Final matrix size: 311,156 source items\n",
      "[2025-08-07 18:47:49]  Buy-to-buy matrix generated successfully!\n",
      "[2025-08-07 18:47:49]    Source items: 311,156\n",
      "[2025-08-07 18:47:49]    Generation time: 66.8 seconds\n",
      "[2025-08-07 18:47:50]    BTB generator memory cleaned up\n",
      "[2025-08-07 18:47:50] Post-BTB generation memory: LOW\n",
      "[2025-08-07 18:47:50] \n",
      " MATRIX GENERATION SUMMARY:\n",
      "[2025-08-07 18:47:50]    Click-to-buy success: True\n",
      "[2025-08-07 18:47:50]    Buy-to-buy success: True\n",
      "[2025-08-07 18:47:51]    CTB matrix size: 841,226 source items\n",
      "[2025-08-07 18:47:51]    BTB matrix size: 311,156 source items\n",
      "[2025-08-07 18:47:51]    Total generation time: 330.1 seconds\n",
      "[2025-08-07 18:47:51]  Both matrices generated successfully!\n",
      "[2025-08-07 18:47:51]  Keeping prepared_data in memory for conversion analysis...\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for matrix generation with memory optimization\n",
    "log(\"\\n Preparing data for matrix generation...\")\n",
    "\n",
    "# Memory check before starting\n",
    "initial_memory = check_memory()\n",
    "log(f\"Initial memory status: {initial_memory}\")\n",
    "\n",
    "# Clear any unnecessary data early (but keep prepared_data for now)\n",
    "if 'chunking_strategy' in locals():\n",
    "    del chunking_strategy\n",
    "if 'session_analysis' in locals():\n",
    "    del session_analysis\n",
    "gc.collect()\n",
    "\n",
    "# Initialize generators with enhanced memory management\n",
    "click_to_buy_chunk_size = validation_results[\"chunk_sizes\"][\"click_to_buy\"]\n",
    "buy_to_buy_chunk_size = validation_results[\"chunk_sizes\"][\"buy_to_buy\"]\n",
    "\n",
    "log(\"\\n Initializing memory-efficient matrix generators...\")\n",
    "\n",
    "click_to_buy_generator = ClickToBuyMatrixGenerator(\n",
    "    click_to_buy_chunk_size,\n",
    "    config.CLICK_TO_BUY_TIME_WINDOW_DAYS\n",
    ")\n",
    "\n",
    "buy_to_buy_generator = BuyToBuyMatrixGenerator(\n",
    "    buy_to_buy_chunk_size,\n",
    "    config.BUY_TO_BUY_TIME_WINDOW_DAYS\n",
    ")\n",
    "\n",
    "# Initialize success flags\n",
    "click_to_buy_success = False\n",
    "buy_to_buy_success = False\n",
    "click_to_buy_matrix = {}\n",
    "buy_to_buy_matrix = {}\n",
    "ctb_generation_time = 0\n",
    "btb_generation_time = 0\n",
    "\n",
    "# Generate click-to-buy matrix with comprehensive error handling\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\" GENERATING CLICK-TO-BUY MATRIX\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    ctb_start_time = time.time()\n",
    "\n",
    "    # Pre-generation memory check\n",
    "    pre_ctb_memory = check_memory()\n",
    "    log(f\"Pre-CTB generation memory: {pre_ctb_memory}\")\n",
    "\n",
    "    click_to_buy_matrix = click_to_buy_generator.generate_matrix(prepared_data)\n",
    "\n",
    "    ctb_generation_time = time.time() - ctb_start_time\n",
    "\n",
    "    if click_to_buy_matrix and len(click_to_buy_matrix) > 0:\n",
    "        click_to_buy_success = True\n",
    "        log(f\" Click-to-buy matrix generated successfully!\")\n",
    "        log(f\"   Source items: {len(click_to_buy_matrix):,}\")\n",
    "        log(f\"   Generation time: {ctb_generation_time:.1f} seconds\")\n",
    "    else:\n",
    "        log(\"  Click-to-buy matrix generation completed but resulted in empty matrix\")\n",
    "        click_to_buy_success = False\n",
    "\n",
    "except MemoryError as e:\n",
    "    log(f\" MEMORY ERROR during click-to-buy generation: {e}\")\n",
    "    log(\"   Attempting to recover partial results...\")\n",
    "    click_to_buy_success = False\n",
    "\n",
    "    # Try to get partial results from intermediate saves\n",
    "    try:\n",
    "        import glob\n",
    "        temp_files = glob.glob(f\"{config.OUTPUT_PATH}/ctb_matrix_temp_chunk_*.pkl\")\n",
    "        if temp_files:\n",
    "            log(f\"   Found {len(temp_files)} CTB intermediate files - attempting recovery...\")\n",
    "            latest_file = max(temp_files, key=os.path.getctime)\n",
    "            with open(latest_file, \"rb\") as f:\n",
    "                temp_data = pickle.load(f)\n",
    "                click_to_buy_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "            log(f\"   Recovered partial CTB matrix: {len(click_to_buy_matrix):,} items\")\n",
    "        else:\n",
    "            click_to_buy_matrix = {}\n",
    "    except Exception as recovery_error:\n",
    "        log(f\"   CTB recovery failed: {recovery_error}\")\n",
    "        click_to_buy_matrix = {}\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\" Unexpected error during click-to-buy generation: {e}\")\n",
    "    click_to_buy_success = False\n",
    "\n",
    "    # Try to save current state\n",
    "    try:\n",
    "        if hasattr(click_to_buy_generator, 'covisitation_counts') and click_to_buy_generator.covisitation_counts:\n",
    "            click_to_buy_generator.save_intermediate_results(force_save=True)\n",
    "            # Convert current state to matrix format\n",
    "            current_matrix = {}\n",
    "            for source_aid, targets in click_to_buy_generator.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    current_matrix[source_aid] = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "            click_to_buy_matrix = current_matrix\n",
    "            log(f\"   Saved partial CTB results: {len(click_to_buy_matrix):,} items\")\n",
    "        else:\n",
    "            click_to_buy_matrix = {}\n",
    "    except Exception as save_error:\n",
    "        log(f\"   Failed to save partial CTB results: {save_error}\")\n",
    "        click_to_buy_matrix = {}\n",
    "\n",
    "finally:\n",
    "    # Clean up click-to-buy generator memory\n",
    "    try:\n",
    "        if hasattr(click_to_buy_generator, 'covisitation_counts'):\n",
    "            click_to_buy_generator.covisitation_counts.clear()\n",
    "        gc.collect()\n",
    "        log(\"   CTB generator memory cleaned up\")\n",
    "    except Exception as cleanup_error:\n",
    "        log(f\"   CTB cleanup error: {cleanup_error}\")\n",
    "\n",
    "# Post-CTB memory check\n",
    "post_ctb_memory = check_memory()\n",
    "log(f\"Post-CTB generation memory: {post_ctb_memory}\")\n",
    "\n",
    "# Generate buy-to-buy matrix with comprehensive error handling\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\" GENERATING BUY-TO-BUY MATRIX\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    btb_start_time = time.time()\n",
    "\n",
    "    # Pre-generation memory check\n",
    "    pre_btb_memory = check_memory()\n",
    "    log(f\"Pre-BTB generation memory: {pre_btb_memory}\")\n",
    "\n",
    "    buy_to_buy_matrix = buy_to_buy_generator.generate_matrix(prepared_data)\n",
    "\n",
    "    btb_generation_time = time.time() - btb_start_time\n",
    "\n",
    "    if buy_to_buy_matrix and len(buy_to_buy_matrix) > 0:\n",
    "        buy_to_buy_success = True\n",
    "        log(f\" Buy-to-buy matrix generated successfully!\")\n",
    "        log(f\"   Source items: {len(buy_to_buy_matrix):,}\")\n",
    "        log(f\"   Generation time: {btb_generation_time:.1f} seconds\")\n",
    "    else:\n",
    "        log(\"  Buy-to-buy matrix generation completed but resulted in empty matrix\")\n",
    "        buy_to_buy_success = False\n",
    "\n",
    "except MemoryError as e:\n",
    "    log(f\" MEMORY ERROR during buy-to-buy generation: {e}\")\n",
    "    log(\"   Attempting to recover partial results...\")\n",
    "    buy_to_buy_success = False\n",
    "\n",
    "    # Try to get partial results from intermediate saves\n",
    "    try:\n",
    "        import glob\n",
    "        temp_files = glob.glob(f\"{config.OUTPUT_PATH}/btb_matrix_temp_chunk_*.pkl\")\n",
    "        if temp_files:\n",
    "            log(f\"   Found {len(temp_files)} BTB intermediate files - attempting recovery...\")\n",
    "            latest_file = max(temp_files, key=os.path.getctime)\n",
    "            with open(latest_file, \"rb\") as f:\n",
    "                temp_data = pickle.load(f)\n",
    "                buy_to_buy_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "            log(f\"   Recovered partial BTB matrix: {len(buy_to_buy_matrix):,} items\")\n",
    "        else:\n",
    "            buy_to_buy_matrix = {}\n",
    "    except Exception as recovery_error:\n",
    "        log(f\"   BTB recovery failed: {recovery_error}\")\n",
    "        buy_to_buy_matrix = {}\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\" Unexpected error during buy-to-buy generation: {e}\")\n",
    "    buy_to_buy_success = False\n",
    "\n",
    "    # Try to save current state\n",
    "    try:\n",
    "        if hasattr(buy_to_buy_generator, 'covisitation_counts') and buy_to_buy_generator.covisitation_counts:\n",
    "            buy_to_buy_generator.save_intermediate_results(force_save=True)\n",
    "            # Convert current state to matrix format\n",
    "            current_matrix = {}\n",
    "            for source_aid, targets in buy_to_buy_generator.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    current_matrix[source_aid] = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "            buy_to_buy_matrix = current_matrix\n",
    "            log(f\"   Saved partial BTB results: {len(buy_to_buy_matrix):,} items\")\n",
    "        else:\n",
    "            buy_to_buy_matrix = {}\n",
    "    except Exception as save_error:\n",
    "        log(f\"   Failed to save partial BTB results: {save_error}\")\n",
    "        buy_to_buy_matrix = {}\n",
    "\n",
    "finally:\n",
    "    # Clean up buy-to-buy generator memory\n",
    "    try:\n",
    "        if hasattr(buy_to_buy_generator, 'covisitation_counts'):\n",
    "            buy_to_buy_generator.covisitation_counts.clear()\n",
    "        gc.collect()\n",
    "        log(\"   BTB generator memory cleaned up\")\n",
    "    except Exception as cleanup_error:\n",
    "        log(f\"   BTB cleanup error: {cleanup_error}\")\n",
    "\n",
    "# Post-BTB memory check\n",
    "post_btb_memory = check_memory()\n",
    "log(f\"Post-BTB generation memory: {post_btb_memory}\")\n",
    "\n",
    "# Generation results summary\n",
    "log(f\"\\n MATRIX GENERATION SUMMARY:\")\n",
    "log(f\"   Click-to-buy success: {click_to_buy_success}\")\n",
    "log(f\"   Buy-to-buy success: {buy_to_buy_success}\")\n",
    "log(f\"   CTB matrix size: {len(click_to_buy_matrix):,} source items\")\n",
    "log(f\"   BTB matrix size: {len(buy_to_buy_matrix):,} source items\")\n",
    "log(f\"   Total generation time: {ctb_generation_time + btb_generation_time:.1f} seconds\")\n",
    "\n",
    "# Final validation\n",
    "both_matrices_successful = click_to_buy_success and buy_to_buy_success\n",
    "if both_matrices_successful:\n",
    "    log(\" Both matrices generated successfully!\")\n",
    "elif click_to_buy_success or buy_to_buy_success:\n",
    "    log(\"  Partial success - at least one matrix generated\")\n",
    "else:\n",
    "    log(\" Matrix generation failed - check logs for errors\")\n",
    "\n",
    "# Keep prepared_data for conversion analysis - will be cleaned up later\n",
    "log(\" Keeping prepared_data in memory for conversion analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7H-H6qYCe1d"
   },
   "source": [
    "## MATRIX ANALYSIS AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-tqRXheCgsI",
    "outputId": "cb3e0ffe-cfd9-452c-9795-1612dc49bf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:47:51]  Analyzing generated buy matrices...\n",
      "[2025-08-07 18:47:51]     Click-to-buy matrix:\n",
      "[2025-08-07 18:47:51]       Source items: 841,226\n",
      "[2025-08-07 18:47:51]       Total pairs: 6,851,523\n",
      "[2025-08-07 18:47:51]       Avg candidates: 8.1\n",
      "[2025-08-07 18:47:51]     Buy-to-buy matrix:\n",
      "[2025-08-07 18:47:51]       Source items: 311,156\n",
      "[2025-08-07 18:47:51]       Total pairs: 3,483,335\n",
      "[2025-08-07 18:47:51]       Avg candidates: 11.2\n",
      "[2025-08-07 18:47:51]      Temporal constraint analysis:\n",
      "[2025-08-07 18:47:51]       Valid transitions: 49,261,768 (93.6%)\n",
      "[2025-08-07 18:47:51]       Invalid transitions: 3,378,105\n",
      "[2025-08-07 18:47:51]       Sessions processed: 8,346,669\n",
      "[2025-08-07 18:47:51]     Matrix comparison:\n",
      "[2025-08-07 18:47:52]       Items only in click-to-buy: 590,708\n",
      "[2025-08-07 18:47:52]       Items only in buy-to-buy: 60,638\n",
      "[2025-08-07 18:47:52]       Overlapping items: 250,518\n",
      "[2025-08-07 18:47:52]  Matrix analysis completed!\n"
     ]
    }
   ],
   "source": [
    "def analyze_buy_matrices(click_to_buy_matrix: Dict,\n",
    "                        buy_to_buy_matrix: Dict,\n",
    "                        click_to_buy_generator: ClickToBuyMatrixGenerator) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of both buy matrices\n",
    "\n",
    "    Args:\n",
    "        click_to_buy_matrix: Generated click-to-buy matrix\n",
    "        buy_to_buy_matrix: Generated buy-to-buy matrix\n",
    "        click_to_buy_generator: Generator with temporal stats\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    log(\" Analyzing generated buy matrices...\")\n",
    "\n",
    "    analysis_results = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"click_to_buy_analysis\": {},\n",
    "        \"buy_to_buy_analysis\": {},\n",
    "        \"temporal_analysis\": {},\n",
    "        \"comparison_analysis\": {}\n",
    "    }\n",
    "\n",
    "    # Analyze click-to-buy matrix\n",
    "    if click_to_buy_matrix:\n",
    "        ctb_source_items = len(click_to_buy_matrix)\n",
    "        ctb_total_pairs = sum(len(candidates) for candidates in click_to_buy_matrix.values())\n",
    "        ctb_avg_candidates = ctb_total_pairs / ctb_source_items if ctb_source_items > 0 else 0\n",
    "\n",
    "        analysis_results[\"click_to_buy_analysis\"] = {\n",
    "            \"source_items\": ctb_source_items,\n",
    "            \"total_pairs\": ctb_total_pairs,\n",
    "            \"avg_candidates_per_item\": ctb_avg_candidates,\n",
    "            \"generation_successful\": click_to_buy_success\n",
    "        }\n",
    "\n",
    "        log(f\"    Click-to-buy matrix:\")\n",
    "        log(f\"      Source items: {ctb_source_items:,}\")\n",
    "        log(f\"      Total pairs: {ctb_total_pairs:,}\")\n",
    "        log(f\"      Avg candidates: {ctb_avg_candidates:.1f}\")\n",
    "\n",
    "    # Analyze buy-to-buy matrix\n",
    "    if buy_to_buy_matrix:\n",
    "        btb_source_items = len(buy_to_buy_matrix)\n",
    "        btb_total_pairs = sum(len(candidates) for candidates in buy_to_buy_matrix.values())\n",
    "        btb_avg_candidates = btb_total_pairs / btb_source_items if btb_source_items > 0 else 0\n",
    "\n",
    "        analysis_results[\"buy_to_buy_analysis\"] = {\n",
    "            \"source_items\": btb_source_items,\n",
    "            \"total_pairs\": btb_total_pairs,\n",
    "            \"avg_candidates_per_item\": btb_avg_candidates,\n",
    "            \"generation_successful\": buy_to_buy_success\n",
    "        }\n",
    "\n",
    "        log(f\"    Buy-to-buy matrix:\")\n",
    "        log(f\"      Source items: {btb_source_items:,}\")\n",
    "        log(f\"      Total pairs: {btb_total_pairs:,}\")\n",
    "        log(f\"      Avg candidates: {btb_avg_candidates:.1f}\")\n",
    "\n",
    "    # Temporal analysis for click-to-buy\n",
    "    if hasattr(click_to_buy_generator, 'temporal_stats'):\n",
    "        temporal_stats = click_to_buy_generator.temporal_stats\n",
    "        total_transitions = temporal_stats[\"valid_transitions\"] + temporal_stats[\"invalid_transitions\"]\n",
    "\n",
    "        if total_transitions > 0:\n",
    "            valid_pct = temporal_stats[\"valid_transitions\"] / total_transitions * 100\n",
    "\n",
    "            analysis_results[\"temporal_analysis\"] = {\n",
    "                \"valid_transitions\": temporal_stats[\"valid_transitions\"],\n",
    "                \"invalid_transitions\": temporal_stats[\"invalid_transitions\"],\n",
    "                \"total_transitions\": total_transitions,\n",
    "                \"valid_percentage\": valid_pct,\n",
    "                \"sessions_processed\": temporal_stats[\"total_sessions\"]\n",
    "            }\n",
    "\n",
    "            log(f\"     Temporal constraint analysis:\")\n",
    "            log(f\"      Valid transitions: {temporal_stats['valid_transitions']:,} ({valid_pct:.1f}%)\")\n",
    "            log(f\"      Invalid transitions: {temporal_stats['invalid_transitions']:,}\")\n",
    "            log(f\"      Sessions processed: {temporal_stats['total_sessions']:,}\")\n",
    "\n",
    "    # Comparison analysis\n",
    "    if click_to_buy_matrix and buy_to_buy_matrix:\n",
    "        # Find overlapping source items\n",
    "        ctb_items = set(click_to_buy_matrix.keys())\n",
    "        btb_items = set(buy_to_buy_matrix.keys())\n",
    "        overlap = len(ctb_items.intersection(btb_items))\n",
    "\n",
    "        analysis_results[\"comparison_analysis\"] = {\n",
    "            \"ctb_unique_items\": len(ctb_items - btb_items),\n",
    "            \"btb_unique_items\": len(btb_items - ctb_items),\n",
    "            \"overlapping_items\": overlap,\n",
    "            \"total_unique_items\": len(ctb_items.union(btb_items))\n",
    "        }\n",
    "\n",
    "        log(f\"    Matrix comparison:\")\n",
    "        log(f\"      Items only in click-to-buy: {len(ctb_items - btb_items):,}\")\n",
    "        log(f\"      Items only in buy-to-buy: {len(btb_items - ctb_items):,}\")\n",
    "        log(f\"      Overlapping items: {overlap:,}\")\n",
    "\n",
    "    log(\" Matrix analysis completed!\")\n",
    "    return analysis_results\n",
    "\n",
    "# Perform analysis\n",
    "matrix_analysis = analyze_buy_matrices(click_to_buy_matrix, buy_to_buy_matrix, click_to_buy_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki1PVGZ9Ck9j"
   },
   "source": [
    "## CONVERSION PATTERN ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtMf5IQMCpvg",
    "outputId": "cae69481-a221-4f91-dd6a-09fee291a48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:47:52]  Using existing prepared_data for conversion analysis...\n",
      "[2025-08-07 18:47:52]  Analyzing conversion patterns...\n",
      "[2025-08-07 18:47:52]    Sampling sessions for analysis...\n",
      "[2025-08-07 18:47:57]    Using sample of 5,000 sessions\n",
      "[2025-08-07 18:47:57]    Calculating conversion funnel...\n",
      "[2025-08-07 18:47:57]    Analyzing conversion timing...\n",
      "[2025-08-07 18:47:58]     Conversion analysis results:\n",
      "[2025-08-07 18:47:58]       Click-to-cart rate: 29.6%\n",
      "[2025-08-07 18:47:58]       Click-to-order rate: 13.0%\n",
      "[2025-08-07 18:47:58]       Cart-to-order rate: 43.8%\n",
      "[2025-08-07 18:47:58]       Avg time to conversion: 59.6 hours\n",
      "[2025-08-07 18:47:58]       Timing samples analyzed: 154\n",
      "[2025-08-07 18:47:58]  Conversion pattern analysis completed!\n",
      "[2025-08-07 18:47:59]    Sample data cleaned up\n",
      "[2025-08-07 18:48:02]  prepared_data cleared from memory after conversion analysis\n"
     ]
    }
   ],
   "source": [
    "def analyze_conversion_patterns_safe(data: pl.DataFrame = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze click-to-buy conversion patterns with robust error handling\n",
    "\n",
    "    Args:\n",
    "        data: Training data (optional, will attempt to reload if not provided)\n",
    "\n",
    "    Returns:\n",
    "        dict: Conversion pattern analysis results\n",
    "    \"\"\"\n",
    "    log(\" Analyzing conversion patterns...\")\n",
    "\n",
    "    # Check if data is available\n",
    "    if data is None:\n",
    "        log(\"   No data provided - attempting to reload prepared data...\")\n",
    "        try:\n",
    "            data = pl.read_parquet(f\"{config.OUTPUT_PATH}/covisit_data_prepared.parquet\")\n",
    "            log(f\"   Successfully reloaded data: {data.shape}\")\n",
    "        except Exception as reload_error:\n",
    "            log(f\"   Failed to reload data: {reload_error}\")\n",
    "            log(\"   Returning minimal conversion analysis...\")\n",
    "            return {\n",
    "                \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": \"Could not load data for conversion analysis\",\n",
    "                \"sample_size\": 0,\n",
    "                \"conversion_rates\": {\n",
    "                    \"click_to_cart_percent\": 0.0,\n",
    "                    \"click_to_order_percent\": 0.0,\n",
    "                    \"cart_to_order_percent\": 0.0\n",
    "                },\n",
    "                \"session_counts\": {\n",
    "                    \"sessions_with_clicks\": 0,\n",
    "                    \"sessions_with_carts\": 0,\n",
    "                    \"sessions_with_orders\": 0\n",
    "                },\n",
    "                \"conversion_timing\": {\n",
    "                    \"avg_hours_to_conversion\": 0.0,\n",
    "                    \"median_hours_to_conversion\": 0.0,\n",
    "                    \"conversion_samples\": 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "    try:\n",
    "        # Sample sessions for analysis (reduced sample size to save memory)\n",
    "        log(\"   Sampling sessions for analysis...\")\n",
    "        unique_sessions = data.select(\"session\").unique()\n",
    "        sample_size = min(5000, len(unique_sessions))  # Reduced from 10000 to 5000\n",
    "\n",
    "        sample_sessions = unique_sessions.sample(sample_size, seed=42)[\"session\"].to_list()\n",
    "        sample_data = data.filter(pl.col(\"session\").is_in(sample_sessions))\n",
    "        log(f\"   Using sample of {sample_size:,} sessions\")\n",
    "\n",
    "        # Conversion funnel analysis\n",
    "        log(\"   Calculating conversion funnel...\")\n",
    "        session_conversion = sample_data.group_by(\"session\").agg([\n",
    "            pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"clicks\"),\n",
    "            pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"carts\"),\n",
    "            pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"orders\"),\n",
    "            pl.col(\"aid\").n_unique().alias(\"unique_items\")\n",
    "        ])\n",
    "\n",
    "        # Calculate conversion rates\n",
    "        sessions_with_clicks = session_conversion.filter(pl.col(\"clicks\") > 0).height\n",
    "        sessions_with_carts = session_conversion.filter(pl.col(\"carts\") > 0).height\n",
    "        sessions_with_orders = session_conversion.filter(pl.col(\"orders\") > 0).height\n",
    "\n",
    "        click_to_cart_rate = sessions_with_carts / sessions_with_clicks * 100 if sessions_with_clicks > 0 else 0\n",
    "        click_to_order_rate = sessions_with_orders / sessions_with_clicks * 100 if sessions_with_clicks > 0 else 0\n",
    "        cart_to_order_rate = sessions_with_orders / sessions_with_carts * 100 if sessions_with_carts > 0 else 0\n",
    "\n",
    "        # Time to conversion analysis (smaller sample to save memory and time)\n",
    "        log(\"   Analyzing conversion timing...\")\n",
    "        conversion_times = []\n",
    "\n",
    "        # Convert to pandas for time analysis (smaller sample)\n",
    "        time_analysis_sample = sample_sessions[:500]  # Even smaller sample for timing analysis\n",
    "        time_sample_data = sample_data.filter(pl.col(\"session\").is_in(time_analysis_sample))\n",
    "        sample_pd = time_sample_data.to_pandas()\n",
    "\n",
    "        for session_id in time_analysis_sample:\n",
    "            try:\n",
    "                session_events = sample_pd[sample_pd['session'] == session_id].sort_values('ts')\n",
    "\n",
    "                if len(session_events) > 1:\n",
    "                    clicks_data = session_events[session_events['type'] == 'clicks']\n",
    "                    purchases_data = session_events[session_events['type'].isin(['carts', 'orders'])]\n",
    "\n",
    "                    if len(clicks_data) > 0 and len(purchases_data) > 0:\n",
    "                        first_click = clicks_data['ts'].min()\n",
    "                        first_purchase = purchases_data['ts'].min()\n",
    "\n",
    "                        if pd.notna(first_click) and pd.notna(first_purchase) and first_purchase > first_click:\n",
    "                            time_to_conversion = (first_purchase - first_click) / (1000 * 60 * 60)  # Hours\n",
    "                            if 0 < time_to_conversion < 24 * 30:  # Reasonable range: 0-30 days\n",
    "                                conversion_times.append(time_to_conversion)\n",
    "            except Exception as session_error:\n",
    "                # Skip problematic sessions\n",
    "                continue\n",
    "\n",
    "        # Conversion time statistics\n",
    "        if conversion_times:\n",
    "            try:\n",
    "                avg_conversion_hours = np.mean(conversion_times)\n",
    "                median_conversion_hours = np.median(conversion_times)\n",
    "            except Exception as stats_error:\n",
    "                log(f\"   Error calculating conversion time stats: {stats_error}\")\n",
    "                avg_conversion_hours = median_conversion_hours = 0\n",
    "        else:\n",
    "            avg_conversion_hours = median_conversion_hours = 0\n",
    "\n",
    "        log(f\"    Conversion analysis results:\")\n",
    "        log(f\"      Click-to-cart rate: {click_to_cart_rate:.1f}%\")\n",
    "        log(f\"      Click-to-order rate: {click_to_order_rate:.1f}%\")\n",
    "        log(f\"      Cart-to-order rate: {cart_to_order_rate:.1f}%\")\n",
    "        log(f\"      Avg time to conversion: {avg_conversion_hours:.1f} hours\")\n",
    "        log(f\"      Timing samples analyzed: {len(conversion_times):,}\")\n",
    "\n",
    "        conversion_analysis = {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"sample_size\": sample_size,\n",
    "            \"conversion_rates\": {\n",
    "                \"click_to_cart_percent\": float(click_to_cart_rate),\n",
    "                \"click_to_order_percent\": float(click_to_order_rate),\n",
    "                \"cart_to_order_percent\": float(cart_to_order_rate)\n",
    "            },\n",
    "            \"session_counts\": {\n",
    "                \"sessions_with_clicks\": int(sessions_with_clicks),\n",
    "                \"sessions_with_carts\": int(sessions_with_carts),\n",
    "                \"sessions_with_orders\": int(sessions_with_orders)\n",
    "            },\n",
    "            \"conversion_timing\": {\n",
    "                \"avg_hours_to_conversion\": float(avg_conversion_hours),\n",
    "                \"median_hours_to_conversion\": float(median_conversion_hours),\n",
    "                \"conversion_samples\": len(conversion_times)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        log(\" Conversion pattern analysis completed!\")\n",
    "\n",
    "        # Clean up sample data to save memory\n",
    "        try:\n",
    "            del sample_data, sample_pd, time_sample_data\n",
    "            gc.collect()\n",
    "            log(\"   Sample data cleaned up\")\n",
    "        except Exception as cleanup_error:\n",
    "            log(f\"   Cleanup warning: {cleanup_error}\")\n",
    "\n",
    "        return conversion_analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\" Error during conversion analysis: {e}\")\n",
    "        log(\"   Returning fallback conversion analysis...\")\n",
    "\n",
    "        # Return fallback analysis\n",
    "        return {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": f\"Conversion analysis failed: {str(e)}\",\n",
    "            \"sample_size\": 0,\n",
    "            \"conversion_rates\": {\n",
    "                \"click_to_cart_percent\": 0.0,\n",
    "                \"click_to_order_percent\": 0.0,\n",
    "                \"cart_to_order_percent\": 0.0\n",
    "            },\n",
    "            \"session_counts\": {\n",
    "                \"sessions_with_clicks\": 0,\n",
    "                \"sessions_with_carts\": 0,\n",
    "                \"sessions_with_orders\": 0\n",
    "            },\n",
    "            \"conversion_timing\": {\n",
    "                \"avg_hours_to_conversion\": 0.0,\n",
    "                \"median_hours_to_conversion\": 0.0,\n",
    "                \"conversion_samples\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Analyze conversion patterns with robust error handling\n",
    "try:\n",
    "    # Check if prepared_data is still available in the current scope\n",
    "    if 'prepared_data' in globals() and prepared_data is not None:\n",
    "        log(\" Using existing prepared_data for conversion analysis...\")\n",
    "        conversion_analysis = analyze_conversion_patterns_safe(prepared_data)\n",
    "    else:\n",
    "        log(\" prepared_data not available in global scope - attempting to reload...\")\n",
    "        conversion_analysis = analyze_conversion_patterns_safe(None)\n",
    "\n",
    "except Exception as analysis_error:\n",
    "    log(f\" Critical error in conversion analysis: {analysis_error}\")\n",
    "    # Create minimal fallback analysis\n",
    "    conversion_analysis = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"error\": f\"Critical conversion analysis failure: {str(analysis_error)}\",\n",
    "        \"sample_size\": 0,\n",
    "        \"conversion_rates\": {\n",
    "            \"click_to_cart_percent\": 0.0,\n",
    "            \"click_to_order_percent\": 0.0,\n",
    "            \"cart_to_order_percent\": 0.0\n",
    "        },\n",
    "        \"session_counts\": {\n",
    "            \"sessions_with_clicks\": 0,\n",
    "            \"sessions_with_carts\": 0,\n",
    "            \"sessions_with_orders\": 0\n",
    "        },\n",
    "        \"conversion_timing\": {\n",
    "            \"avg_hours_to_conversion\": 0.0,\n",
    "            \"median_hours_to_conversion\": 0.0,\n",
    "            \"conversion_samples\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Now clean up prepared_data to free memory\n",
    "try:\n",
    "    if 'prepared_data' in globals():\n",
    "        del prepared_data\n",
    "        gc.collect()\n",
    "        log(\" prepared_data cleared from memory after conversion analysis\")\n",
    "except Exception as cleanup_error:\n",
    "    log(f\"   Cleanup warning: {cleanup_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75YmwJGhCqXq"
   },
   "source": [
    "## SAVE OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoMcZ6B5Cs97",
    "outputId": "129532f0-bf15-448d-c3cb-11981dab7884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:48:02]  Saving buy matrices outputs with enhanced error handling...\n",
      "[2025-08-07 18:48:07]     click_to_buy_matrix.pkl saved (65.3 MB)\n",
      "[2025-08-07 18:48:10]     buy_to_buy_matrix.pkl saved (32.3 MB)\n",
      "[2025-08-07 18:48:10]     buy_matrices_statistics.json saved\n",
      "[2025-08-07 18:48:10]     temporal_validation.json saved\n",
      "[2025-08-07 18:48:10]     conversion_analysis.json saved\n",
      "[2025-08-07 18:48:10]     buy_matrices_memory_log.json saved\n",
      "[2025-08-07 18:48:10]     part_2a3_summary.json saved\n",
      "[2025-08-07 18:48:10]  All buy matrices outputs saved successfully!\n",
      "[2025-08-07 18:48:10]  Output saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert non-JSON serializable objects to JSON-compatible types\n",
    "\n",
    "    Args:\n",
    "        obj: Object to convert\n",
    "\n",
    "    Returns:\n",
    "        JSON-serializable version of the object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # Handle numpy scalars\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_buy_matrices_outputs_enhanced(click_to_buy_matrix: Dict,\n",
    "                                     buy_to_buy_matrix: Dict,\n",
    "                                     matrix_analysis: Dict,\n",
    "                                     conversion_analysis: Dict,\n",
    "                                     validation_results: Dict,\n",
    "                                     memory_log: List,\n",
    "                                     click_to_buy_success: bool,\n",
    "                                     buy_to_buy_success: bool,\n",
    "                                     ctb_generation_time: float,\n",
    "                                     btb_generation_time: float):\n",
    "    \"\"\"\n",
    "    Save all outputs from buy matrices generation with proper JSON handling and error recovery\n",
    "\n",
    "    Args:\n",
    "        click_to_buy_matrix: Generated click-to-buy matrix\n",
    "        buy_to_buy_matrix: Generated buy-to-buy matrix\n",
    "        matrix_analysis: Matrix analysis results\n",
    "        conversion_analysis: Conversion pattern analysis\n",
    "        validation_results: Input validation results\n",
    "        memory_log: Memory usage log\n",
    "        click_to_buy_success: Success status of CTB generation\n",
    "        buy_to_buy_success: Success status of BTB generation\n",
    "        ctb_generation_time: Time taken for CTB generation\n",
    "        btb_generation_time: Time taken for BTB generation\n",
    "    \"\"\"\n",
    "    log(\" Saving buy matrices outputs with enhanced error handling...\")\n",
    "\n",
    "    try:\n",
    "        saved_files = {}\n",
    "\n",
    "        # 1. Save click-to-buy matrix\n",
    "        ctb_path = f\"{config.OUTPUT_PATH}/click_to_buy_matrix.pkl\"\n",
    "        with open(ctb_path, \"wb\") as f:\n",
    "            pickle.dump(click_to_buy_matrix, f)\n",
    "\n",
    "        ctb_size = os.path.getsize(ctb_path) / (1024*1024)\n",
    "        log(f\"    click_to_buy_matrix.pkl saved ({ctb_size:.1f} MB)\")\n",
    "        saved_files[\"ctb_path\"] = ctb_path\n",
    "\n",
    "        # 2. Save buy-to-buy matrix\n",
    "        btb_path = f\"{config.OUTPUT_PATH}/buy_to_buy_matrix.pkl\"\n",
    "        with open(btb_path, \"wb\") as f:\n",
    "            pickle.dump(buy_to_buy_matrix, f)\n",
    "\n",
    "        btb_size = os.path.getsize(btb_path) / (1024*1024)\n",
    "        log(f\"    buy_to_buy_matrix.pkl saved ({btb_size:.1f} MB)\")\n",
    "        saved_files[\"btb_path\"] = btb_path\n",
    "\n",
    "        # 3. Save matrix statistics (with JSON serialization fix)\n",
    "        stats_path = f\"{config.OUTPUT_PATH}/buy_matrices_statistics.json\"\n",
    "\n",
    "        # Convert analysis to JSON-serializable format\n",
    "        json_safe_analysis = convert_to_json_serializable(matrix_analysis)\n",
    "\n",
    "        with open(stats_path, \"w\") as f:\n",
    "            json.dump(json_safe_analysis, f, indent=2, default=str)\n",
    "        log(f\"    buy_matrices_statistics.json saved\")\n",
    "        saved_files[\"stats_path\"] = stats_path\n",
    "\n",
    "        # 4. Save temporal validation results\n",
    "        temporal_path = f\"{config.OUTPUT_PATH}/temporal_validation.json\"\n",
    "        temporal_data = {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"time_windows\": convert_to_json_serializable(validation_results.get(\"time_windows\", {})),\n",
    "            \"temporal_analysis\": convert_to_json_serializable(matrix_analysis.get(\"temporal_analysis\", {})),\n",
    "            \"validation_notes\": [\n",
    "                f\"Click-to-buy window: {config.CLICK_TO_BUY_TIME_WINDOW_DAYS} days\",\n",
    "                f\"Buy-to-buy window: {config.BUY_TO_BUY_TIME_WINDOW_DAYS} days\",\n",
    "                \"Temporal constraints enforced for all relationships\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        with open(temporal_path, \"w\") as f:\n",
    "            json.dump(temporal_data, f, indent=2, default=str)\n",
    "        log(f\"    temporal_validation.json saved\")\n",
    "        saved_files[\"temporal_path\"] = temporal_path\n",
    "\n",
    "        # 5. Save conversion analysis (with JSON conversion and error handling)\n",
    "        conversion_path = f\"{config.OUTPUT_PATH}/conversion_analysis.json\"\n",
    "\n",
    "        try:\n",
    "            # Validate conversion_analysis before saving\n",
    "            if conversion_analysis and isinstance(conversion_analysis, dict):\n",
    "                json_safe_conversion = convert_to_json_serializable(conversion_analysis)\n",
    "            else:\n",
    "                log(\"    Invalid conversion analysis - creating fallback\")\n",
    "                json_safe_conversion = {\n",
    "                    \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "                    \"error\": \"Conversion analysis data was invalid or missing\",\n",
    "                    \"sample_size\": 0,\n",
    "                    \"conversion_rates\": {\"click_to_cart_percent\": 0.0, \"click_to_order_percent\": 0.0, \"cart_to_order_percent\": 0.0},\n",
    "                    \"session_counts\": {\"sessions_with_clicks\": 0, \"sessions_with_carts\": 0, \"sessions_with_orders\": 0},\n",
    "                    \"conversion_timing\": {\"avg_hours_to_conversion\": 0.0, \"median_hours_to_conversion\": 0.0, \"conversion_samples\": 0}\n",
    "                }\n",
    "        except Exception as conv_prep_error:\n",
    "            log(f\"    Error preparing conversion analysis for save: {conv_prep_error}\")\n",
    "            json_safe_conversion = {\n",
    "                \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": f\"Error preparing conversion data: {str(conv_prep_error)}\",\n",
    "                \"sample_size\": 0,\n",
    "                \"conversion_rates\": {\"click_to_cart_percent\": 0.0, \"click_to_order_percent\": 0.0, \"cart_to_order_percent\": 0.0},\n",
    "                \"session_counts\": {\"sessions_with_clicks\": 0, \"sessions_with_carts\": 0, \"sessions_with_orders\": 0},\n",
    "                \"conversion_timing\": {\"avg_hours_to_conversion\": 0.0, \"median_hours_to_conversion\": 0.0, \"conversion_samples\": 0}\n",
    "            }\n",
    "\n",
    "        with open(conversion_path, \"w\") as f:\n",
    "            json.dump(json_safe_conversion, f, indent=2, default=str)\n",
    "        log(f\"    conversion_analysis.json saved\")\n",
    "        saved_files[\"conversion_path\"] = conversion_path\n",
    "\n",
    "        # 6. Save memory usage log (with JSON conversion)\n",
    "        memory_path = f\"{config.OUTPUT_PATH}/buy_matrices_memory_log.json\"\n",
    "\n",
    "        try:\n",
    "            json_safe_memory_log = convert_to_json_serializable(memory_log)\n",
    "        except Exception as memory_convert_error:\n",
    "            log(f\"    Error converting memory log: {memory_convert_error}\")\n",
    "            json_safe_memory_log = []\n",
    "\n",
    "        memory_data = {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"memory_log\": json_safe_memory_log,\n",
    "            \"generation_successful\": {\n",
    "                \"click_to_buy\": click_to_buy_success,\n",
    "                \"buy_to_buy\": buy_to_buy_success,\n",
    "                \"both_matrices\": click_to_buy_success and buy_to_buy_success\n",
    "            },\n",
    "            \"generation_times\": {\n",
    "                \"click_to_buy_seconds\": ctb_generation_time,\n",
    "                \"buy_to_buy_seconds\": btb_generation_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(memory_path, \"w\") as f:\n",
    "            json.dump(memory_data, f, indent=2, default=str)\n",
    "        log(f\"    buy_matrices_memory_log.json saved\")\n",
    "        saved_files[\"memory_path\"] = memory_path\n",
    "\n",
    "        # 7. Save comprehensive summary\n",
    "        try:\n",
    "            summary = {\n",
    "                \"notebook\": \"Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\",\n",
    "                \"completion_timestamp\": datetime.now().isoformat(),\n",
    "                \"generation_results\": {\n",
    "                    \"click_to_buy_successful\": click_to_buy_success,\n",
    "                    \"buy_to_buy_successful\": buy_to_buy_success,\n",
    "                    \"both_matrices_generated\": click_to_buy_success and buy_to_buy_success\n",
    "                },\n",
    "                \"inputs_used\": {\n",
    "                    \"covisit_data_prepared.parquet\": \"Optimized training data from Part 2A1\",\n",
    "                    \"chunking_strategy.json\": f\"Chunk sizes: CTB={validation_results.get('chunk_sizes', {}).get('click_to_buy', 0):,}, BTB={validation_results.get('chunk_sizes', {}).get('buy_to_buy', 0):,}\",\n",
    "                    \"session_analysis.json\": \"Session insights for optimization\"\n",
    "                },\n",
    "                \"outputs_generated\": {\n",
    "                    \"click_to_buy_matrix.pkl\": f\"{matrix_analysis.get('click_to_buy_analysis', {}).get('source_items', 0):,} source items\",\n",
    "                    \"buy_to_buy_matrix.pkl\": f\"{matrix_analysis.get('buy_to_buy_analysis', {}).get('source_items', 0):,} source items\",\n",
    "                    \"buy_matrices_statistics.json\": \"Comprehensive matrix analysis\",\n",
    "                    \"temporal_validation.json\": \"Temporal constraint validation\",\n",
    "                    \"conversion_analysis.json\": \"Click-to-buy conversion patterns (with error handling)\",\n",
    "                    \"buy_matrices_memory_log.json\": \"Memory usage tracking\"\n",
    "                },\n",
    "                \"key_metrics\": convert_to_json_serializable({\n",
    "                    \"ctb_source_items\": matrix_analysis.get(\"click_to_buy_analysis\", {}).get(\"source_items\", 0),\n",
    "                    \"btb_source_items\": matrix_analysis.get(\"buy_to_buy_analysis\", {}).get(\"source_items\", 0),\n",
    "                    \"total_file_size_mb\": float(ctb_size + btb_size),\n",
    "                    \"valid_temporal_transitions\": matrix_analysis.get(\"temporal_analysis\", {}).get(\"valid_transitions\", 0),\n",
    "                    \"peak_memory_percent\": max([entry.get(\"memory_percent\", 0) for entry in memory_log]) if memory_log else 0,\n",
    "                    \"conversion_analysis_status\": \"completed\" if not conversion_analysis.get(\"error\") else \"completed_with_errors\"\n",
    "                }),\n",
    "                \"memory_management\": {\n",
    "                    \"intermediate_saves_used\": True,\n",
    "                    \"emergency_saves_triggered\": any(entry.get(\"memory_percent\", 0) > 90 for entry in memory_log) if memory_log else False,\n",
    "                    \"memory_efficient_processing\": True\n",
    "                },\n",
    "                \"data_quality\": {\n",
    "                    \"conversion_analysis_available\": \"error\" not in conversion_analysis if conversion_analysis else False,\n",
    "                    \"temporal_validation_passed\": matrix_analysis.get(\"temporal_analysis\", {}).get(\"valid_percentage\", 0) > 50\n",
    "                },\n",
    "                \"next_step\": \"Run Part 2A4: Matrix Consolidation & Validation\" if (click_to_buy_success and buy_to_buy_success) else \"Review errors and retry with adjusted parameters\"\n",
    "            }\n",
    "        except Exception as summary_error:\n",
    "            log(f\"    Error creating comprehensive summary: {summary_error}\")\n",
    "            # Create minimal summary\n",
    "            summary = {\n",
    "                \"notebook\": \"Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\",\n",
    "                \"completion_timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": f\"Summary creation error: {str(summary_error)}\",\n",
    "                \"basic_status\": \"Matrices saved but summary incomplete\"\n",
    "            }\n",
    "\n",
    "        summary_path = f\"{config.OUTPUT_PATH}/part_2a3_summary.json\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        log(f\"    part_2a3_summary.json saved\")\n",
    "        saved_files[\"summary_path\"] = summary_path\n",
    "\n",
    "        log(\" All buy matrices outputs saved successfully!\")\n",
    "        return saved_files\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\" Error saving outputs: {e}\")\n",
    "        log(f\"   Error type: {type(e).__name__}\")\n",
    "        log(\"   Attempting fallback saves...\")\n",
    "\n",
    "        # Fallback: save essential files only\n",
    "        try:\n",
    "            fallback_files = {}\n",
    "\n",
    "            # Save matrices (most important)\n",
    "            if click_to_buy_matrix:\n",
    "                ctb_fallback_path = f\"{config.OUTPUT_PATH}/click_to_buy_matrix.pkl\"\n",
    "                with open(ctb_fallback_path, \"wb\") as f:\n",
    "                    pickle.dump(click_to_buy_matrix, f)\n",
    "                fallback_files[\"ctb_path\"] = ctb_fallback_path\n",
    "                ctb_size = os.path.getsize(ctb_fallback_path) / (1024*1024)\n",
    "                log(f\"    Fallback: click_to_buy_matrix.pkl saved ({ctb_size:.1f} MB)\")\n",
    "\n",
    "            if buy_to_buy_matrix:\n",
    "                btb_fallback_path = f\"{config.OUTPUT_PATH}/buy_to_buy_matrix.pkl\"\n",
    "                with open(btb_fallback_path, \"wb\") as f:\n",
    "                    pickle.dump(buy_to_buy_matrix, f)\n",
    "                fallback_files[\"btb_path\"] = btb_fallback_path\n",
    "                btb_size = os.path.getsize(btb_fallback_path) / (1024*1024)\n",
    "                log(f\"    Fallback: buy_to_buy_matrix.pkl saved ({btb_size:.1f} MB)\")\n",
    "\n",
    "            # Save basic summary as text (avoiding JSON issues)\n",
    "            summary_txt_path = f\"{config.OUTPUT_PATH}/part_2a3_summary.txt\"\n",
    "            with open(summary_txt_path, \"w\") as f:\n",
    "                f.write(f\"Part 2A3 Summary - Emergency Save\\n\")\n",
    "                f.write(f\"Generated: {datetime.now().isoformat()}\\n\")\n",
    "                f.write(f\"Click-to-buy Success: {click_to_buy_success}\\n\")\n",
    "                f.write(f\"Buy-to-buy Success: {buy_to_buy_success}\\n\")\n",
    "                f.write(f\"CTB Source Items: {len(click_to_buy_matrix):,}\\n\")\n",
    "                f.write(f\"BTB Source Items: {len(buy_to_buy_matrix):,}\\n\")\n",
    "                f.write(f\"CTB File Size: {ctb_size:.1f} MB\\n\")\n",
    "                f.write(f\"BTB File Size: {btb_size:.1f} MB\\n\")\n",
    "                f.write(f\"Total File Size: {(ctb_size + btb_size):.1f} MB\\n\")\n",
    "                f.write(f\"Note: This is an emergency save due to JSON serialization issues\\n\")\n",
    "                f.write(f\"Main matrices were saved successfully\\n\")\n",
    "            fallback_files[\"summary_txt_path\"] = summary_txt_path\n",
    "            log(f\"    Fallback: part_2a3_summary.txt saved\")\n",
    "\n",
    "            return fallback_files\n",
    "\n",
    "        except Exception as fallback_error:\n",
    "            log(f\" Fallback save also failed: {fallback_error}\")\n",
    "            raise e\n",
    "\n",
    "# Save all outputs with enhanced error handling\n",
    "try:\n",
    "    output_paths = save_buy_matrices_outputs_enhanced(\n",
    "        click_to_buy_matrix,\n",
    "        buy_to_buy_matrix,\n",
    "        matrix_analysis,\n",
    "        conversion_analysis,\n",
    "        validation_results,\n",
    "        memory_log,\n",
    "        click_to_buy_success,\n",
    "        buy_to_buy_success,\n",
    "        ctb_generation_time,\n",
    "        btb_generation_time\n",
    "    )\n",
    "    log(\" Output saving completed successfully!\")\n",
    "except Exception as save_error:\n",
    "    log(f\" Critical error saving outputs: {save_error}\")\n",
    "    # Create minimal output paths for summary section\n",
    "    output_paths = {\n",
    "        \"ctb_path\": f\"{config.OUTPUT_PATH}/click_to_buy_matrix.pkl\",\n",
    "        \"btb_path\": f\"{config.OUTPUT_PATH}/buy_to_buy_matrix.pkl\",\n",
    "        \"stats_path\": f\"{config.OUTPUT_PATH}/buy_matrices_statistics.json\",\n",
    "        \"temporal_path\": f\"{config.OUTPUT_PATH}/temporal_validation.json\",\n",
    "        \"conversion_path\": f\"{config.OUTPUT_PATH}/conversion_analysis.json\",\n",
    "        \"summary_path\": f\"{config.OUTPUT_PATH}/part_2a3_summary.json\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTpOTHoiCxgN"
   },
   "source": [
    "## FINAL SUMMARY AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFIOv3tFCyVr",
    "outputId": "916d068d-fc29-401c-83fd-1d143dba2211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:48:10] \n",
      "================================================================================\n",
      "[2025-08-07 18:48:10] PART 2A3 COMPLETED: CLICK-TO-BUY & BUY-TO-BUY MATRIX GENERATION\n",
      "[2025-08-07 18:48:10] ================================================================================\n",
      "[2025-08-07 18:48:10] \n",
      " MATRIX GENERATION RESULTS:\n",
      "[2025-08-07 18:48:10]  Click-to-buy matrix:\n",
      "[2025-08-07 18:48:10]     Source items: 841,226\n",
      "[2025-08-07 18:48:10]     Total pairs: 6,851,523\n",
      "[2025-08-07 18:48:10]     Avg candidates: 8.1\n",
      "[2025-08-07 18:48:10]  Buy-to-buy matrix:\n",
      "[2025-08-07 18:48:10]     Source items: 311,156\n",
      "[2025-08-07 18:48:10]     Total pairs: 3,483,335\n",
      "[2025-08-07 18:48:10]     Avg candidates: 11.2\n",
      "[2025-08-07 18:48:10]  Temporal validation:\n",
      "[2025-08-07 18:48:10]      Valid transitions: 49,261,768\n",
      "[2025-08-07 18:48:10]      Invalid transitions: 3,378,105\n",
      "[2025-08-07 18:48:10]      Valid percentage: 93.6%\n",
      "[2025-08-07 18:48:10] \n",
      " CONVERSION INSIGHTS:\n",
      "[2025-08-07 18:48:10]  Conversion analysis results:\n",
      "[2025-08-07 18:48:10]    Click-to-cart rate: 29.6%\n",
      "[2025-08-07 18:48:10]    Click-to-order rate: 13.0%\n",
      "[2025-08-07 18:48:10]    Cart-to-order rate: 43.8%\n",
      "[2025-08-07 18:48:10]    Avg time to conversion: 59.6 hours\n",
      "[2025-08-07 18:48:10]    Sample size: 5,000 sessions\n",
      "[2025-08-07 18:48:10] \n",
      " OUTPUT FILES GENERATED:\n",
      "[2025-08-07 18:48:10]     click_to_buy_matrix.pkl (65.3 MB)\n",
      "[2025-08-07 18:48:10]     buy_to_buy_matrix.pkl (32.3 MB)\n",
      "[2025-08-07 18:48:10]     buy_matrices_statistics.json\n",
      "[2025-08-07 18:48:10]     temporal_validation.json\n",
      "[2025-08-07 18:48:10]     conversion_analysis.json\n",
      "[2025-08-07 18:48:10]     buy_matrices_memory_log.json\n",
      "[2025-08-07 18:48:10]     part_2a3_summary.json\n",
      "[2025-08-07 18:48:10]  Files location: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-07 18:48:10] \n",
      " QUALITY ASSESSMENT:\n",
      "[2025-08-07 18:48:10]    Both matrices generated:  yes\n",
      "[2025-08-07 18:48:10]    Temporal validation (>50%):  yes (93.6%)\n",
      "[2025-08-07 18:48:10]    Conversion rates reasonable:  yes (29.6%)\n",
      "[2025-08-07 18:48:10]    File sizes appropriate:  yes (97.6 MB total)\n",
      "[2025-08-07 18:48:10] \n",
      " Overall Quality:  EXCELLENT (4/4 criteria met)\n",
      "[2025-08-07 18:48:10] \n",
      "  PERFORMANCE METRICS:\n",
      "[2025-08-07 18:48:10]    Total generation time: 330.1 seconds (5.5 minutes)\n",
      "[2025-08-07 18:48:10]    Click-to-buy time: 263.3 seconds\n",
      "[2025-08-07 18:48:10]    Buy-to-buy time: 66.8 seconds\n",
      "[2025-08-07 18:48:10]    Processing speed: ~11652.9 sessions/second\n",
      "[2025-08-07 18:48:10] \n",
      " RECOMMENDATIONS:\n",
      "[2025-08-07 18:48:10]     Matrix generation completed successfully!\n",
      "[2025-08-07 18:48:10]     Ready to proceed to Part 2A4: Matrix Consolidation & Validation\n",
      "[2025-08-07 18:48:10]     Both matrices have good quality and are suitable for recommendations\n",
      "[2025-08-07 18:48:10]     Memory usage was acceptable (20.2% peak)\n",
      "[2025-08-07 18:48:10] \n",
      " PERFORMING FINAL CLEANUP...\n",
      "[2025-08-07 18:48:12]      Cleaned up: click_to_buy_matrix (841226 items), buy_to_buy_matrix (311156 items), click_to_buy_generator, buy_to_buy_generator\n",
      "[2025-08-07 18:48:12]     Final memory status: LOW\n",
      "[2025-08-07 18:48:12] \n",
      " Part 2A3 processing completed!\n",
      "[2025-08-07 18:48:12]  Check all output files in: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-07 18:48:13]  Execution summary saved: part_2a3_execution_summary.json\n",
      "[2025-08-07 18:48:13]  Part 2A3 finished!\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2A3 COMPLETED: CLICK-TO-BUY & BUY-TO-BUY MATRIX GENERATION\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "log(f\"\\n MATRIX GENERATION RESULTS:\")\n",
    "if matrix_analysis.get(\"click_to_buy_analysis\"):\n",
    "    ctb_analysis = matrix_analysis[\"click_to_buy_analysis\"]\n",
    "    log(f\" Click-to-buy matrix:\")\n",
    "    log(f\"    Source items: {ctb_analysis['source_items']:,}\")\n",
    "    log(f\"    Total pairs: {ctb_analysis['total_pairs']:,}\")\n",
    "    log(f\"    Avg candidates: {ctb_analysis['avg_candidates_per_item']:.1f}\")\n",
    "\n",
    "if matrix_analysis.get(\"buy_to_buy_analysis\"):\n",
    "    btb_analysis = matrix_analysis[\"buy_to_buy_analysis\"]\n",
    "    log(f\" Buy-to-buy matrix:\")\n",
    "    log(f\"    Source items: {btb_analysis['source_items']:,}\")\n",
    "    log(f\"    Total pairs: {btb_analysis['total_pairs']:,}\")\n",
    "    log(f\"    Avg candidates: {btb_analysis['avg_candidates_per_item']:.1f}\")\n",
    "\n",
    "if matrix_analysis.get(\"temporal_analysis\"):\n",
    "    temporal = matrix_analysis[\"temporal_analysis\"]\n",
    "    log(f\" Temporal validation:\")\n",
    "    log(f\"     Valid transitions: {temporal.get('valid_transitions', 0):,}\")\n",
    "    log(f\"     Invalid transitions: {temporal.get('invalid_transitions', 0):,}\")\n",
    "    log(f\"     Valid percentage: {temporal.get('valid_percentage', 0):.1f}%\")\n",
    "\n",
    "log(f\"\\n CONVERSION INSIGHTS:\")\n",
    "# Handle cases where conversion analysis might have errors\n",
    "try:\n",
    "    conv_rates = conversion_analysis.get(\"conversion_rates\", {})\n",
    "    conv_timing = conversion_analysis.get(\"conversion_timing\", {})\n",
    "\n",
    "    # Check if there was an error in conversion analysis\n",
    "    if \"error\" in conversion_analysis:\n",
    "        log(f\"  Conversion analysis had issues: {conversion_analysis['error']}\")\n",
    "        log(f\" Using fallback conversion metrics:\")\n",
    "    else:\n",
    "        log(f\" Conversion analysis results:\")\n",
    "\n",
    "    log(f\"   Click-to-cart rate: {conv_rates.get('click_to_cart_percent', 0):.1f}%\")\n",
    "    log(f\"   Click-to-order rate: {conv_rates.get('click_to_order_percent', 0):.1f}%\")\n",
    "    log(f\"   Cart-to-order rate: {conv_rates.get('cart_to_order_percent', 0):.1f}%\")\n",
    "    log(f\"   Avg time to conversion: {conv_timing.get('avg_hours_to_conversion', 0):.1f} hours\")\n",
    "    log(f\"   Sample size: {conversion_analysis.get('sample_size', 0):,} sessions\")\n",
    "\n",
    "except Exception as conv_error:\n",
    "    log(f\"  Error displaying conversion insights: {conv_error}\")\n",
    "    # Provide minimal fallback display\n",
    "    log(f\" Conversion analysis: Not available due to errors\")\n",
    "\n",
    "log(f\"\\n OUTPUT FILES GENERATED:\")\n",
    "try:\n",
    "    if 'output_paths' in globals() and output_paths:\n",
    "        for description, path in output_paths.items():\n",
    "            filename = os.path.basename(path)\n",
    "            try:\n",
    "                if path.endswith('.pkl') and os.path.exists(path):\n",
    "                    file_size = os.path.getsize(path) / (1024*1024)\n",
    "                    log(f\"    {filename} ({file_size:.1f} MB)\")\n",
    "                elif os.path.exists(path):\n",
    "                    log(f\"    {filename}\")\n",
    "                else:\n",
    "                    log(f\"    {filename} (not found)\")\n",
    "            except Exception as file_error:\n",
    "                log(f\"    {filename} (error checking: {file_error})\")\n",
    "        log(f\" Files location: {config.OUTPUT_PATH}\")\n",
    "    else:\n",
    "        log(\"    Output paths not available - files may not have been saved properly\")\n",
    "\n",
    "except Exception as file_list_error:\n",
    "    log(f\"  Error listing output files: {file_list_error}\")\n",
    "\n",
    "# Quality assessment with robust error handling\n",
    "log(f\"\\n QUALITY ASSESSMENT:\")\n",
    "try:\n",
    "    matrices_generated = click_to_buy_success and buy_to_buy_success\n",
    "    temporal_valid = matrix_analysis.get(\"temporal_analysis\", {}).get(\"valid_percentage\", 0) > 50\n",
    "\n",
    "    # Handle conversion rate check safely\n",
    "    try:\n",
    "        conversion_reasonable = conv_rates.get(\"click_to_cart_percent\", 0) > 1\n",
    "        conv_rate_display = conv_rates.get(\"click_to_cart_percent\", 0)\n",
    "    except:\n",
    "        conversion_reasonable = False\n",
    "        conv_rate_display = 0\n",
    "\n",
    "    log(f\"   Both matrices generated: {' yes' if matrices_generated else ' no'}\")\n",
    "    log(f\"   Temporal validation (>50%): {' yes' if temporal_valid else ' no'} ({matrix_analysis.get('temporal_analysis', {}).get('valid_percentage', 0):.1f}%)\")\n",
    "    log(f\"   Conversion rates reasonable: {' yes' if conversion_reasonable else ' no'} ({conv_rate_display:.1f}%)\")\n",
    "\n",
    "    # File size check with error handling\n",
    "    try:\n",
    "        if 'output_paths' in globals() and output_paths and 'ctb_path' in output_paths and 'btb_path' in output_paths:\n",
    "            total_size = sum(os.path.getsize(output_paths[key]) for key in ['ctb_path', 'btb_path'] if key in output_paths and os.path.exists(output_paths[key]))\n",
    "            file_sizes_ok = total_size < 500 * 1024 * 1024  # 500MB limit\n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "            log(f\"   File sizes appropriate: {' yes' if file_sizes_ok else ' large'} ({total_size_mb:.1f} MB total)\")\n",
    "        else:\n",
    "            log(f\"   File sizes appropriate:  unknown\")\n",
    "            file_sizes_ok = True  # Assume OK if we can't check\n",
    "    except Exception as size_error:\n",
    "        log(f\"   File sizes appropriate:  error checking ({size_error})\")\n",
    "        file_sizes_ok = True\n",
    "\n",
    "    # Overall quality assessment\n",
    "    quality_checks = [matrices_generated, temporal_valid, conversion_reasonable, file_sizes_ok]\n",
    "    quality_score = sum(quality_checks)\n",
    "\n",
    "    if quality_score >= 3:\n",
    "        overall_quality = \" EXCELLENT\"\n",
    "    elif quality_score >= 2:\n",
    "        overall_quality = \" GOOD\"\n",
    "    elif quality_score >= 1:\n",
    "        overall_quality = \"  ACCEPTABLE\"\n",
    "    else:\n",
    "        overall_quality = \" NEEDS REVIEW\"\n",
    "\n",
    "    log(f\"\\n Overall Quality: {overall_quality} ({quality_score}/4 criteria met)\")\n",
    "\n",
    "except Exception as quality_error:\n",
    "    log(f\"  Error in quality assessment: {quality_error}\")\n",
    "    log(f\" Overall Quality:  Unable to assess due to errors\")\n",
    "\n",
    "# Performance metrics\n",
    "try:\n",
    "    total_generation_time = ctb_generation_time + btb_generation_time\n",
    "    if total_generation_time > 0:\n",
    "        log(f\"\\n  PERFORMANCE METRICS:\")\n",
    "        log(f\"   Total generation time: {total_generation_time:.1f} seconds ({total_generation_time/60:.1f} minutes)\")\n",
    "        log(f\"   Click-to-buy time: {ctb_generation_time:.1f} seconds\")\n",
    "        log(f\"   Buy-to-buy time: {btb_generation_time:.1f} seconds\")\n",
    "\n",
    "        # Processing speed estimate\n",
    "        try:\n",
    "            total_sessions_processed = validation_results.get(\"mixed_sessions\", 0)\n",
    "            if total_sessions_processed > 0:\n",
    "                sessions_per_second = total_sessions_processed / total_generation_time\n",
    "                log(f\"   Processing speed: ~{sessions_per_second:.1f} sessions/second\")\n",
    "        except Exception as speed_error:\n",
    "            log(f\"   Processing speed: Unable to calculate\")\n",
    "\n",
    "except Exception as perf_error:\n",
    "    log(f\"  Error calculating performance metrics: {perf_error}\")\n",
    "\n",
    "# Next steps recommendations\n",
    "log(f\"\\n RECOMMENDATIONS:\")\n",
    "try:\n",
    "    if both_matrices_successful:\n",
    "        log(f\"    Matrix generation completed successfully!\")\n",
    "        log(f\"    Ready to proceed to Part 2A4: Matrix Consolidation & Validation\")\n",
    "        log(f\"    Both matrices have good quality and are suitable for recommendations\")\n",
    "    else:\n",
    "        log(f\"     Matrix generation incomplete:\")\n",
    "        if click_to_buy_success:\n",
    "            log(f\"       Click-to-buy matrix: Success\")\n",
    "        else:\n",
    "            log(f\"       Click-to-buy matrix: Failed\")\n",
    "        if buy_to_buy_success:\n",
    "            log(f\"       Buy-to-buy matrix: Success\")\n",
    "        else:\n",
    "            log(f\"       Buy-to-buy matrix: Failed\")\n",
    "        log(f\"    Review logs for error details and retry if needed\")\n",
    "\n",
    "    # Memory optimization suggestions\n",
    "    try:\n",
    "        if 'memory_log' in globals() and memory_log:\n",
    "            peak_memory = max([entry.get(\"memory_percent\", 0) for entry in memory_log])\n",
    "            if peak_memory > 85:\n",
    "                log(f\"    Memory usage was high ({peak_memory:.1f}% peak)\")\n",
    "                log(f\"      Consider reducing chunk sizes for future runs\")\n",
    "            elif peak_memory > 0:\n",
    "                log(f\"    Memory usage was acceptable ({peak_memory:.1f}% peak)\")\n",
    "        else:\n",
    "            log(f\"    Memory usage: No data available\")\n",
    "    except Exception as memory_rec_error:\n",
    "        log(f\"    Memory usage: Unable to analyze\")\n",
    "\n",
    "except Exception as rec_error:\n",
    "    log(f\"  Error generating recommendations: {rec_error}\")\n",
    "\n",
    "# Final cleanup\n",
    "log(f\"\\n PERFORMING FINAL CLEANUP...\")\n",
    "cleanup_items = []\n",
    "\n",
    "try:\n",
    "    # Clean up any remaining large objects\n",
    "    items_to_cleanup = ['click_to_buy_matrix', 'buy_to_buy_matrix', 'click_to_buy_generator', 'buy_to_buy_generator']\n",
    "\n",
    "    for item in items_to_cleanup:\n",
    "        if item in globals():\n",
    "            try:\n",
    "                item_obj = globals()[item]\n",
    "                item_size = len(item_obj) if hasattr(item_obj, '__len__') else 'unknown'\n",
    "                del globals()[item]\n",
    "                cleanup_items.append(f\"{item} ({item_size} items)\" if item_size != 'unknown' else item)\n",
    "            except Exception as item_cleanup_error:\n",
    "                cleanup_items.append(f\"{item} (cleanup error)\")\n",
    "\n",
    "    # Force garbage collection\n",
    "    for i in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "    if cleanup_items:\n",
    "        log(f\"     Cleaned up: {', '.join(cleanup_items)}\")\n",
    "\n",
    "    final_memory_status = check_memory()\n",
    "    log(f\"    Final memory status: {final_memory_status}\")\n",
    "\n",
    "except Exception as final_cleanup_error:\n",
    "    log(f\"    Cleanup error: {final_cleanup_error}\")\n",
    "\n",
    "log(f\"\\n Part 2A3 processing completed!\")\n",
    "log(f\" Check all output files in: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Save final execution summary\n",
    "try:\n",
    "    execution_summary = {\n",
    "        \"notebook\": \"Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\",\n",
    "        \"completion_timestamp\": datetime.now().isoformat(),\n",
    "        \"success_status\": {\n",
    "            \"click_to_buy_successful\": click_to_buy_success,\n",
    "            \"buy_to_buy_successful\": buy_to_buy_success,\n",
    "            \"both_successful\": click_to_buy_success and buy_to_buy_success\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"total_generation_time_seconds\": total_generation_time if 'total_generation_time' in locals() else 0,\n",
    "            \"peak_memory_percent\": peak_memory if 'peak_memory' in locals() else 0\n",
    "        },\n",
    "        \"quality_score\": quality_score if 'quality_score' in locals() else 0,\n",
    "        \"next_step\": \"Part 2A4: Matrix Consolidation & Validation\" if both_matrices_successful else \"Review errors and retry\"\n",
    "    }\n",
    "\n",
    "    with open(f\"{config.OUTPUT_PATH}/part_2a3_execution_summary.json\", \"w\") as f:\n",
    "        json.dump(execution_summary, f, indent=2, default=str)\n",
    "\n",
    "    log(f\" Execution summary saved: part_2a3_execution_summary.json\")\n",
    "\n",
    "except Exception as summary_save_error:\n",
    "    log(f\"  Could not save execution summary: {summary_save_error}\")\n",
    "\n",
    "log(f\" Part 2A3 finished!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
