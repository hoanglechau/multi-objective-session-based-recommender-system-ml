{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFcl6E3I7kxk"
   },
   "source": [
    "# Part 2A2 Click to Click Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCLkEvu56GTL",
    "outputId": "870b3c09-1f9c-4bb7-e15b-6235566509da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars==0.20.31 in /usr/local/lib/python3.11/dist-packages (0.20.31)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install polars==0.20.31\n",
    "!pip install psutil\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIejG3u87xgg",
    "outputId": "7fe0405d-d121-4bac-b5c9-18536e258a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k_PeGtxX7y78"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output'\n",
    "\n",
    "    # Matrix generation parameters\n",
    "    MAX_CANDIDATES_PER_ITEM = 40      # Maximum candidates to store per source item\n",
    "    MEMORY_CHECK_INTERVAL = 50        # Check memory every N chunks\n",
    "    EMERGENCY_MEMORY_THRESHOLD = 90   # Emergency stop if memory > 90%\n",
    "    TIME_WINDOW_HOURS = 24           # Click co-visitation time window\n",
    "\n",
    "    # Safety parameters\n",
    "    MAX_PROCESSING_TIME_HOURS = 3    # Maximum processing time before timeout\n",
    "    AUTO_SAVE_INTERVAL = 1000        # Auto-save every N chunks\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVce163-71Xb"
   },
   "source": [
    "## LOGGING AND MONITORING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B61HHbVE73-6",
    "outputId": "590e6105-a1b6-4022-ea89-fa413e66880f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 16:13:19] ================================================================================\n",
      "[2025-08-07 16:13:19] OTTO PART 2A2: CLICK-TO-CLICK MATRIX GENERATION STARTED\n",
      "[2025-08-07 16:13:19] ================================================================================\n",
      "[2025-08-07 16:13:19] Initial memory status: LOW\n"
     ]
    }
   ],
   "source": [
    "def setup_logging_and_monitoring():\n",
    "    \"\"\"Setup comprehensive logging and memory monitoring\"\"\"\n",
    "    log_file = f\"{config.OUTPUT_PATH}/click_matrix_generation_log.txt\"\n",
    "    memory_log = []\n",
    "\n",
    "    def log_message(message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        print(log_entry)\n",
    "\n",
    "        # Also write to file\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_entry + \"\\n\")\n",
    "\n",
    "    def check_memory_usage():\n",
    "        \"\"\"Check current memory usage and log critical levels\"\"\"\n",
    "        memory = psutil.virtual_memory()\n",
    "        memory_pct = memory.percent\n",
    "        available_gb = memory.available / (1024**3)\n",
    "\n",
    "        memory_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"memory_percent\": memory_pct,\n",
    "            \"available_gb\": available_gb,\n",
    "            \"used_gb\": memory.used / (1024**3)\n",
    "        }\n",
    "        memory_log.append(memory_entry)\n",
    "\n",
    "        if memory_pct > config.EMERGENCY_MEMORY_THRESHOLD:\n",
    "            log_message(f\"CRITICAL MEMORY WARNING: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"CRITICAL\"\n",
    "        elif memory_pct > 80:\n",
    "            log_message(f\"HIGH MEMORY USAGE: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"HIGH\"\n",
    "        elif memory_pct > 60:\n",
    "            log_message(f\"Memory usage: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "            return \"NORMAL\"\n",
    "        else:\n",
    "            return \"LOW\"\n",
    "\n",
    "    return log_message, check_memory_usage, memory_log\n",
    "\n",
    "log, check_memory, memory_log = setup_logging_and_monitoring()\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"OTTO PART 2A2: CLICK-TO-CLICK MATRIX GENERATION STARTED\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Initial memory check\n",
    "initial_memory_status = check_memory()\n",
    "log(f\"Initial memory status: {initial_memory_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA6GegvR77Vw"
   },
   "source": [
    "## INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnHLKxCp79lE",
    "outputId": "c8091694-6d1a-4d09-ba12-98f2b3d0ffab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 16:13:19] Validating and loading inputs from Part 2A1...\n",
      "[2025-08-07 16:13:19] covisit_data_prepared.parquet - 1605.4 MB\n",
      "[2025-08-07 16:13:19] chunking_strategy.json - 0.0 MB\n",
      "[2025-08-07 16:13:19] session_analysis.json - 0.0 MB\n",
      "[2025-08-07 16:13:19] All required input files found!\n",
      "[2025-08-07 16:13:19] \n",
      "Loading input data...\n",
      "[2025-08-07 16:13:19]    Loading optimized training data...\n",
      "[2025-08-07 16:13:32]    Prepared data: (216384937, 7) (5159.0 MB)\n",
      "[2025-08-07 16:13:32]    Loading chunking strategy...\n",
      "[2025-08-07 16:13:32]    Chunking strategy loaded\n",
      "[2025-08-07 16:13:32]    Loading session analysis...\n",
      "[2025-08-07 16:13:32]    Session analysis loaded\n",
      "[2025-08-07 16:13:32]    Validating data for click matrix generation...\n",
      "[2025-08-07 16:13:39]       Click events validation:\n",
      "[2025-08-07 16:13:39]       Total click events: 194,625,054\n",
      "[2025-08-07 16:13:39]       Sessions with clicks: 12,899,779\n",
      "[2025-08-07 16:13:39]       Avg clicks per session: 15.1\n",
      "[2025-08-07 16:13:39]    Using chunk size: 50,000 sessions\n",
      "[2025-08-07 16:13:39] Input validation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def validate_and_load_inputs():\n",
    "    \"\"\"\n",
    "    Validate and load all required inputs from Part 2A1\n",
    "\n",
    "    Returns:\n",
    "        tuple: (prepared_data, chunking_strategy, session_analysis, validation_results)\n",
    "    \"\"\"\n",
    "    log(\"Validating and loading inputs from Part 2A1...\")\n",
    "\n",
    "    # Required input files\n",
    "    required_files = {\n",
    "        \"covisit_data_prepared.parquet\": \"Optimized training data from Part 2A1\",\n",
    "        \"chunking_strategy.json\": \"Memory management configuration from Part 2A1\",\n",
    "        \"session_analysis.json\": \"Session analysis results from Part 2A1\"\n",
    "    }\n",
    "\n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            missing_files.append(f\"{filename} - {description}\")\n",
    "        else:\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "            log(f\"{filename} - {file_size:.1f} MB\")\n",
    "\n",
    "    if missing_files:\n",
    "        log(\"MISSING REQUIRED INPUT FILES:\")\n",
    "        for missing in missing_files:\n",
    "            log(f\"   {missing}\")\n",
    "        log(\"\\nTO FIX THIS:\")\n",
    "        log(\"   Run Part 2A1 (Data Preparation & Session Analysis) to generate required files\")\n",
    "        raise FileNotFoundError(\"Required input files are missing!\")\n",
    "\n",
    "    log(\"All required input files found!\")\n",
    "\n",
    "    # Load data\n",
    "    log(\"\\nLoading input data...\")\n",
    "\n",
    "    try:\n",
    "        # Load prepared data\n",
    "        log(\"   Loading optimized training data...\")\n",
    "        prepared_data = pl.read_parquet(f\"{config.OUTPUT_PATH}/covisit_data_prepared.parquet\")\n",
    "        log(f\"   Prepared data: {prepared_data.shape} ({prepared_data.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "        # Load chunking strategy\n",
    "        log(\"   Loading chunking strategy...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/chunking_strategy.json\", \"r\") as f:\n",
    "            chunking_strategy = json.load(f)\n",
    "        log(f\"   Chunking strategy loaded\")\n",
    "\n",
    "        # Load session analysis\n",
    "        log(\"   Loading session analysis...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/session_analysis.json\", \"r\") as f:\n",
    "            session_analysis = json.load(f)\n",
    "        log(f\"   Session analysis loaded\")\n",
    "\n",
    "        # Validate data for click matrix generation\n",
    "        log(\"   Validating data for click matrix generation...\")\n",
    "\n",
    "        # Check for click events\n",
    "        click_events = prepared_data.filter(pl.col(\"type\") == \"clicks\")\n",
    "        click_sessions = click_events.select(\"session\").n_unique()\n",
    "        total_clicks = len(click_events)\n",
    "\n",
    "        log(f\"      Click events validation:\")\n",
    "        log(f\"      Total click events: {total_clicks:,}\")\n",
    "        log(f\"      Sessions with clicks: {click_sessions:,}\")\n",
    "        log(f\"      Avg clicks per session: {total_clicks / click_sessions:.1f}\")\n",
    "\n",
    "        if total_clicks == 0:\n",
    "            raise ValueError(\"No click events found in prepared data!\")\n",
    "\n",
    "        # Check chunking strategy for click-to-click\n",
    "        if \"click_to_click\" not in chunking_strategy.get(\"chunk_sizes\", {}):\n",
    "            log(\"   No click-to-click chunk size in strategy, using default\")\n",
    "            chunk_size = 10000\n",
    "        else:\n",
    "            chunk_size = chunking_strategy[\"chunk_sizes\"][\"click_to_click\"]\n",
    "\n",
    "        log(f\"   Using chunk size: {chunk_size:,} sessions\")\n",
    "\n",
    "        validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_click_events\": total_clicks,\n",
    "            \"click_sessions\": click_sessions,\n",
    "            \"avg_clicks_per_session\": total_clicks / click_sessions,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"memory_pressure\": chunking_strategy.get(\"memory_pressure\", \"UNKNOWN\")\n",
    "        }\n",
    "\n",
    "        log(\"Input validation completed successfully!\")\n",
    "        return prepared_data, chunking_strategy, session_analysis, validation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error loading input data: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Load and validate inputs\n",
    "prepared_data, chunking_strategy, session_analysis, validation_results = validate_and_load_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSRCTVmn8Ea6"
   },
   "source": [
    "## CLICK-TO-CLICK MATRIX GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CfEbTlvA8H1V"
   },
   "outputs": [],
   "source": [
    "class ClickToClickMatrixGenerator:\n",
    "    \"\"\"\n",
    "    Memory-optimized click-to-click co-visitation matrix generator\n",
    "    Uses progressive saving and memory management to handle large datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.covisitation_counts = {}\n",
    "        self.processed_sessions = 0\n",
    "        self.processed_chunks = 0\n",
    "        self.start_time = time.time()\n",
    "        self.last_save_time = time.time()\n",
    "        self.last_memory_cleanup = time.time()\n",
    "\n",
    "        # Memory management settings\n",
    "        self.max_pairs_in_memory = 5000000  # Max 5M pairs before cleanup\n",
    "        self.cleanup_threshold_mb = 8000    # Cleanup if memory > 8GB\n",
    "        self.save_interval_chunks = 25      # Save every 25 chunks\n",
    "\n",
    "        log(\"    Initializing memory-efficient click-to-click matrix generator...\")\n",
    "        log(f\"   Chunk size: {self.chunk_size:,} sessions\")\n",
    "        log(f\"   Max candidates per item: {config.MAX_CANDIDATES_PER_ITEM}\")\n",
    "        log(f\"   Time window: {config.TIME_WINDOW_HOURS} hours\")\n",
    "        log(f\"   Memory management: {self.max_pairs_in_memory:,} max pairs, cleanup every {self.save_interval_chunks} chunks\")\n",
    "\n",
    "    def process_session_chunk(self, session_chunk: List[int], click_data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a chunk of sessions with memory-efficient pair generation\n",
    "        \"\"\"\n",
    "        chunk_start_time = time.time()\n",
    "        chunk_covisitations = {}\n",
    "\n",
    "        # Filter data for this chunk\n",
    "        chunk_data = click_data.filter(pl.col(\"session\").is_in(session_chunk))\n",
    "\n",
    "        if len(chunk_data) == 0:\n",
    "            return chunk_covisitations\n",
    "\n",
    "        # Process each session with limits\n",
    "        session_groups = chunk_data.group_by(\"session\").agg([\n",
    "            pl.col(\"aid\").alias(\"aids\"),\n",
    "            pl.col(\"ts\").alias(\"timestamps\")\n",
    "        ])\n",
    "\n",
    "        for row in session_groups.iter_rows():\n",
    "            session_id, aids, timestamps = row\n",
    "\n",
    "            if len(aids) < 2:  # Need at least 2 items\n",
    "                continue\n",
    "\n",
    "            # Limit session size to prevent memory explosion\n",
    "            if len(aids) > 100:  # Skip very long sessions to save memory\n",
    "                continue\n",
    "\n",
    "            # Create aid-timestamp pairs\n",
    "            aid_ts_pairs = list(zip(aids, timestamps))\n",
    "            time_window_ms = config.TIME_WINDOW_HOURS * 60 * 60 * 1000\n",
    "\n",
    "            # Generate co-visitation pairs (with limits)\n",
    "            pairs_in_session = 0\n",
    "            max_pairs_per_session = 1000  # Limit pairs per session\n",
    "\n",
    "            for i in range(len(aid_ts_pairs)):\n",
    "                for j in range(i + 1, len(aid_ts_pairs)):\n",
    "                    if pairs_in_session >= max_pairs_per_session:\n",
    "                        break\n",
    "\n",
    "                    aid1, ts1 = aid_ts_pairs[i]\n",
    "                    aid2, ts2 = aid_ts_pairs[j]\n",
    "\n",
    "                    # Check if within time window\n",
    "                    if abs(ts2 - ts1) <= time_window_ms and aid1 != aid2:\n",
    "                        # Add both directions but with limits\n",
    "                        chunk_covisitations[(aid1, aid2)] = chunk_covisitations.get((aid1, aid2), 0) + 1\n",
    "                        chunk_covisitations[(aid2, aid1)] = chunk_covisitations.get((aid2, aid1), 0) + 1\n",
    "                        pairs_in_session += 2\n",
    "\n",
    "                if pairs_in_session >= max_pairs_per_session:\n",
    "                    break\n",
    "\n",
    "        return chunk_covisitations\n",
    "\n",
    "    def merge_chunk_results(self, chunk_covisitations: Dict):\n",
    "        \"\"\"\n",
    "        Merge chunk results with memory management\n",
    "        \"\"\"\n",
    "        for (aid1, aid2), count in chunk_covisitations.items():\n",
    "            if aid1 not in self.covisitation_counts:\n",
    "                self.covisitation_counts[aid1] = {}\n",
    "\n",
    "            self.covisitation_counts[aid1][aid2] = self.covisitation_counts[aid1].get(aid2, 0) + count\n",
    "\n",
    "    def estimate_memory_usage(self) -> float:\n",
    "        \"\"\"Estimate current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            total_pairs = sum(len(targets) for targets in self.covisitation_counts.values())\n",
    "            # Rough estimate: each pair takes ~50 bytes (item IDs + count + overhead)\n",
    "            estimated_mb = total_pairs * 50 / (1024 * 1024)\n",
    "            return estimated_mb\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def cleanup_memory(self, force_aggressive: bool = False):\n",
    "        \"\"\"\n",
    "        Clean up memory by keeping only top candidates\n",
    "        \"\"\"\n",
    "        log(f\"   Performing memory cleanup (aggressive={force_aggressive})...\")\n",
    "\n",
    "        before_size = len(self.covisitation_counts)\n",
    "        before_memory = self.estimate_memory_usage()\n",
    "\n",
    "        # Keep only top candidates per source item\n",
    "        max_candidates = config.MAX_CANDIDATES_PER_ITEM if not force_aggressive else 20\n",
    "\n",
    "        cleaned_counts = {}\n",
    "        for source_aid, targets in self.covisitation_counts.items():\n",
    "            if len(targets) > max_candidates:\n",
    "                # Keep only top candidates\n",
    "                sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                top_targets = dict(sorted_targets[:max_candidates])\n",
    "                cleaned_counts[source_aid] = top_targets\n",
    "            else:\n",
    "                cleaned_counts[source_aid] = targets\n",
    "\n",
    "        self.covisitation_counts = cleaned_counts\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        after_memory = self.estimate_memory_usage()\n",
    "        log(f\"   Memory cleanup completed: {before_memory:.1f}MB â†’ {after_memory:.1f}MB\")\n",
    "        self.last_memory_cleanup = time.time()\n",
    "\n",
    "    def save_intermediate_results(self, force_save: bool = False):\n",
    "        \"\"\"\n",
    "        Save intermediate results and clean up memory\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        if force_save or (self.processed_chunks % self.save_interval_chunks == 0 and self.processed_chunks > 0):\n",
    "            log(f\"   Saving intermediate results (chunk {self.processed_chunks})...\")\n",
    "\n",
    "            try:\n",
    "                temp_path = f\"{config.OUTPUT_PATH}/click_matrix_temp_chunk_{self.processed_chunks}.pkl\"\n",
    "\n",
    "                # Convert to final format before saving\n",
    "                temp_matrix = {}\n",
    "                for source_aid, targets in self.covisitation_counts.items():\n",
    "                    if targets:\n",
    "                        sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                        top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                        temp_matrix[source_aid] = top_targets\n",
    "\n",
    "                with open(temp_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"partial_matrix\": temp_matrix,\n",
    "                        \"processed_sessions\": self.processed_sessions,\n",
    "                        \"processed_chunks\": self.processed_chunks,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }, f)\n",
    "\n",
    "                file_size = os.path.getsize(temp_path) / (1024*1024)\n",
    "                log(f\"   Intermediate results saved: {temp_path} ({file_size:.1f} MB)\")\n",
    "                self.last_save_time = current_time\n",
    "\n",
    "                # Clear memory after saving\n",
    "                self.covisitation_counts.clear()\n",
    "                gc.collect()\n",
    "                log(f\"   Memory cleared after save\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"   Failed to save intermediate results: {e}\")\n",
    "\n",
    "    def generate_matrix(self, click_data: pl.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate complete click-to-click co-visitation matrix with memory management\n",
    "        \"\"\"\n",
    "        log(\"Starting memory-efficient click-to-click matrix generation...\")\n",
    "\n",
    "        # Get unique sessions with clicks\n",
    "        click_sessions = click_data.select(\"session\").unique().sort(\"session\")\n",
    "        total_sessions = len(click_sessions)\n",
    "\n",
    "        log(f\"   Processing {total_sessions:,} sessions with clicks\")\n",
    "        log(f\"   Using {self.chunk_size:,} sessions per chunk\")\n",
    "\n",
    "        num_chunks = (total_sessions + self.chunk_size - 1) // self.chunk_size\n",
    "        log(f\"   Total chunks to process: {num_chunks}\")\n",
    "\n",
    "        # Process sessions in chunks\n",
    "        session_list = click_sessions[\"session\"].to_list()\n",
    "        intermediate_files = []\n",
    "\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            # Check processing time limit\n",
    "            total_elapsed = time.time() - self.start_time\n",
    "            if total_elapsed > (config.MAX_PROCESSING_TIME_HOURS * 3600):\n",
    "                log(f\"PROCESSING TIME LIMIT REACHED ({config.MAX_PROCESSING_TIME_HOURS} hours)\")\n",
    "                break\n",
    "\n",
    "            # Memory check\n",
    "            if chunk_idx % config.MEMORY_CHECK_INTERVAL == 0:\n",
    "                memory_status = check_memory()\n",
    "                if memory_status == \"CRITICAL\":\n",
    "                    log(f\"CRITICAL MEMORY USAGE - EMERGENCY SAVE AND CLEANUP\")\n",
    "                    self.save_intermediate_results(force_save=True)\n",
    "                    break\n",
    "\n",
    "            # Get session chunk\n",
    "            start_idx = chunk_idx * self.chunk_size\n",
    "            end_idx = min(start_idx + self.chunk_size, total_sessions)\n",
    "            session_chunk = session_list[start_idx:end_idx]\n",
    "\n",
    "            log(f\"   Processing chunk {chunk_idx + 1}/{num_chunks} ({end_idx/total_sessions*100:.1f}%)\")\n",
    "\n",
    "            try:\n",
    "                # Process chunk\n",
    "                chunk_covisitations = self.process_session_chunk(session_chunk, click_data)\n",
    "                self.merge_chunk_results(chunk_covisitations)\n",
    "\n",
    "                # Update counters\n",
    "                self.processed_sessions += len(session_chunk)\n",
    "                self.processed_chunks += 1\n",
    "\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                pairs_found = len(chunk_covisitations)\n",
    "                current_memory = self.estimate_memory_usage()\n",
    "\n",
    "                log(f\"      Chunk completed: {pairs_found:,} pairs, {chunk_time:.1f}s, ~{current_memory:.0f}MB\")\n",
    "\n",
    "                # Memory management\n",
    "                if (current_memory > self.cleanup_threshold_mb or\n",
    "                    self.processed_chunks % self.save_interval_chunks == 0):\n",
    "                    self.save_intermediate_results()\n",
    "\n",
    "                # Cleanup\n",
    "                del chunk_covisitations\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"      Error processing chunk {chunk_idx + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Final processing - collect all intermediate results\n",
    "        log(\"   Collecting and merging intermediate results...\")\n",
    "        final_matrix = self._merge_intermediate_results()\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "        log(f\"Click-to-click matrix generation completed!\")\n",
    "        log(f\"   Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "        log(f\"   Processed sessions: {self.processed_sessions:,}\")\n",
    "        log(f\"   Final matrix size: {len(final_matrix):,} source items\")\n",
    "\n",
    "        return final_matrix\n",
    "\n",
    "    def _merge_intermediate_results(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Merge all intermediate result files into final matrix\n",
    "        \"\"\"\n",
    "        log(\"   Merging intermediate results...\")\n",
    "\n",
    "        final_matrix = {}\n",
    "        temp_files = []\n",
    "\n",
    "        # Find all temporary files\n",
    "        import glob\n",
    "        temp_pattern = f\"{config.OUTPUT_PATH}/click_matrix_temp_chunk_*.pkl\"\n",
    "        temp_files = glob.glob(temp_pattern)\n",
    "\n",
    "        log(f\"   Found {len(temp_files)} intermediate files to merge\")\n",
    "\n",
    "        # Merge current memory state\n",
    "        if self.covisitation_counts:\n",
    "            log(\"   Adding current memory state to final matrix...\")\n",
    "            for source_aid, targets in self.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    top_targets = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    final_matrix[source_aid] = top_targets\n",
    "\n",
    "        # Merge intermediate files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                log(f\"   Merging {os.path.basename(temp_file)}...\")\n",
    "                with open(temp_file, \"rb\") as f:\n",
    "                    temp_data = pickle.load(f)\n",
    "                    temp_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "\n",
    "                # Merge into final matrix\n",
    "                for source_aid, candidates in temp_matrix.items():\n",
    "                    if source_aid in final_matrix:\n",
    "                        # Merge candidates\n",
    "                        existing_dict = dict(final_matrix[source_aid])\n",
    "                        for target_aid, score in candidates:\n",
    "                            existing_dict[target_aid] = existing_dict.get(target_aid, 0) + score\n",
    "\n",
    "                        # Keep top candidates\n",
    "                        sorted_candidates = sorted(existing_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                        final_matrix[source_aid] = sorted_candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "                    else:\n",
    "                        final_matrix[source_aid] = candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "\n",
    "                # Clean up temp file\n",
    "                os.remove(temp_file)\n",
    "                log(f\"   Cleaned up {os.path.basename(temp_file)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"   Error processing {temp_file}: {e}\")\n",
    "\n",
    "        log(f\"   Final matrix merged: {len(final_matrix):,} source items\")\n",
    "        return final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK6mZrbS8Nfb"
   },
   "source": [
    "## MATRIX GENERATION EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_GUi1wn8PH_",
    "outputId": "a7cd0fbe-252e-4eb1-b826-c8e16c1745c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 16:13:39] \n",
      "Preparing click data for matrix generation...\n",
      "[2025-08-07 16:13:39] Initial memory status: LOW\n",
      "[2025-08-07 16:13:39]    Filtering and optimizing click data...\n",
      "[2025-08-07 16:13:50] Click data prepared: (194625054, 3) (2969.7 MB)\n",
      "[2025-08-07 16:13:50]    Original prepared data cleared from memory\n",
      "[2025-08-07 16:13:50] Memory after cleanup: LOW\n",
      "[2025-08-07 16:13:50] \n",
      "Initializing memory-efficient matrix generator...\n",
      "[2025-08-07 16:13:50]     Initializing memory-efficient click-to-click matrix generator...\n",
      "[2025-08-07 16:13:50]    Chunk size: 50,000 sessions\n",
      "[2025-08-07 16:13:50]    Max candidates per item: 40\n",
      "[2025-08-07 16:13:50]    Time window: 24 hours\n",
      "[2025-08-07 16:13:50]    Memory management: 5,000,000 max pairs, cleanup every 25 chunks\n",
      "[2025-08-07 16:13:50] Pre-generation memory status: LOW\n",
      "[2025-08-07 16:13:50] Starting matrix generation with memory management...\n",
      "[2025-08-07 16:13:50] Starting memory-efficient click-to-click matrix generation...\n",
      "[2025-08-07 16:13:50]    Processing 12,899,779 sessions with clicks\n",
      "[2025-08-07 16:13:50]    Using 50,000 sessions per chunk\n",
      "[2025-08-07 16:13:50]    Total chunks to process: 258\n",
      "[2025-08-07 16:13:51]    Processing chunk 1/258 (0.4%)\n",
      "[2025-08-07 16:14:05]       Chunk completed: 4,822,064 pairs, 14.3s, ~230MB\n",
      "[2025-08-07 16:14:06]    Processing chunk 2/258 (0.8%)\n",
      "[2025-08-07 16:14:20]       Chunk completed: 4,860,604 pairs, 14.7s, ~449MB\n",
      "[2025-08-07 16:14:21]    Processing chunk 3/258 (1.2%)\n",
      "[2025-08-07 16:14:36]       Chunk completed: 4,991,346 pairs, 14.9s, ~669MB\n",
      "[2025-08-07 16:14:36]    Processing chunk 4/258 (1.6%)\n",
      "[2025-08-07 16:14:51]       Chunk completed: 4,867,316 pairs, 14.7s, ~878MB\n",
      "[2025-08-07 16:14:51]    Processing chunk 5/258 (1.9%)\n",
      "[2025-08-07 16:15:06]       Chunk completed: 4,737,134 pairs, 14.6s, ~1078MB\n",
      "[2025-08-07 16:15:06]    Processing chunk 6/258 (2.3%)\n",
      "[2025-08-07 16:15:21]       Chunk completed: 4,598,078 pairs, 14.4s, ~1269MB\n",
      "[2025-08-07 16:15:21]    Processing chunk 7/258 (2.7%)\n",
      "[2025-08-07 16:15:35]       Chunk completed: 4,489,164 pairs, 14.1s, ~1452MB\n",
      "[2025-08-07 16:15:36]    Processing chunk 8/258 (3.1%)\n",
      "[2025-08-07 16:15:50]       Chunk completed: 4,451,794 pairs, 14.0s, ~1631MB\n",
      "[2025-08-07 16:15:50]    Processing chunk 9/258 (3.5%)\n",
      "[2025-08-07 16:16:04]       Chunk completed: 4,384,944 pairs, 13.8s, ~1805MB\n",
      "[2025-08-07 16:16:04]    Processing chunk 10/258 (3.9%)\n",
      "[2025-08-07 16:16:18]       Chunk completed: 4,387,218 pairs, 13.5s, ~1978MB\n",
      "[2025-08-07 16:16:18]    Processing chunk 11/258 (4.3%)\n",
      "[2025-08-07 16:16:32]       Chunk completed: 4,416,934 pairs, 13.6s, ~2150MB\n",
      "[2025-08-07 16:16:32]    Processing chunk 12/258 (4.7%)\n",
      "[2025-08-07 16:16:46]       Chunk completed: 4,335,318 pairs, 13.3s, ~2318MB\n",
      "[2025-08-07 16:16:46]    Processing chunk 13/258 (5.0%)\n",
      "[2025-08-07 16:17:00]       Chunk completed: 4,333,488 pairs, 13.5s, ~2484MB\n",
      "[2025-08-07 16:17:00]    Processing chunk 14/258 (5.4%)\n",
      "[2025-08-07 16:17:14]       Chunk completed: 4,320,876 pairs, 13.7s, ~2648MB\n",
      "[2025-08-07 16:17:14]    Processing chunk 15/258 (5.8%)\n",
      "[2025-08-07 16:17:28]       Chunk completed: 4,266,488 pairs, 13.7s, ~2809MB\n",
      "[2025-08-07 16:17:28]    Processing chunk 16/258 (6.2%)\n",
      "[2025-08-07 16:17:42]       Chunk completed: 4,285,134 pairs, 13.8s, ~2969MB\n",
      "[2025-08-07 16:17:43]    Processing chunk 17/258 (6.6%)\n",
      "[2025-08-07 16:17:56]       Chunk completed: 4,282,616 pairs, 13.6s, ~3128MB\n",
      "[2025-08-07 16:17:57]    Processing chunk 18/258 (7.0%)\n",
      "[2025-08-07 16:18:11]       Chunk completed: 4,297,562 pairs, 13.8s, ~3287MB\n",
      "[2025-08-07 16:18:11]    Processing chunk 19/258 (7.4%)\n",
      "[2025-08-07 16:18:25]       Chunk completed: 4,341,042 pairs, 13.7s, ~3446MB\n",
      "[2025-08-07 16:18:25]    Processing chunk 20/258 (7.8%)\n",
      "[2025-08-07 16:18:39]       Chunk completed: 4,322,332 pairs, 13.5s, ~3605MB\n",
      "[2025-08-07 16:18:39]    Processing chunk 21/258 (8.1%)\n",
      "[2025-08-07 16:19:04]       Chunk completed: 4,338,976 pairs, 24.2s, ~3763MB\n",
      "[2025-08-07 16:19:04]    Processing chunk 22/258 (8.5%)\n",
      "[2025-08-07 16:19:24]       Chunk completed: 4,194,042 pairs, 19.8s, ~3913MB\n",
      "[2025-08-07 16:19:24]    Processing chunk 23/258 (8.9%)\n",
      "[2025-08-07 16:19:40]       Chunk completed: 4,065,362 pairs, 15.4s, ~4059MB\n",
      "[2025-08-07 16:19:40]    Processing chunk 24/258 (9.3%)\n",
      "[2025-08-07 16:19:54]       Chunk completed: 4,076,908 pairs, 13.5s, ~4204MB\n",
      "[2025-08-07 16:19:54]    Processing chunk 25/258 (9.7%)\n",
      "[2025-08-07 16:20:08]       Chunk completed: 4,170,634 pairs, 13.8s, ~4350MB\n",
      "[2025-08-07 16:20:08]    Saving intermediate results (chunk 25)...\n",
      "[2025-08-07 16:21:06]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_25.pkl (260.9 MB)\n",
      "[2025-08-07 16:21:12]    Memory cleared after save\n",
      "[2025-08-07 16:21:16]    Processing chunk 26/258 (10.1%)\n",
      "[2025-08-07 16:21:29]       Chunk completed: 4,481,696 pairs, 13.0s, ~214MB\n",
      "[2025-08-07 16:21:30]    Processing chunk 27/258 (10.5%)\n",
      "[2025-08-07 16:21:43]       Chunk completed: 4,326,132 pairs, 13.0s, ~410MB\n",
      "[2025-08-07 16:21:43]    Processing chunk 28/258 (10.9%)\n",
      "[2025-08-07 16:21:56]       Chunk completed: 4,131,516 pairs, 12.6s, ~592MB\n",
      "[2025-08-07 16:21:56]    Processing chunk 29/258 (11.2%)\n",
      "[2025-08-07 16:22:09]       Chunk completed: 3,939,996 pairs, 12.4s, ~761MB\n",
      "[2025-08-07 16:22:09]    Processing chunk 30/258 (11.6%)\n",
      "[2025-08-07 16:22:21]       Chunk completed: 3,940,324 pairs, 12.3s, ~927MB\n",
      "[2025-08-07 16:22:22]    Processing chunk 31/258 (12.0%)\n",
      "[2025-08-07 16:22:34]       Chunk completed: 3,831,300 pairs, 11.9s, ~1086MB\n",
      "[2025-08-07 16:22:34]    Processing chunk 32/258 (12.4%)\n",
      "[2025-08-07 16:22:46]       Chunk completed: 3,869,670 pairs, 12.0s, ~1245MB\n",
      "[2025-08-07 16:22:46]    Processing chunk 33/258 (12.8%)\n",
      "[2025-08-07 16:22:58]       Chunk completed: 3,882,372 pairs, 12.1s, ~1402MB\n",
      "[2025-08-07 16:22:59]    Processing chunk 34/258 (13.2%)\n",
      "[2025-08-07 16:23:11]       Chunk completed: 3,834,420 pairs, 12.1s, ~1555MB\n",
      "[2025-08-07 16:23:11]    Processing chunk 35/258 (13.6%)\n",
      "[2025-08-07 16:23:24]       Chunk completed: 3,818,092 pairs, 12.1s, ~1705MB\n",
      "[2025-08-07 16:23:24]    Processing chunk 36/258 (14.0%)\n",
      "[2025-08-07 16:23:36]       Chunk completed: 3,821,150 pairs, 12.2s, ~1854MB\n",
      "[2025-08-07 16:23:37]    Processing chunk 37/258 (14.3%)\n",
      "[2025-08-07 16:23:49]       Chunk completed: 3,819,362 pairs, 12.3s, ~2002MB\n",
      "[2025-08-07 16:23:49]    Processing chunk 38/258 (14.7%)\n",
      "[2025-08-07 16:24:02]       Chunk completed: 3,839,150 pairs, 12.2s, ~2150MB\n",
      "[2025-08-07 16:24:02]    Processing chunk 39/258 (15.1%)\n",
      "[2025-08-07 16:24:14]       Chunk completed: 3,838,058 pairs, 12.4s, ~2296MB\n",
      "[2025-08-07 16:24:15]    Processing chunk 40/258 (15.5%)\n",
      "[2025-08-07 16:24:27]       Chunk completed: 3,775,162 pairs, 12.0s, ~2439MB\n",
      "[2025-08-07 16:24:27]    Processing chunk 41/258 (15.9%)\n",
      "[2025-08-07 16:24:39]       Chunk completed: 3,672,304 pairs, 11.9s, ~2577MB\n",
      "[2025-08-07 16:24:40]    Processing chunk 42/258 (16.3%)\n",
      "[2025-08-07 16:24:51]       Chunk completed: 3,576,414 pairs, 11.8s, ~2710MB\n",
      "[2025-08-07 16:24:52]    Processing chunk 43/258 (16.7%)\n",
      "[2025-08-07 16:25:04]       Chunk completed: 3,619,150 pairs, 11.8s, ~2843MB\n",
      "[2025-08-07 16:25:04]    Processing chunk 44/258 (17.1%)\n",
      "[2025-08-07 16:25:17]       Chunk completed: 3,782,378 pairs, 12.5s, ~2982MB\n",
      "[2025-08-07 16:25:17]    Processing chunk 45/258 (17.4%)\n",
      "[2025-08-07 16:25:30]       Chunk completed: 3,973,032 pairs, 12.8s, ~3128MB\n",
      "[2025-08-07 16:25:30]    Processing chunk 46/258 (17.8%)\n",
      "[2025-08-07 16:25:43]       Chunk completed: 3,696,252 pairs, 12.1s, ~3264MB\n",
      "[2025-08-07 16:25:43]    Processing chunk 47/258 (18.2%)\n",
      "[2025-08-07 16:25:55]       Chunk completed: 3,600,600 pairs, 11.9s, ~3394MB\n",
      "[2025-08-07 16:25:55]    Processing chunk 48/258 (18.6%)\n",
      "[2025-08-07 16:26:07]       Chunk completed: 3,510,224 pairs, 11.8s, ~3521MB\n",
      "[2025-08-07 16:26:08]    Processing chunk 49/258 (19.0%)\n",
      "[2025-08-07 16:26:19]       Chunk completed: 3,500,320 pairs, 11.6s, ~3645MB\n",
      "[2025-08-07 16:26:20]    Processing chunk 50/258 (19.4%)\n",
      "[2025-08-07 16:26:31]       Chunk completed: 3,496,246 pairs, 11.5s, ~3770MB\n",
      "[2025-08-07 16:26:31]    Saving intermediate results (chunk 50)...\n",
      "[2025-08-07 16:27:19]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_50.pkl (241.6 MB)\n",
      "[2025-08-07 16:27:24]    Memory cleared after save\n",
      "[2025-08-07 16:27:28]    Processing chunk 51/258 (19.8%)\n",
      "[2025-08-07 16:27:38]       Chunk completed: 3,567,716 pairs, 10.5s, ~170MB\n",
      "[2025-08-07 16:27:38]    Processing chunk 52/258 (20.2%)\n",
      "[2025-08-07 16:27:49]       Chunk completed: 3,632,672 pairs, 11.0s, ~334MB\n",
      "[2025-08-07 16:27:50]    Processing chunk 53/258 (20.5%)\n",
      "[2025-08-07 16:28:01]       Chunk completed: 3,602,160 pairs, 10.9s, ~493MB\n",
      "[2025-08-07 16:28:01]    Processing chunk 54/258 (20.9%)\n",
      "[2025-08-07 16:28:12]       Chunk completed: 3,529,220 pairs, 10.8s, ~644MB\n",
      "[2025-08-07 16:28:12]    Processing chunk 55/258 (21.3%)\n",
      "[2025-08-07 16:28:23]       Chunk completed: 3,467,856 pairs, 10.7s, ~790MB\n",
      "[2025-08-07 16:28:23]    Processing chunk 56/258 (21.7%)\n",
      "[2025-08-07 16:28:34]       Chunk completed: 3,467,768 pairs, 11.0s, ~933MB\n",
      "[2025-08-07 16:28:35]    Processing chunk 57/258 (22.1%)\n",
      "[2025-08-07 16:28:46]       Chunk completed: 3,424,394 pairs, 10.9s, ~1073MB\n",
      "[2025-08-07 16:28:46]    Processing chunk 58/258 (22.5%)\n",
      "[2025-08-07 16:28:57]       Chunk completed: 3,418,458 pairs, 10.8s, ~1211MB\n",
      "[2025-08-07 16:28:57]    Processing chunk 59/258 (22.9%)\n",
      "[2025-08-07 16:29:08]       Chunk completed: 3,351,482 pairs, 10.7s, ~1344MB\n",
      "[2025-08-07 16:29:08]    Processing chunk 60/258 (23.3%)\n",
      "[2025-08-07 16:29:19]       Chunk completed: 3,279,192 pairs, 10.7s, ~1473MB\n",
      "[2025-08-07 16:29:19]    Processing chunk 61/258 (23.6%)\n",
      "[2025-08-07 16:29:39]       Chunk completed: 3,526,316 pairs, 19.9s, ~1611MB\n",
      "[2025-08-07 16:29:40]    Processing chunk 62/258 (24.0%)\n",
      "[2025-08-07 16:29:57]       Chunk completed: 3,489,966 pairs, 16.8s, ~1747MB\n",
      "[2025-08-07 16:29:57]    Processing chunk 63/258 (24.4%)\n",
      "[2025-08-07 16:30:10]       Chunk completed: 2,824,900 pairs, 13.2s, ~1855MB\n",
      "[2025-08-07 16:30:10]    Processing chunk 64/258 (24.8%)\n",
      "[2025-08-07 16:30:23]       Chunk completed: 3,190,938 pairs, 12.8s, ~1977MB\n",
      "[2025-08-07 16:30:24]    Processing chunk 65/258 (25.2%)\n",
      "[2025-08-07 16:30:34]       Chunk completed: 3,156,270 pairs, 10.7s, ~2097MB\n",
      "[2025-08-07 16:30:35]    Processing chunk 66/258 (25.6%)\n",
      "[2025-08-07 16:30:46]       Chunk completed: 3,194,152 pairs, 10.7s, ~2217MB\n",
      "[2025-08-07 16:30:46]    Processing chunk 67/258 (26.0%)\n",
      "[2025-08-07 16:30:57]       Chunk completed: 3,254,174 pairs, 10.8s, ~2337MB\n",
      "[2025-08-07 16:30:57]    Processing chunk 68/258 (26.4%)\n",
      "[2025-08-07 16:31:08]       Chunk completed: 3,198,532 pairs, 10.6s, ~2455MB\n",
      "[2025-08-07 16:31:08]    Processing chunk 69/258 (26.7%)\n",
      "[2025-08-07 16:31:19]       Chunk completed: 3,177,874 pairs, 10.7s, ~2572MB\n",
      "[2025-08-07 16:31:19]    Processing chunk 70/258 (27.1%)\n",
      "[2025-08-07 16:31:30]       Chunk completed: 3,216,900 pairs, 10.9s, ~2689MB\n",
      "[2025-08-07 16:31:31]    Processing chunk 71/258 (27.5%)\n",
      "[2025-08-07 16:31:42]       Chunk completed: 3,253,426 pairs, 11.0s, ~2808MB\n",
      "[2025-08-07 16:31:42]    Processing chunk 72/258 (27.9%)\n",
      "[2025-08-07 16:31:53]       Chunk completed: 3,227,718 pairs, 10.9s, ~2925MB\n",
      "[2025-08-07 16:31:53]    Processing chunk 73/258 (28.3%)\n",
      "[2025-08-07 16:32:04]       Chunk completed: 3,174,446 pairs, 10.8s, ~3039MB\n",
      "[2025-08-07 16:32:05]    Processing chunk 74/258 (28.7%)\n",
      "[2025-08-07 16:32:15]       Chunk completed: 3,080,890 pairs, 10.6s, ~3148MB\n",
      "[2025-08-07 16:32:16]    Processing chunk 75/258 (29.1%)\n",
      "[2025-08-07 16:32:27]       Chunk completed: 3,197,966 pairs, 10.9s, ~3262MB\n",
      "[2025-08-07 16:32:27]    Saving intermediate results (chunk 75)...\n",
      "[2025-08-07 16:33:12]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_75.pkl (221.8 MB)\n",
      "[2025-08-07 16:33:16]    Memory cleared after save\n",
      "[2025-08-07 16:33:20]    Processing chunk 76/258 (29.5%)\n",
      "[2025-08-07 16:33:30]       Chunk completed: 3,410,612 pairs, 10.1s, ~163MB\n",
      "[2025-08-07 16:33:30]    Processing chunk 77/258 (29.8%)\n",
      "[2025-08-07 16:33:40]       Chunk completed: 3,156,190 pairs, 10.1s, ~306MB\n",
      "[2025-08-07 16:33:41]    Processing chunk 78/258 (30.2%)\n",
      "[2025-08-07 16:33:51]       Chunk completed: 3,161,254 pairs, 10.1s, ~446MB\n",
      "[2025-08-07 16:33:51]    Processing chunk 79/258 (30.6%)\n",
      "[2025-08-07 16:34:01]       Chunk completed: 3,107,318 pairs, 10.2s, ~580MB\n",
      "[2025-08-07 16:34:02]    Processing chunk 80/258 (31.0%)\n",
      "[2025-08-07 16:34:12]       Chunk completed: 3,273,742 pairs, 10.6s, ~718MB\n",
      "[2025-08-07 16:34:13]    Processing chunk 81/258 (31.4%)\n",
      "[2025-08-07 16:34:23]       Chunk completed: 3,312,444 pairs, 10.5s, ~856MB\n",
      "[2025-08-07 16:34:24]    Processing chunk 82/258 (31.8%)\n",
      "[2025-08-07 16:34:34]       Chunk completed: 3,262,026 pairs, 10.4s, ~990MB\n",
      "[2025-08-07 16:34:34]    Processing chunk 83/258 (32.2%)\n",
      "[2025-08-07 16:34:45]       Chunk completed: 3,283,554 pairs, 10.7s, ~1124MB\n",
      "[2025-08-07 16:34:45]    Processing chunk 84/258 (32.6%)\n",
      "[2025-08-07 16:34:56]       Chunk completed: 3,432,442 pairs, 10.8s, ~1263MB\n",
      "[2025-08-07 16:34:57]    Processing chunk 85/258 (32.9%)\n",
      "[2025-08-07 16:35:07]       Chunk completed: 3,328,690 pairs, 10.8s, ~1395MB\n",
      "[2025-08-07 16:35:08]    Processing chunk 86/258 (33.3%)\n",
      "[2025-08-07 16:35:19]       Chunk completed: 3,227,046 pairs, 10.7s, ~1522MB\n",
      "[2025-08-07 16:35:19]    Processing chunk 87/258 (33.7%)\n",
      "[2025-08-07 16:35:30]       Chunk completed: 3,422,550 pairs, 11.1s, ~1656MB\n",
      "[2025-08-07 16:35:30]    Processing chunk 88/258 (34.1%)\n",
      "[2025-08-07 16:35:41]       Chunk completed: 3,289,222 pairs, 10.8s, ~1784MB\n",
      "[2025-08-07 16:35:42]    Processing chunk 89/258 (34.5%)\n",
      "[2025-08-07 16:35:52]       Chunk completed: 3,140,258 pairs, 10.7s, ~1905MB\n",
      "[2025-08-07 16:35:53]    Processing chunk 90/258 (34.9%)\n",
      "[2025-08-07 16:36:03]       Chunk completed: 3,137,996 pairs, 10.5s, ~2025MB\n",
      "[2025-08-07 16:36:04]    Processing chunk 91/258 (35.3%)\n",
      "[2025-08-07 16:36:14]       Chunk completed: 3,150,012 pairs, 10.5s, ~2144MB\n",
      "[2025-08-07 16:36:15]    Processing chunk 92/258 (35.7%)\n",
      "[2025-08-07 16:36:25]       Chunk completed: 3,201,068 pairs, 10.5s, ~2264MB\n",
      "[2025-08-07 16:36:26]    Processing chunk 93/258 (36.0%)\n",
      "[2025-08-07 16:36:36]       Chunk completed: 3,208,056 pairs, 10.6s, ~2383MB\n",
      "[2025-08-07 16:36:37]    Processing chunk 94/258 (36.4%)\n",
      "[2025-08-07 16:36:47]       Chunk completed: 3,210,404 pairs, 10.6s, ~2503MB\n",
      "[2025-08-07 16:36:48]    Processing chunk 95/258 (36.8%)\n",
      "[2025-08-07 16:36:58]       Chunk completed: 3,212,968 pairs, 10.7s, ~2621MB\n",
      "[2025-08-07 16:36:59]    Processing chunk 96/258 (37.2%)\n",
      "[2025-08-07 16:37:10]       Chunk completed: 3,199,646 pairs, 10.8s, ~2739MB\n",
      "[2025-08-07 16:37:10]    Processing chunk 97/258 (37.6%)\n",
      "[2025-08-07 16:37:22]       Chunk completed: 3,495,704 pairs, 11.5s, ~2867MB\n",
      "[2025-08-07 16:37:22]    Processing chunk 98/258 (38.0%)\n",
      "[2025-08-07 16:37:34]       Chunk completed: 3,430,614 pairs, 11.4s, ~2993MB\n",
      "[2025-08-07 16:37:34]    Processing chunk 99/258 (38.4%)\n",
      "[2025-08-07 16:37:45]       Chunk completed: 3,334,384 pairs, 11.3s, ~3115MB\n",
      "[2025-08-07 16:37:46]    Processing chunk 100/258 (38.8%)\n",
      "[2025-08-07 16:37:57]       Chunk completed: 3,369,530 pairs, 11.4s, ~3237MB\n",
      "[2025-08-07 16:37:57]    Saving intermediate results (chunk 100)...\n",
      "[2025-08-07 16:38:50]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_100.pkl (223.6 MB)\n",
      "[2025-08-07 16:38:55]    Memory cleared after save\n",
      "[2025-08-07 16:38:58]    Processing chunk 101/258 (39.1%)\n",
      "[2025-08-07 16:39:08]       Chunk completed: 3,298,878 pairs, 10.2s, ~157MB\n",
      "[2025-08-07 16:39:09]    Processing chunk 102/258 (39.5%)\n",
      "[2025-08-07 16:39:19]       Chunk completed: 3,314,636 pairs, 10.3s, ~307MB\n",
      "[2025-08-07 16:39:19]    Processing chunk 103/258 (39.9%)\n",
      "[2025-08-07 16:39:30]       Chunk completed: 3,219,960 pairs, 10.3s, ~449MB\n",
      "[2025-08-07 16:39:30]    Processing chunk 104/258 (40.3%)\n",
      "[2025-08-07 16:39:40]       Chunk completed: 3,118,758 pairs, 10.1s, ~583MB\n",
      "[2025-08-07 16:39:40]    Processing chunk 105/258 (40.7%)\n",
      "[2025-08-07 16:39:50]       Chunk completed: 3,158,422 pairs, 10.0s, ~717MB\n",
      "[2025-08-07 16:39:51]    Processing chunk 106/258 (41.1%)\n",
      "[2025-08-07 16:40:01]       Chunk completed: 3,017,030 pairs, 9.9s, ~842MB\n",
      "[2025-08-07 16:40:01]    Processing chunk 107/258 (41.5%)\n",
      "[2025-08-07 16:40:18]       Chunk completed: 2,982,354 pairs, 16.6s, ~964MB\n",
      "[2025-08-07 16:40:18]    Processing chunk 108/258 (41.9%)\n",
      "[2025-08-07 16:40:36]       Chunk completed: 3,207,010 pairs, 18.0s, ~1095MB\n",
      "[2025-08-07 16:40:37]    Processing chunk 109/258 (42.2%)\n",
      "[2025-08-07 16:40:53]       Chunk completed: 3,010,926 pairs, 16.1s, ~1216MB\n",
      "[2025-08-07 16:40:53]    Processing chunk 110/258 (42.6%)\n",
      "[2025-08-07 16:41:06]       Chunk completed: 2,948,972 pairs, 12.8s, ~1334MB\n",
      "[2025-08-07 16:41:06]    Processing chunk 111/258 (43.0%)\n",
      "[2025-08-07 16:41:16]       Chunk completed: 2,924,188 pairs, 10.1s, ~1447MB\n",
      "[2025-08-07 16:41:17]    Processing chunk 112/258 (43.4%)\n",
      "[2025-08-07 16:41:27]       Chunk completed: 3,044,132 pairs, 10.3s, ~1564MB\n",
      "[2025-08-07 16:41:27]    Processing chunk 113/258 (43.8%)\n",
      "[2025-08-07 16:41:38]       Chunk completed: 3,063,488 pairs, 10.4s, ~1683MB\n",
      "[2025-08-07 16:41:38]    Processing chunk 114/258 (44.2%)\n",
      "[2025-08-07 16:41:49]       Chunk completed: 3,032,674 pairs, 10.3s, ~1799MB\n",
      "[2025-08-07 16:41:49]    Processing chunk 115/258 (44.6%)\n",
      "[2025-08-07 16:41:59]       Chunk completed: 2,950,522 pairs, 10.1s, ~1911MB\n",
      "[2025-08-07 16:41:59]    Processing chunk 116/258 (45.0%)\n",
      "[2025-08-07 16:42:10]       Chunk completed: 3,092,468 pairs, 10.6s, ~2028MB\n",
      "[2025-08-07 16:42:10]    Processing chunk 117/258 (45.3%)\n",
      "[2025-08-07 16:42:21]       Chunk completed: 2,903,496 pairs, 10.1s, ~2138MB\n",
      "[2025-08-07 16:42:21]    Processing chunk 118/258 (45.7%)\n",
      "[2025-08-07 16:42:31]       Chunk completed: 2,892,286 pairs, 10.1s, ~2246MB\n",
      "[2025-08-07 16:42:31]    Processing chunk 119/258 (46.1%)\n",
      "[2025-08-07 16:42:42]       Chunk completed: 2,816,978 pairs, 10.1s, ~2351MB\n",
      "[2025-08-07 16:42:42]    Processing chunk 120/258 (46.5%)\n",
      "[2025-08-07 16:42:52]       Chunk completed: 2,757,274 pairs, 9.7s, ~2451MB\n",
      "[2025-08-07 16:42:52]    Processing chunk 121/258 (46.9%)\n",
      "[2025-08-07 16:43:02]       Chunk completed: 2,762,904 pairs, 9.7s, ~2550MB\n",
      "[2025-08-07 16:43:02]    Processing chunk 122/258 (47.3%)\n",
      "[2025-08-07 16:43:12]       Chunk completed: 2,684,908 pairs, 9.7s, ~2647MB\n",
      "[2025-08-07 16:43:12]    Processing chunk 123/258 (47.7%)\n",
      "[2025-08-07 16:43:22]       Chunk completed: 2,691,352 pairs, 9.8s, ~2744MB\n",
      "[2025-08-07 16:43:23]    Processing chunk 124/258 (48.1%)\n",
      "[2025-08-07 16:43:33]       Chunk completed: 2,670,372 pairs, 9.9s, ~2840MB\n",
      "[2025-08-07 16:43:33]    Processing chunk 125/258 (48.5%)\n",
      "[2025-08-07 16:43:43]       Chunk completed: 2,645,840 pairs, 9.8s, ~2934MB\n",
      "[2025-08-07 16:43:43]    Saving intermediate results (chunk 125)...\n",
      "[2025-08-07 16:44:28]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_125.pkl (210.1 MB)\n",
      "[2025-08-07 16:44:32]    Memory cleared after save\n",
      "[2025-08-07 16:44:35]    Processing chunk 126/258 (48.8%)\n",
      "[2025-08-07 16:44:44]       Chunk completed: 2,817,712 pairs, 9.2s, ~134MB\n",
      "[2025-08-07 16:44:45]    Processing chunk 127/258 (49.2%)\n",
      "[2025-08-07 16:44:54]       Chunk completed: 2,713,120 pairs, 8.8s, ~258MB\n",
      "[2025-08-07 16:44:54]    Processing chunk 128/258 (49.6%)\n",
      "[2025-08-07 16:45:02]       Chunk completed: 2,565,174 pairs, 8.6s, ~371MB\n",
      "[2025-08-07 16:45:03]    Processing chunk 129/258 (50.0%)\n",
      "[2025-08-07 16:45:12]       Chunk completed: 2,553,128 pairs, 8.8s, ~481MB\n",
      "[2025-08-07 16:45:12]    Processing chunk 130/258 (50.4%)\n",
      "[2025-08-07 16:45:21]       Chunk completed: 2,659,946 pairs, 9.0s, ~594MB\n",
      "[2025-08-07 16:45:21]    Processing chunk 131/258 (50.8%)\n",
      "[2025-08-07 16:45:31]       Chunk completed: 2,685,174 pairs, 9.3s, ~704MB\n",
      "[2025-08-07 16:45:31]    Processing chunk 132/258 (51.2%)\n",
      "[2025-08-07 16:45:40]       Chunk completed: 2,697,226 pairs, 9.2s, ~814MB\n",
      "[2025-08-07 16:45:41]    Processing chunk 133/258 (51.6%)\n",
      "[2025-08-07 16:45:50]       Chunk completed: 2,616,886 pairs, 9.1s, ~920MB\n",
      "[2025-08-07 16:45:50]    Processing chunk 134/258 (51.9%)\n",
      "[2025-08-07 16:45:59]       Chunk completed: 2,612,812 pairs, 9.2s, ~1024MB\n",
      "[2025-08-07 16:46:00]    Processing chunk 135/258 (52.3%)\n",
      "[2025-08-07 16:46:09]       Chunk completed: 2,754,260 pairs, 9.6s, ~1135MB\n",
      "[2025-08-07 16:46:10]    Processing chunk 136/258 (52.7%)\n",
      "[2025-08-07 16:46:19]       Chunk completed: 2,588,948 pairs, 9.2s, ~1237MB\n",
      "[2025-08-07 16:46:19]    Processing chunk 137/258 (53.1%)\n",
      "[2025-08-07 16:46:29]       Chunk completed: 2,620,296 pairs, 9.3s, ~1341MB\n",
      "[2025-08-07 16:46:29]    Processing chunk 138/258 (53.5%)\n",
      "[2025-08-07 16:46:39]       Chunk completed: 2,595,978 pairs, 9.9s, ~1440MB\n",
      "[2025-08-07 16:46:39]    Processing chunk 139/258 (53.9%)\n",
      "[2025-08-07 16:46:50]       Chunk completed: 2,635,294 pairs, 10.5s, ~1541MB\n",
      "[2025-08-07 16:46:50]    Processing chunk 140/258 (54.3%)\n",
      "[2025-08-07 16:47:00]       Chunk completed: 2,544,528 pairs, 9.8s, ~1638MB\n",
      "[2025-08-07 16:47:01]    Processing chunk 141/258 (54.7%)\n",
      "[2025-08-07 16:47:11]       Chunk completed: 2,616,422 pairs, 10.4s, ~1738MB\n",
      "[2025-08-07 16:47:11]    Processing chunk 142/258 (55.0%)\n",
      "[2025-08-07 16:47:22]       Chunk completed: 2,431,588 pairs, 10.4s, ~1829MB\n",
      "[2025-08-07 16:47:22]    Processing chunk 143/258 (55.4%)\n",
      "[2025-08-07 16:47:33]       Chunk completed: 2,543,738 pairs, 10.6s, ~1924MB\n",
      "[2025-08-07 16:47:33]    Processing chunk 144/258 (55.8%)\n",
      "[2025-08-07 16:47:44]       Chunk completed: 2,385,494 pairs, 10.0s, ~2013MB\n",
      "[2025-08-07 16:47:44]    Processing chunk 145/258 (56.2%)\n",
      "[2025-08-07 16:47:55]       Chunk completed: 2,368,140 pairs, 10.7s, ~2102MB\n",
      "[2025-08-07 16:47:55]    Processing chunk 146/258 (56.6%)\n",
      "[2025-08-07 16:48:05]       Chunk completed: 2,374,784 pairs, 9.6s, ~2189MB\n",
      "[2025-08-07 16:48:05]    Processing chunk 147/258 (57.0%)\n",
      "[2025-08-07 16:48:16]       Chunk completed: 2,455,308 pairs, 11.0s, ~2278MB\n",
      "[2025-08-07 16:48:17]    Processing chunk 148/258 (57.4%)\n",
      "[2025-08-07 16:48:27]       Chunk completed: 2,412,116 pairs, 10.6s, ~2365MB\n",
      "[2025-08-07 16:48:28]    Processing chunk 149/258 (57.8%)\n",
      "[2025-08-07 16:48:39]       Chunk completed: 2,451,062 pairs, 10.9s, ~2454MB\n",
      "[2025-08-07 16:48:39]    Processing chunk 150/258 (58.1%)\n",
      "[2025-08-07 16:48:51]       Chunk completed: 2,409,778 pairs, 11.2s, ~2541MB\n",
      "[2025-08-07 16:48:51]    Saving intermediate results (chunk 150)...\n",
      "[2025-08-07 16:49:39]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_150.pkl (193.1 MB)\n",
      "[2025-08-07 16:49:43]    Memory cleared after save\n",
      "[2025-08-07 16:49:46]    Processing chunk 151/258 (58.5%)\n",
      "[2025-08-07 16:49:54]       Chunk completed: 2,378,646 pairs, 8.3s, ~113MB\n",
      "[2025-08-07 16:49:55]    Processing chunk 152/258 (58.9%)\n",
      "[2025-08-07 16:50:03]       Chunk completed: 2,507,414 pairs, 8.6s, ~228MB\n",
      "[2025-08-07 16:50:04]    Processing chunk 153/258 (59.3%)\n",
      "[2025-08-07 16:50:12]       Chunk completed: 2,517,228 pairs, 8.8s, ~340MB\n",
      "[2025-08-07 16:50:13]    Processing chunk 154/258 (59.7%)\n",
      "[2025-08-07 16:50:21]       Chunk completed: 2,325,644 pairs, 8.3s, ~441MB\n",
      "[2025-08-07 16:50:21]    Processing chunk 155/258 (60.1%)\n",
      "[2025-08-07 16:50:30]       Chunk completed: 2,316,118 pairs, 8.4s, ~540MB\n",
      "[2025-08-07 16:50:30]    Processing chunk 156/258 (60.5%)\n",
      "[2025-08-07 16:50:39]       Chunk completed: 2,393,154 pairs, 8.6s, ~640MB\n",
      "[2025-08-07 16:50:39]    Processing chunk 157/258 (60.9%)\n",
      "[2025-08-07 16:50:48]       Chunk completed: 2,434,592 pairs, 8.6s, ~741MB\n",
      "[2025-08-07 16:50:48]    Processing chunk 158/258 (61.2%)\n",
      "[2025-08-07 16:50:57]       Chunk completed: 2,339,154 pairs, 8.6s, ~836MB\n",
      "[2025-08-07 16:50:57]    Processing chunk 159/258 (61.6%)\n",
      "[2025-08-07 16:51:06]       Chunk completed: 2,369,972 pairs, 8.7s, ~932MB\n",
      "[2025-08-07 16:51:06]    Processing chunk 160/258 (62.0%)\n",
      "[2025-08-07 16:51:15]       Chunk completed: 2,356,072 pairs, 8.9s, ~1027MB\n",
      "[2025-08-07 16:51:15]    Processing chunk 161/258 (62.4%)\n",
      "[2025-08-07 16:51:24]       Chunk completed: 2,275,750 pairs, 8.6s, ~1117MB\n",
      "[2025-08-07 16:51:24]    Processing chunk 162/258 (62.8%)\n",
      "[2025-08-07 16:51:34]       Chunk completed: 2,656,354 pairs, 9.3s, ~1223MB\n",
      "[2025-08-07 16:51:34]    Processing chunk 163/258 (63.2%)\n",
      "[2025-08-07 16:51:43]       Chunk completed: 2,427,276 pairs, 9.0s, ~1318MB\n",
      "[2025-08-07 16:51:44]    Processing chunk 164/258 (63.6%)\n",
      "[2025-08-07 16:51:52]       Chunk completed: 2,426,142 pairs, 8.8s, ~1413MB\n",
      "[2025-08-07 16:51:53]    Processing chunk 165/258 (64.0%)\n",
      "[2025-08-07 16:52:01]       Chunk completed: 2,418,814 pairs, 8.7s, ~1506MB\n",
      "[2025-08-07 16:52:02]    Processing chunk 166/258 (64.3%)\n",
      "[2025-08-07 16:52:11]       Chunk completed: 2,423,112 pairs, 8.7s, ~1598MB\n",
      "[2025-08-07 16:52:11]    Processing chunk 167/258 (64.7%)\n",
      "[2025-08-07 16:52:24]       Chunk completed: 2,477,752 pairs, 13.3s, ~1692MB\n",
      "[2025-08-07 16:52:25]    Processing chunk 168/258 (65.1%)\n",
      "[2025-08-07 16:52:34]       Chunk completed: 2,433,224 pairs, 8.8s, ~1782MB\n",
      "[2025-08-07 16:52:34]    Processing chunk 169/258 (65.5%)\n",
      "[2025-08-07 16:52:43]       Chunk completed: 2,317,662 pairs, 8.5s, ~1868MB\n",
      "[2025-08-07 16:52:43]    Processing chunk 170/258 (65.9%)\n",
      "[2025-08-07 16:52:51]       Chunk completed: 2,189,308 pairs, 8.3s, ~1949MB\n",
      "[2025-08-07 16:52:52]    Processing chunk 171/258 (66.3%)\n",
      "[2025-08-07 16:53:00]       Chunk completed: 2,224,940 pairs, 8.4s, ~2031MB\n",
      "[2025-08-07 16:53:00]    Processing chunk 172/258 (66.7%)\n",
      "[2025-08-07 16:53:09]       Chunk completed: 2,193,804 pairs, 8.2s, ~2111MB\n",
      "[2025-08-07 16:53:09]    Processing chunk 173/258 (67.1%)\n",
      "[2025-08-07 16:53:17]       Chunk completed: 2,221,734 pairs, 8.3s, ~2192MB\n",
      "[2025-08-07 16:53:18]    Processing chunk 174/258 (67.4%)\n",
      "[2025-08-07 16:53:27]       Chunk completed: 2,394,002 pairs, 8.7s, ~2281MB\n",
      "[2025-08-07 16:53:27]    Processing chunk 175/258 (67.8%)\n",
      "[2025-08-07 16:53:35]       Chunk completed: 2,299,180 pairs, 8.5s, ~2366MB\n",
      "[2025-08-07 16:53:35]    Saving intermediate results (chunk 175)...\n",
      "[2025-08-07 16:54:07]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_175.pkl (184.2 MB)\n",
      "[2025-08-07 16:54:11]    Memory cleared after save\n",
      "[2025-08-07 16:54:13]    Processing chunk 176/258 (68.2%)\n",
      "[2025-08-07 16:54:21]       Chunk completed: 2,276,936 pairs, 8.1s, ~109MB\n",
      "[2025-08-07 16:54:22]    Processing chunk 177/258 (68.6%)\n",
      "[2025-08-07 16:54:29]       Chunk completed: 2,204,202 pairs, 7.6s, ~209MB\n",
      "[2025-08-07 16:54:29]    Processing chunk 178/258 (69.0%)\n",
      "[2025-08-07 16:54:37]       Chunk completed: 2,195,916 pairs, 7.6s, ~306MB\n",
      "[2025-08-07 16:54:37]    Processing chunk 179/258 (69.4%)\n",
      "[2025-08-07 16:54:45]       Chunk completed: 2,199,944 pairs, 7.8s, ~401MB\n",
      "[2025-08-07 16:54:46]    Processing chunk 180/258 (69.8%)\n",
      "[2025-08-07 16:54:53]       Chunk completed: 2,143,862 pairs, 7.6s, ~492MB\n",
      "[2025-08-07 16:54:53]    Processing chunk 181/258 (70.2%)\n",
      "[2025-08-07 16:55:01]       Chunk completed: 2,177,694 pairs, 7.8s, ~584MB\n",
      "[2025-08-07 16:55:02]    Processing chunk 182/258 (70.5%)\n",
      "[2025-08-07 16:55:09]       Chunk completed: 2,108,926 pairs, 7.7s, ~671MB\n",
      "[2025-08-07 16:55:10]    Processing chunk 183/258 (70.9%)\n",
      "[2025-08-07 16:55:18]       Chunk completed: 2,188,582 pairs, 8.0s, ~761MB\n",
      "[2025-08-07 16:55:18]    Processing chunk 184/258 (71.3%)\n",
      "[2025-08-07 16:55:26]       Chunk completed: 2,380,262 pairs, 8.2s, ~859MB\n",
      "[2025-08-07 16:55:26]    Processing chunk 185/258 (71.7%)\n",
      "[2025-08-07 16:55:35]       Chunk completed: 2,277,828 pairs, 8.1s, ~951MB\n",
      "[2025-08-07 16:55:35]    Processing chunk 186/258 (72.1%)\n",
      "[2025-08-07 16:55:43]       Chunk completed: 2,284,442 pairs, 8.1s, ~1041MB\n",
      "[2025-08-07 16:55:43]    Processing chunk 187/258 (72.5%)\n",
      "[2025-08-07 16:55:52]       Chunk completed: 2,285,918 pairs, 8.1s, ~1129MB\n",
      "[2025-08-07 16:55:52]    Processing chunk 188/258 (72.9%)\n",
      "[2025-08-07 16:56:00]       Chunk completed: 2,290,110 pairs, 8.0s, ~1219MB\n",
      "[2025-08-07 16:56:00]    Processing chunk 189/258 (73.3%)\n",
      "[2025-08-07 16:56:08]       Chunk completed: 2,266,240 pairs, 8.1s, ~1307MB\n",
      "[2025-08-07 16:56:09]    Processing chunk 190/258 (73.6%)\n",
      "[2025-08-07 16:56:17]       Chunk completed: 2,333,362 pairs, 8.3s, ~1397MB\n",
      "[2025-08-07 16:56:17]    Processing chunk 191/258 (74.0%)\n",
      "[2025-08-07 16:56:26]       Chunk completed: 2,552,820 pairs, 8.6s, ~1497MB\n",
      "[2025-08-07 16:56:26]    Processing chunk 192/258 (74.4%)\n",
      "[2025-08-07 16:56:35]       Chunk completed: 2,329,420 pairs, 8.2s, ~1587MB\n",
      "[2025-08-07 16:56:35]    Processing chunk 193/258 (74.8%)\n",
      "[2025-08-07 16:56:43]       Chunk completed: 2,272,314 pairs, 8.1s, ~1671MB\n",
      "[2025-08-07 16:56:44]    Processing chunk 194/258 (75.2%)\n",
      "[2025-08-07 16:56:52]       Chunk completed: 2,255,922 pairs, 8.1s, ~1754MB\n",
      "[2025-08-07 16:56:52]    Processing chunk 195/258 (75.6%)\n",
      "[2025-08-07 16:57:00]       Chunk completed: 2,237,106 pairs, 8.0s, ~1837MB\n",
      "[2025-08-07 16:57:00]    Processing chunk 196/258 (76.0%)\n",
      "[2025-08-07 16:57:09]       Chunk completed: 2,217,070 pairs, 8.1s, ~1919MB\n",
      "[2025-08-07 16:57:09]    Processing chunk 197/258 (76.4%)\n",
      "[2025-08-07 16:57:17]       Chunk completed: 2,297,846 pairs, 8.3s, ~2005MB\n",
      "[2025-08-07 16:57:18]    Processing chunk 198/258 (76.7%)\n",
      "[2025-08-07 16:57:26]       Chunk completed: 2,065,334 pairs, 7.9s, ~2082MB\n",
      "[2025-08-07 16:57:26]    Processing chunk 199/258 (77.1%)\n",
      "[2025-08-07 16:57:34]       Chunk completed: 2,231,524 pairs, 8.1s, ~2163MB\n",
      "[2025-08-07 16:57:35]    Processing chunk 200/258 (77.5%)\n",
      "[2025-08-07 16:57:43]       Chunk completed: 2,186,506 pairs, 8.1s, ~2242MB\n",
      "[2025-08-07 16:57:43]    Saving intermediate results (chunk 200)...\n",
      "[2025-08-07 16:58:13]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_200.pkl (179.3 MB)\n",
      "[2025-08-07 16:58:16]    Memory cleared after save\n",
      "[2025-08-07 16:58:19]    Processing chunk 201/258 (77.9%)\n",
      "[2025-08-07 16:58:26]       Chunk completed: 2,070,290 pairs, 7.2s, ~99MB\n",
      "[2025-08-07 16:58:26]    Processing chunk 202/258 (78.3%)\n",
      "[2025-08-07 16:58:34]       Chunk completed: 2,138,756 pairs, 7.4s, ~196MB\n",
      "[2025-08-07 16:58:34]    Processing chunk 203/258 (78.7%)\n",
      "[2025-08-07 16:58:42]       Chunk completed: 2,228,602 pairs, 7.7s, ~296MB\n",
      "[2025-08-07 16:58:42]    Processing chunk 204/258 (79.1%)\n",
      "[2025-08-07 16:58:50]       Chunk completed: 2,209,308 pairs, 7.7s, ~393MB\n",
      "[2025-08-07 16:58:50]    Processing chunk 205/258 (79.5%)\n",
      "[2025-08-07 16:58:58]       Chunk completed: 2,052,222 pairs, 7.5s, ~481MB\n",
      "[2025-08-07 16:58:58]    Processing chunk 206/258 (79.8%)\n",
      "[2025-08-07 16:59:06]       Chunk completed: 2,140,046 pairs, 7.8s, ~571MB\n",
      "[2025-08-07 16:59:06]    Processing chunk 207/258 (80.2%)\n",
      "[2025-08-07 16:59:14]       Chunk completed: 2,187,462 pairs, 7.8s, ~662MB\n",
      "[2025-08-07 16:59:14]    Processing chunk 208/258 (80.6%)\n",
      "[2025-08-07 16:59:22]       Chunk completed: 2,170,260 pairs, 7.9s, ~752MB\n",
      "[2025-08-07 16:59:22]    Processing chunk 209/258 (81.0%)\n",
      "[2025-08-07 16:59:30]       Chunk completed: 2,102,848 pairs, 7.8s, ~838MB\n",
      "[2025-08-07 16:59:31]    Processing chunk 210/258 (81.4%)\n",
      "[2025-08-07 16:59:39]       Chunk completed: 2,253,750 pairs, 7.9s, ~930MB\n",
      "[2025-08-07 16:59:39]    Processing chunk 211/258 (81.8%)\n",
      "[2025-08-07 16:59:51]       Chunk completed: 2,089,454 pairs, 12.1s, ~1013MB\n",
      "[2025-08-07 16:59:51]    Processing chunk 212/258 (82.2%)\n",
      "[2025-08-07 17:00:06]       Chunk completed: 2,224,792 pairs, 14.3s, ~1101MB\n",
      "[2025-08-07 17:00:06]    Processing chunk 213/258 (82.6%)\n",
      "[2025-08-07 17:00:20]       Chunk completed: 2,201,186 pairs, 14.0s, ~1186MB\n",
      "[2025-08-07 17:00:20]    Processing chunk 214/258 (82.9%)\n",
      "[2025-08-07 17:00:32]       Chunk completed: 2,216,474 pairs, 11.7s, ~1273MB\n",
      "[2025-08-07 17:00:32]    Processing chunk 215/258 (83.3%)\n",
      "[2025-08-07 17:00:43]       Chunk completed: 2,254,858 pairs, 10.3s, ~1362MB\n",
      "[2025-08-07 17:00:43]    Processing chunk 216/258 (83.7%)\n",
      "[2025-08-07 17:00:52]       Chunk completed: 2,431,692 pairs, 8.5s, ~1456MB\n",
      "[2025-08-07 17:00:52]    Processing chunk 217/258 (84.1%)\n",
      "[2025-08-07 17:01:00]       Chunk completed: 2,254,762 pairs, 8.3s, ~1544MB\n",
      "[2025-08-07 17:01:01]    Processing chunk 218/258 (84.5%)\n",
      "[2025-08-07 17:01:09]       Chunk completed: 2,243,992 pairs, 8.1s, ~1629MB\n",
      "[2025-08-07 17:01:09]    Processing chunk 219/258 (84.9%)\n",
      "[2025-08-07 17:01:17]       Chunk completed: 2,250,700 pairs, 8.3s, ~1712MB\n",
      "[2025-08-07 17:01:18]    Processing chunk 220/258 (85.3%)\n",
      "[2025-08-07 17:01:26]       Chunk completed: 2,164,384 pairs, 8.0s, ~1793MB\n",
      "[2025-08-07 17:01:26]    Processing chunk 221/258 (85.7%)\n",
      "[2025-08-07 17:01:34]       Chunk completed: 2,080,094 pairs, 7.9s, ~1870MB\n",
      "[2025-08-07 17:01:34]    Processing chunk 222/258 (86.0%)\n",
      "[2025-08-07 17:01:42]       Chunk completed: 1,997,740 pairs, 7.8s, ~1944MB\n",
      "[2025-08-07 17:01:43]    Processing chunk 223/258 (86.4%)\n",
      "[2025-08-07 17:01:51]       Chunk completed: 2,262,320 pairs, 8.2s, ~2029MB\n",
      "[2025-08-07 17:01:51]    Processing chunk 224/258 (86.8%)\n",
      "[2025-08-07 17:01:59]       Chunk completed: 2,076,784 pairs, 7.8s, ~2106MB\n",
      "[2025-08-07 17:01:59]    Processing chunk 225/258 (87.2%)\n",
      "[2025-08-07 17:02:07]       Chunk completed: 2,051,778 pairs, 7.9s, ~2179MB\n",
      "[2025-08-07 17:02:07]    Saving intermediate results (chunk 225)...\n",
      "[2025-08-07 17:02:38]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_225.pkl (176.4 MB)\n",
      "[2025-08-07 17:02:41]    Memory cleared after save\n",
      "[2025-08-07 17:02:44]    Processing chunk 226/258 (87.6%)\n",
      "[2025-08-07 17:02:51]       Chunk completed: 1,981,090 pairs, 7.1s, ~94MB\n",
      "[2025-08-07 17:02:51]    Processing chunk 227/258 (88.0%)\n",
      "[2025-08-07 17:02:58]       Chunk completed: 1,969,390 pairs, 7.1s, ~184MB\n",
      "[2025-08-07 17:02:58]    Processing chunk 228/258 (88.4%)\n",
      "[2025-08-07 17:03:06]       Chunk completed: 2,114,398 pairs, 7.5s, ~279MB\n",
      "[2025-08-07 17:03:06]    Processing chunk 229/258 (88.8%)\n",
      "[2025-08-07 17:03:13]       Chunk completed: 1,830,096 pairs, 7.2s, ~359MB\n",
      "[2025-08-07 17:03:14]    Processing chunk 230/258 (89.1%)\n",
      "[2025-08-07 17:03:21]       Chunk completed: 1,832,494 pairs, 7.1s, ~438MB\n",
      "[2025-08-07 17:03:21]    Processing chunk 231/258 (89.5%)\n",
      "[2025-08-07 17:03:28]       Chunk completed: 1,900,980 pairs, 7.3s, ~517MB\n",
      "[2025-08-07 17:03:29]    Processing chunk 232/258 (89.9%)\n",
      "[2025-08-07 17:03:36]       Chunk completed: 1,863,670 pairs, 7.1s, ~594MB\n",
      "[2025-08-07 17:03:36]    Processing chunk 233/258 (90.3%)\n",
      "[2025-08-07 17:03:43]       Chunk completed: 1,831,746 pairs, 7.2s, ~669MB\n",
      "[2025-08-07 17:03:44]    Processing chunk 234/258 (90.7%)\n",
      "[2025-08-07 17:03:51]       Chunk completed: 2,095,312 pairs, 7.6s, ~756MB\n",
      "[2025-08-07 17:03:52]    Processing chunk 235/258 (91.1%)\n",
      "[2025-08-07 17:03:59]       Chunk completed: 1,976,956 pairs, 7.5s, ~838MB\n",
      "[2025-08-07 17:03:59]    Processing chunk 236/258 (91.5%)\n",
      "[2025-08-07 17:04:07]       Chunk completed: 1,945,644 pairs, 7.6s, ~915MB\n",
      "[2025-08-07 17:04:07]    Processing chunk 237/258 (91.9%)\n",
      "[2025-08-07 17:04:15]       Chunk completed: 1,849,018 pairs, 7.4s, ~987MB\n",
      "[2025-08-07 17:04:15]    Processing chunk 238/258 (92.2%)\n",
      "[2025-08-07 17:04:22]       Chunk completed: 1,786,046 pairs, 7.3s, ~1058MB\n",
      "[2025-08-07 17:04:23]    Processing chunk 239/258 (92.6%)\n",
      "[2025-08-07 17:04:33]       Chunk completed: 2,019,952 pairs, 9.9s, ~1138MB\n",
      "[2025-08-07 17:04:33]    Processing chunk 240/258 (93.0%)\n",
      "[2025-08-07 17:04:41]       Chunk completed: 1,908,938 pairs, 7.4s, ~1213MB\n",
      "[2025-08-07 17:04:41]    Processing chunk 241/258 (93.4%)\n",
      "[2025-08-07 17:04:48]       Chunk completed: 1,805,220 pairs, 7.4s, ~1281MB\n",
      "[2025-08-07 17:04:49]    Processing chunk 242/258 (93.8%)\n",
      "[2025-08-07 17:04:56]       Chunk completed: 1,849,384 pairs, 7.5s, ~1351MB\n",
      "[2025-08-07 17:04:56]    Processing chunk 243/258 (94.2%)\n",
      "[2025-08-07 17:05:04]       Chunk completed: 1,815,082 pairs, 7.5s, ~1420MB\n",
      "[2025-08-07 17:05:04]    Processing chunk 244/258 (94.6%)\n",
      "[2025-08-07 17:05:13]       Chunk completed: 2,115,620 pairs, 9.1s, ~1502MB\n",
      "[2025-08-07 17:05:14]    Processing chunk 245/258 (95.0%)\n",
      "[2025-08-07 17:05:22]       Chunk completed: 1,920,136 pairs, 7.8s, ~1576MB\n",
      "[2025-08-07 17:05:22]    Processing chunk 246/258 (95.4%)\n",
      "[2025-08-07 17:05:30]       Chunk completed: 1,974,198 pairs, 7.6s, ~1650MB\n",
      "[2025-08-07 17:05:30]    Processing chunk 247/258 (95.7%)\n",
      "[2025-08-07 17:05:38]       Chunk completed: 2,187,782 pairs, 8.0s, ~1734MB\n",
      "[2025-08-07 17:05:38]    Processing chunk 248/258 (96.1%)\n",
      "[2025-08-07 17:05:46]       Chunk completed: 2,098,974 pairs, 7.8s, ~1813MB\n",
      "[2025-08-07 17:05:47]    Processing chunk 249/258 (96.5%)\n",
      "[2025-08-07 17:05:54]       Chunk completed: 2,065,600 pairs, 7.7s, ~1891MB\n",
      "[2025-08-07 17:05:55]    Processing chunk 250/258 (96.9%)\n",
      "[2025-08-07 17:06:03]       Chunk completed: 2,069,634 pairs, 7.8s, ~1968MB\n",
      "[2025-08-07 17:06:03]    Saving intermediate results (chunk 250)...\n",
      "[2025-08-07 17:06:31]    Intermediate results saved: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/click_matrix_temp_chunk_250.pkl (165.6 MB)\n",
      "[2025-08-07 17:06:34]    Memory cleared after save\n",
      "[2025-08-07 17:06:37]    Processing chunk 251/258 (97.3%)\n",
      "[2025-08-07 17:06:44]       Chunk completed: 2,179,734 pairs, 7.6s, ~104MB\n",
      "[2025-08-07 17:06:44]    Processing chunk 252/258 (97.7%)\n",
      "[2025-08-07 17:06:52]       Chunk completed: 2,140,272 pairs, 7.3s, ~202MB\n",
      "[2025-08-07 17:06:52]    Processing chunk 253/258 (98.1%)\n",
      "[2025-08-07 17:06:59]       Chunk completed: 2,224,298 pairs, 7.4s, ~301MB\n",
      "[2025-08-07 17:07:00]    Processing chunk 254/258 (98.5%)\n",
      "[2025-08-07 17:07:07]       Chunk completed: 2,240,026 pairs, 7.7s, ~399MB\n",
      "[2025-08-07 17:07:08]    Processing chunk 255/258 (98.8%)\n",
      "[2025-08-07 17:07:15]       Chunk completed: 2,169,752 pairs, 7.6s, ~492MB\n",
      "[2025-08-07 17:07:16]    Processing chunk 256/258 (99.2%)\n",
      "[2025-08-07 17:07:23]       Chunk completed: 2,008,410 pairs, 7.4s, ~575MB\n",
      "[2025-08-07 17:07:23]    Processing chunk 257/258 (99.6%)\n",
      "[2025-08-07 17:07:31]       Chunk completed: 1,791,274 pairs, 7.1s, ~649MB\n",
      "[2025-08-07 17:07:31]    Processing chunk 258/258 (100.0%)\n",
      "[2025-08-07 17:07:38]       Chunk completed: 1,698,102 pairs, 7.1s, ~718MB\n",
      "[2025-08-07 17:07:38]    Collecting and merging intermediate results...\n",
      "[2025-08-07 17:07:38]    Merging intermediate results...\n",
      "[2025-08-07 17:07:38]    Found 10 intermediate files to merge\n",
      "[2025-08-07 17:07:38]    Adding current memory state to final matrix...\n",
      "[2025-08-07 17:07:45]    Merging click_matrix_temp_chunk_25.pkl...\n",
      "[2025-08-07 17:08:23]    Cleaned up click_matrix_temp_chunk_25.pkl\n",
      "[2025-08-07 17:08:23]    Merging click_matrix_temp_chunk_50.pkl...\n",
      "[2025-08-07 17:09:12]    Cleaned up click_matrix_temp_chunk_50.pkl\n",
      "[2025-08-07 17:09:12]    Merging click_matrix_temp_chunk_75.pkl...\n",
      "[2025-08-07 17:10:14]    Cleaned up click_matrix_temp_chunk_75.pkl\n",
      "[2025-08-07 17:10:14]    Merging click_matrix_temp_chunk_100.pkl...\n",
      "[2025-08-07 17:11:33]    Cleaned up click_matrix_temp_chunk_100.pkl\n",
      "[2025-08-07 17:11:33]    Merging click_matrix_temp_chunk_125.pkl...\n",
      "[2025-08-07 17:12:41]    Cleaned up click_matrix_temp_chunk_125.pkl\n",
      "[2025-08-07 17:12:41]    Merging click_matrix_temp_chunk_150.pkl...\n",
      "[2025-08-07 17:14:06]    Cleaned up click_matrix_temp_chunk_150.pkl\n",
      "[2025-08-07 17:14:06]    Merging click_matrix_temp_chunk_175.pkl...\n",
      "[2025-08-07 17:15:33]    Cleaned up click_matrix_temp_chunk_175.pkl\n",
      "[2025-08-07 17:15:33]    Merging click_matrix_temp_chunk_200.pkl...\n",
      "[2025-08-07 17:16:54]    Cleaned up click_matrix_temp_chunk_200.pkl\n",
      "[2025-08-07 17:16:54]    Merging click_matrix_temp_chunk_225.pkl...\n",
      "[2025-08-07 17:18:18]    Cleaned up click_matrix_temp_chunk_225.pkl\n",
      "[2025-08-07 17:18:18]    Merging click_matrix_temp_chunk_250.pkl...\n",
      "[2025-08-07 17:19:57]    Cleaned up click_matrix_temp_chunk_250.pkl\n",
      "[2025-08-07 17:19:57]    Final matrix merged: 1,839,483 source items\n",
      "[2025-08-07 17:19:58] Click-to-click matrix generation completed!\n",
      "[2025-08-07 17:19:58]    Total time: 3968.2 seconds (66.1 minutes)\n",
      "[2025-08-07 17:19:58]    Processed sessions: 12,899,779\n",
      "[2025-08-07 17:19:58]    Final matrix size: 1,839,483 source items\n",
      "[2025-08-07 17:19:58] Matrix generation completed successfully!\n",
      "[2025-08-07 17:19:58] Generated matrix with 1,839,483 source items\n",
      "[2025-08-07 17:20:10] Final cleanup completed\n",
      "[2025-08-07 17:20:10] Post-generation memory status: LOW\n",
      "[2025-08-07 17:20:10] \n",
      "Generation Results:\n",
      "[2025-08-07 17:20:10]   Success: True\n",
      "[2025-08-07 17:20:10]   Matrix size: 1,839,483 source items\n",
      "[2025-08-07 17:20:10]   Generation time: 3979.9 seconds (66.3 minutes)\n",
      "[2025-08-07 17:20:10]   Memory status: LOW\n",
      "[2025-08-07 17:20:11]   Total pairs: 63,503,324\n",
      "[2025-08-07 17:20:11]   Avg candidates per item: 34.5\n"
     ]
    }
   ],
   "source": [
    "log(\"\\nPreparing click data for matrix generation...\")\n",
    "\n",
    "# Memory check before starting\n",
    "initial_memory = check_memory()\n",
    "log(f\"Initial memory status: {initial_memory}\")\n",
    "\n",
    "# Prepare click data with memory optimization\n",
    "log(\"   Filtering and optimizing click data...\")\n",
    "click_data = prepared_data.filter(pl.col(\"type\") == \"clicks\").select([\"session\", \"aid\", \"ts\"]).sort([\"session\", \"ts\"])\n",
    "log(f\"Click data prepared: {click_data.shape} ({click_data.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "# Clear original data from memory immediately\n",
    "del prepared_data\n",
    "gc.collect()\n",
    "log(\"   Original prepared data cleared from memory\")\n",
    "\n",
    "# Memory check after cleanup\n",
    "post_cleanup_memory = check_memory()\n",
    "log(f\"Memory after cleanup: {post_cleanup_memory}\")\n",
    "\n",
    "# Initialize memory-efficient matrix generator\n",
    "chunk_size = validation_results[\"chunk_size\"]\n",
    "log(f\"\\nInitializing memory-efficient matrix generator...\")\n",
    "generator = ClickToClickMatrixGenerator(chunk_size)\n",
    "\n",
    "# Pre-generation memory check\n",
    "pre_generation_memory = check_memory()\n",
    "log(f\"Pre-generation memory status: {pre_generation_memory}\")\n",
    "\n",
    "# Set up emergency memory monitoring\n",
    "def emergency_memory_check():\n",
    "    \"\"\"Emergency memory check during generation\"\"\"\n",
    "    memory = psutil.virtual_memory()\n",
    "    if memory.percent > 95:\n",
    "        log(f\"EMERGENCY: Memory usage at {memory.percent:.1f}%\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Generate matrix with comprehensive error handling\n",
    "generation_successful = False\n",
    "click_to_click_matrix = {}\n",
    "\n",
    "try:\n",
    "    log(\"Starting matrix generation with memory management...\")\n",
    "\n",
    "    # Monitor memory during generation\n",
    "    generation_start = time.time()\n",
    "\n",
    "    # Generate matrix with automatic memory management\n",
    "    click_to_click_matrix = generator.generate_matrix(click_data)\n",
    "\n",
    "    if click_to_click_matrix and len(click_to_click_matrix) > 0:\n",
    "        generation_successful = True\n",
    "        log(f\"Matrix generation completed successfully!\")\n",
    "        log(f\"Generated matrix with {len(click_to_click_matrix):,} source items\")\n",
    "    else:\n",
    "        log(\"Matrix generation completed but resulted in empty matrix\")\n",
    "        generation_successful = False\n",
    "\n",
    "except MemoryError as e:\n",
    "    log(f\"MEMORY ERROR during matrix generation: {e}\")\n",
    "    log(\"Attempting to recover partial results...\")\n",
    "    generation_successful = False\n",
    "\n",
    "    # Try to get partial results from intermediate saves\n",
    "    try:\n",
    "        import glob\n",
    "        temp_files = glob.glob(f\"{config.OUTPUT_PATH}/click_matrix_temp_chunk_*.pkl\")\n",
    "        if temp_files:\n",
    "            log(f\"Found {len(temp_files)} intermediate files - attempting partial recovery...\")\n",
    "            # Load the most recent intermediate result\n",
    "            latest_file = max(temp_files, key=os.path.getctime)\n",
    "            with open(latest_file, \"rb\") as f:\n",
    "                temp_data = pickle.load(f)\n",
    "                click_to_click_matrix = temp_data.get(\"partial_matrix\", {})\n",
    "            log(f\"Recovered partial matrix with {len(click_to_click_matrix):,} items from {os.path.basename(latest_file)}\")\n",
    "        else:\n",
    "            click_to_click_matrix = {}\n",
    "    except Exception as recovery_error:\n",
    "        log(f\"Recovery failed: {recovery_error}\")\n",
    "        click_to_click_matrix = {}\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Unexpected error during matrix generation: {e}\")\n",
    "    log(\"Attempting to save partial results...\")\n",
    "    generation_successful = False\n",
    "\n",
    "    # Try to save current state\n",
    "    try:\n",
    "        if hasattr(generator, 'covisitation_counts') and generator.covisitation_counts:\n",
    "            generator.save_intermediate_results(force_save=True)\n",
    "            # Convert current state to matrix format\n",
    "            current_matrix = {}\n",
    "            for source_aid, targets in generator.covisitation_counts.items():\n",
    "                if targets:\n",
    "                    sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "                    current_matrix[source_aid] = sorted_targets[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "            click_to_click_matrix = current_matrix\n",
    "            log(f\"Saved partial results with {len(click_to_click_matrix):,} items\")\n",
    "        else:\n",
    "            click_to_click_matrix = {}\n",
    "    except Exception as save_error:\n",
    "        log(f\"Failed to save partial results: {save_error}\")\n",
    "        click_to_click_matrix = {}\n",
    "\n",
    "finally:\n",
    "    # Final cleanup\n",
    "    try:\n",
    "        if 'click_data' in locals():\n",
    "            del click_data\n",
    "        if 'generator' in locals() and hasattr(generator, 'covisitation_counts'):\n",
    "            generator.covisitation_counts.clear()\n",
    "        gc.collect()\n",
    "        log(\"Final cleanup completed\")\n",
    "    except Exception as cleanup_error:\n",
    "        log(f\"Cleanup error: {cleanup_error}\")\n",
    "\n",
    "# Post-generation memory check\n",
    "post_generation_memory = check_memory()\n",
    "log(f\"Post-generation memory status: {post_generation_memory}\")\n",
    "\n",
    "# Generation results summary\n",
    "generation_time = time.time() - generation_start if 'generation_start' in locals() else 0\n",
    "log(f\"\\nGeneration Results:\")\n",
    "log(f\"  Success: {generation_successful}\")\n",
    "log(f\"  Matrix size: {len(click_to_click_matrix):,} source items\")\n",
    "log(f\"  Generation time: {generation_time:.1f} seconds ({generation_time/60:.1f} minutes)\")\n",
    "log(f\"  Memory status: {post_generation_memory}\")\n",
    "\n",
    "# Final validation\n",
    "if click_to_click_matrix:\n",
    "    total_pairs = sum(len(candidates) for candidates in click_to_click_matrix.values())\n",
    "    log(f\"  Total pairs: {total_pairs:,}\")\n",
    "    log(f\"  Avg candidates per item: {total_pairs/len(click_to_click_matrix):.1f}\")\n",
    "else:\n",
    "    log(\"  WARNING: No matrix generated - check logs for errors\")\n",
    "    total_pairs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9js_rsAr8RyC"
   },
   "source": [
    "## MATRIX ANALYSIS AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0_shIeL8UIF",
    "outputId": "4e99bf12-db31-4c73-c963-5dfbaa1b735a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 17:20:11] Analyzing generated click-to-click matrix...\n",
      "[2025-08-07 17:20:11]     Basic statistics:\n",
      "[2025-08-07 17:20:11]     Source items: 1,839,483\n",
      "[2025-08-07 17:20:11]     Total pairs: 63,503,324\n",
      "[2025-08-07 17:20:11]     Avg candidates per item: 34.5\n",
      "[2025-08-07 17:20:12]       Candidate count distribution:\n",
      "[2025-08-07 17:20:12]       Min: 1, Max: 40\n",
      "[2025-08-07 17:20:12]       Mean: 34.5, Median: 40.0\n",
      "[2025-08-07 17:20:13]       Co-visitation score distribution (sample of 100 items):\n",
      "[2025-08-07 17:20:13]       Min: 1, Max: 22925\n",
      "[2025-08-07 17:20:13]       Mean: 245.2, Median: 24.0\n",
      "[2025-08-07 17:20:13]       Memory usage during generation:\n",
      "[2025-08-07 17:20:13]       Peak usage: 31.4%\n",
      "[2025-08-07 17:20:13]       Average usage: 18.1%\n",
      "[2025-08-07 17:20:13]       Critical events: 0\n",
      "[2025-08-07 17:20:13]    Sample relationships:\n",
      "[2025-08-07 17:20:13]       Item 1506112 â†’ 369364(52), 434110(40), 1205660(27)\n",
      "[2025-08-07 17:20:13]       Item 111945 â†’ 469872(179), 324152(88), 1650634(87)\n",
      "[2025-08-07 17:20:13]       Item 617851 â†’ 1613745(33), 1338856(33), 1342318(15)\n",
      "[2025-08-07 17:20:13]    Quality Assessment: EXCELLENT (4/4 criteria met)\n",
      "[2025-08-07 17:20:13] Matrix analysis completed!\n"
     ]
    }
   ],
   "source": [
    "def analyze_click_matrix(matrix: Dict, memory_log: List, generation_successful: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the generated click-to-click matrix with robust error handling\n",
    "\n",
    "    Args:\n",
    "        matrix: Generated co-visitation matrix\n",
    "        memory_log: Memory usage log during generation\n",
    "        generation_successful: Whether generation completed successfully\n",
    "\n",
    "    Returns:\n",
    "        dict: Matrix analysis results\n",
    "    \"\"\"\n",
    "    log(\"Analyzing generated click-to-click matrix...\")\n",
    "\n",
    "    if not matrix:\n",
    "        log(\"Empty or no matrix - providing basic analysis\")\n",
    "        return {\n",
    "            \"error\": \"Empty matrix\",\n",
    "            \"generation_successful\": generation_successful,\n",
    "            \"analysis_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    # Basic statistics with error handling\n",
    "    try:\n",
    "        source_items = len(matrix)\n",
    "        total_pairs = sum(len(candidates) for candidates in matrix.values()) if matrix else 0\n",
    "        avg_candidates = total_pairs / source_items if source_items > 0 else 0\n",
    "\n",
    "        log(f\"    Basic statistics:\")\n",
    "        log(f\"    Source items: {source_items:,}\")\n",
    "        log(f\"    Total pairs: {total_pairs:,}\")\n",
    "        log(f\"    Avg candidates per item: {avg_candidates:.1f}\")\n",
    "\n",
    "        # Candidate count distribution\n",
    "        candidate_counts = [len(candidates) for candidates in matrix.values()]\n",
    "\n",
    "        count_stats = {}\n",
    "        if candidate_counts:\n",
    "            count_stats = {\n",
    "                \"min\": min(candidate_counts),\n",
    "                \"max\": max(candidate_counts),\n",
    "                \"mean\": np.mean(candidate_counts),\n",
    "                \"median\": np.median(candidate_counts),\n",
    "                \"std\": np.std(candidate_counts)\n",
    "            }\n",
    "\n",
    "            log(f\"      Candidate count distribution:\")\n",
    "            log(f\"      Min: {count_stats['min']}, Max: {count_stats['max']}\")\n",
    "            log(f\"      Mean: {count_stats['mean']:.1f}, Median: {count_stats['median']:.1f}\")\n",
    "\n",
    "        # Score distribution (sample to avoid memory issues)\n",
    "        all_scores = []\n",
    "        sample_size = min(100, len(matrix))  # Smaller sample to save memory\n",
    "        sample_items = list(matrix.keys())[:sample_size]\n",
    "\n",
    "        score_stats = {}\n",
    "        try:\n",
    "            for item in sample_items:\n",
    "                if isinstance(matrix[item], list):\n",
    "                    # Handle list of tuples format [(aid, score), ...]\n",
    "                    scores = [score for _, score in matrix[item] if isinstance(score, (int, float))]\n",
    "                    all_scores.extend(scores)\n",
    "\n",
    "            if all_scores:\n",
    "                score_stats = {\n",
    "                    \"min\": min(all_scores),\n",
    "                    \"max\": max(all_scores),\n",
    "                    \"mean\": np.mean(all_scores),\n",
    "                    \"median\": np.median(all_scores)\n",
    "                }\n",
    "\n",
    "                log(f\"      Co-visitation score distribution (sample of {sample_size} items):\")\n",
    "                log(f\"      Min: {score_stats['min']}, Max: {score_stats['max']}\")\n",
    "                log(f\"      Mean: {score_stats['mean']:.1f}, Median: {score_stats['median']:.1f}\")\n",
    "\n",
    "        except Exception as score_error:\n",
    "            log(f\"      Error analyzing scores: {score_error}\")\n",
    "            score_stats = {\"error\": str(score_error)}\n",
    "\n",
    "        # Memory usage analysis\n",
    "        memory_analysis = {}\n",
    "        if memory_log:\n",
    "            try:\n",
    "                memory_usage = [entry[\"memory_percent\"] for entry in memory_log if \"memory_percent\" in entry]\n",
    "                if memory_usage:\n",
    "                    memory_analysis = {\n",
    "                        \"peak_memory_percent\": max(memory_usage),\n",
    "                        \"avg_memory_percent\": np.mean(memory_usage),\n",
    "                        \"memory_checks\": len(memory_log),\n",
    "                        \"critical_memory_events\": sum(1 for usage in memory_usage if usage > 90)\n",
    "                    }\n",
    "\n",
    "                    log(f\"      Memory usage during generation:\")\n",
    "                    log(f\"      Peak usage: {memory_analysis['peak_memory_percent']:.1f}%\")\n",
    "                    log(f\"      Average usage: {memory_analysis['avg_memory_percent']:.1f}%\")\n",
    "                    log(f\"      Critical events: {memory_analysis['critical_memory_events']}\")\n",
    "            except Exception as memory_error:\n",
    "                log(f\"      Error analyzing memory usage: {memory_error}\")\n",
    "                memory_analysis = {\"error\": str(memory_error)}\n",
    "\n",
    "        # Sample relationships for validation (smaller sample)\n",
    "        sample_relationships = {}\n",
    "        sample_count = min(5, len(matrix))  # Even smaller sample\n",
    "\n",
    "        if sample_count > 0:\n",
    "            try:\n",
    "                sample_items = list(matrix.keys())[:sample_count]\n",
    "                for item in sample_items:\n",
    "                    if isinstance(matrix[item], list):\n",
    "                        # Take top 3 candidates only for display\n",
    "                        top_candidates = matrix[item][:3]\n",
    "                        sample_relationships[str(item)] = top_candidates\n",
    "\n",
    "                log(f\"   Sample relationships:\")\n",
    "                for item, candidates in list(sample_relationships.items())[:3]:\n",
    "                    if candidates:\n",
    "                        candidate_str = \", \".join([f\"{aid}({score})\" for aid, score in candidates])\n",
    "                        log(f\"      Item {item} â†’ {candidate_str}\")\n",
    "            except Exception as sample_error:\n",
    "                log(f\"      Error creating sample relationships: {sample_error}\")\n",
    "                sample_relationships = {\"error\": str(sample_error)}\n",
    "\n",
    "        # Quality assessment\n",
    "        quality_metrics = {\n",
    "            \"has_data\": source_items > 0,\n",
    "            \"reasonable_size\": source_items > 100,\n",
    "            \"good_coverage\": total_pairs > 1000,\n",
    "            \"balanced_candidates\": count_stats.get(\"mean\", 0) > 1 if count_stats else False\n",
    "        }\n",
    "\n",
    "        overall_quality = sum(quality_metrics.values())\n",
    "        quality_level = \"EXCELLENT\" if overall_quality >= 4 else \"GOOD\" if overall_quality >= 3 else \"FAIR\" if overall_quality >= 2 else \"POOR\"\n",
    "\n",
    "        log(f\"   Quality Assessment: {quality_level} ({overall_quality}/4 criteria met)\")\n",
    "\n",
    "        analysis_results = {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"generation_successful\": generation_successful,\n",
    "            \"matrix_size\": {\n",
    "                \"source_items\": source_items,\n",
    "                \"total_pairs\": total_pairs,\n",
    "                \"avg_candidates_per_item\": avg_candidates\n",
    "            },\n",
    "            \"candidate_count_stats\": count_stats,\n",
    "            \"score_distribution\": score_stats,\n",
    "            \"memory_usage\": memory_analysis,\n",
    "            \"sample_relationships\": sample_relationships,\n",
    "            \"quality_metrics\": quality_metrics,\n",
    "            \"overall_quality\": quality_level\n",
    "        }\n",
    "\n",
    "        log(\"Matrix analysis completed!\")\n",
    "        return analysis_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error during matrix analysis: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"generation_successful\": generation_successful,\n",
    "            \"analysis_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Analyze the generated matrix with robust error handling\n",
    "try:\n",
    "    matrix_analysis = analyze_click_matrix(click_to_click_matrix, memory_log, generation_successful)\n",
    "except Exception as analysis_error:\n",
    "    log(f\"Critical error in matrix analysis: {analysis_error}\")\n",
    "    matrix_analysis = {\n",
    "        \"error\": str(analysis_error),\n",
    "        \"generation_successful\": generation_successful,\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"matrix_size\": {\n",
    "            \"source_items\": len(click_to_click_matrix) if click_to_click_matrix else 0,\n",
    "            \"total_pairs\": 0,\n",
    "            \"avg_candidates_per_item\": 0\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8tWhTbk8XMm"
   },
   "source": [
    "## SAVE OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0NYVCSa8Y2U",
    "outputId": "bbc72e12-f609-4cd4-dd7e-afb22057ec2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 17:24:36] Saving click-to-click matrix outputs...\n",
      "[2025-08-07 17:25:34]    click_to_click_matrix.pkl saved (556.6 MB)\n",
      "[2025-08-07 17:25:34]    click_matrix_statistics.json saved\n",
      "[2025-08-07 17:25:35]    click_matrix_samples.json saved\n",
      "[2025-08-07 17:25:35]    memory_usage_log.json saved\n",
      "[2025-08-07 17:25:35]    part_2a2_summary.json saved\n",
      "[2025-08-07 17:25:35] All click matrix outputs saved successfully!\n",
      "[2025-08-07 17:25:35] Output saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "## SAVE OUTPUTS (Fixed JSON Serialization)\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert non-JSON serializable objects to JSON-compatible types\n",
    "\n",
    "    Args:\n",
    "        obj: Object to convert\n",
    "\n",
    "    Returns:\n",
    "        JSON-serializable version of the object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # Handle numpy scalars\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_click_matrix_outputs(matrix: Dict,\n",
    "                             analysis: Dict,\n",
    "                             memory_log: List,\n",
    "                             validation_results: Dict):\n",
    "    \"\"\"\n",
    "    Save all outputs from click-to-click matrix generation with proper JSON handling\n",
    "\n",
    "    Args:\n",
    "        matrix: Generated co-visitation matrix\n",
    "        analysis: Matrix analysis results\n",
    "        memory_log: Memory usage log\n",
    "        validation_results: Input validation results\n",
    "    \"\"\"\n",
    "    log(\"Saving click-to-click matrix outputs...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Save click-to-click matrix (main output)\n",
    "        matrix_path = f\"{config.OUTPUT_PATH}/click_to_click_matrix.pkl\"\n",
    "        with open(matrix_path, \"wb\") as f:\n",
    "            pickle.dump(matrix, f)\n",
    "\n",
    "        file_size = os.path.getsize(matrix_path) / (1024*1024)\n",
    "        log(f\"   click_to_click_matrix.pkl saved ({file_size:.1f} MB)\")\n",
    "\n",
    "        # 2. Save matrix statistics (with JSON serialization fix)\n",
    "        stats_path = f\"{config.OUTPUT_PATH}/click_matrix_statistics.json\"\n",
    "\n",
    "        # Convert analysis to JSON-serializable format\n",
    "        json_safe_analysis = convert_to_json_serializable(analysis)\n",
    "\n",
    "        with open(stats_path, \"w\") as f:\n",
    "            json.dump(json_safe_analysis, f, indent=2, default=str)\n",
    "        log(f\"   click_matrix_statistics.json saved\")\n",
    "\n",
    "        # 3. Save sample relationships for validation\n",
    "        samples_path = f\"{config.OUTPUT_PATH}/click_matrix_samples.json\"\n",
    "\n",
    "        # Prepare sample data with JSON conversion\n",
    "        sample_data = {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"sample_relationships\": convert_to_json_serializable(analysis.get(\"sample_relationships\", {})),\n",
    "            \"matrix_size\": convert_to_json_serializable(analysis.get(\"matrix_size\", {})),\n",
    "            \"top_source_items\": [int(item) for item in list(matrix.keys())[:20]] if matrix else []\n",
    "        }\n",
    "\n",
    "        with open(samples_path, \"w\") as f:\n",
    "            json.dump(sample_data, f, indent=2, default=str)\n",
    "        log(f\"   click_matrix_samples.json saved\")\n",
    "\n",
    "        # 4. Save memory usage log (with JSON conversion)\n",
    "        memory_path = f\"{config.OUTPUT_PATH}/memory_usage_log.json\"\n",
    "\n",
    "        # Convert memory log and analysis to JSON-safe format\n",
    "        json_safe_memory_log = convert_to_json_serializable(memory_log)\n",
    "        json_safe_memory_analysis = convert_to_json_serializable(analysis.get(\"memory_usage\", {}))\n",
    "\n",
    "        memory_data = {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"memory_log\": json_safe_memory_log,\n",
    "            \"memory_analysis\": json_safe_memory_analysis,\n",
    "            \"generation_successful\": bool(analysis.get(\"generation_successful\", False))\n",
    "        }\n",
    "\n",
    "        with open(memory_path, \"w\") as f:\n",
    "            json.dump(memory_data, f, indent=2, default=str)\n",
    "        log(f\"   memory_usage_log.json saved\")\n",
    "\n",
    "        # 5. Save comprehensive summary (with all JSON conversions)\n",
    "        summary = {\n",
    "            \"notebook\": \"Part 2A2: Click-to-Click Matrix Generation\",\n",
    "            \"completion_timestamp\": datetime.now().isoformat(),\n",
    "            \"generation_successful\": bool(analysis.get(\"generation_successful\", False)),\n",
    "            \"inputs_used\": {\n",
    "                \"covisit_data_prepared.parquet\": \"Click events from prepared data\",\n",
    "                \"chunking_strategy.json\": f\"Chunk size: {validation_results.get('chunk_size', 'unknown'):,}\",\n",
    "                \"session_analysis.json\": \"Session insights for optimization\"\n",
    "            },\n",
    "            \"outputs_generated\": {\n",
    "                \"click_to_click_matrix.pkl\": f\"{analysis.get('matrix_size', {}).get('source_items', 0):,} source items\",\n",
    "                \"click_matrix_statistics.json\": \"Comprehensive matrix analysis\",\n",
    "                \"click_matrix_samples.json\": \"Sample relationships for validation\",\n",
    "                \"memory_usage_log.json\": \"Memory usage tracking during generation\"\n",
    "            },\n",
    "            \"key_metrics\": convert_to_json_serializable({\n",
    "                \"source_items\": analysis.get(\"matrix_size\", {}).get(\"source_items\", 0),\n",
    "                \"total_pairs\": analysis.get(\"matrix_size\", {}).get(\"total_pairs\", 0),\n",
    "                \"peak_memory_percent\": analysis.get(\"memory_usage\", {}).get(\"peak_memory_percent\", 0),\n",
    "                \"matrix_file_size_mb\": float(file_size),\n",
    "                \"overall_quality\": analysis.get(\"overall_quality\", \"UNKNOWN\")\n",
    "            }),\n",
    "            \"quality_assessment\": convert_to_json_serializable(analysis.get(\"quality_metrics\", {})),\n",
    "            \"next_step\": \"Run Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\" if analysis.get(\"generation_successful\", False) else \"Review errors and retry\"\n",
    "        }\n",
    "\n",
    "        summary_path = f\"{config.OUTPUT_PATH}/part_2a2_summary.json\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        log(f\"   part_2a2_summary.json saved\")\n",
    "\n",
    "        log(\"All click matrix outputs saved successfully!\")\n",
    "\n",
    "        return {\n",
    "            \"matrix_path\": matrix_path,\n",
    "            \"stats_path\": stats_path,\n",
    "            \"samples_path\": samples_path,\n",
    "            \"memory_path\": memory_path,\n",
    "            \"summary_path\": summary_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error saving outputs: {e}\")\n",
    "        log(f\"Error type: {type(e).__name__}\")\n",
    "        log(f\"Attempting to save with fallback methods...\")\n",
    "\n",
    "        # Fallback: try to save essential files only\n",
    "        try:\n",
    "            fallback_paths = {}\n",
    "\n",
    "            # Save matrix (most important)\n",
    "            if matrix:\n",
    "                matrix_path = f\"{config.OUTPUT_PATH}/click_to_click_matrix.pkl\"\n",
    "                with open(matrix_path, \"wb\") as f:\n",
    "                    pickle.dump(matrix, f)\n",
    "                fallback_paths[\"matrix_path\"] = matrix_path\n",
    "                log(f\"   Fallback: click_to_click_matrix.pkl saved\")\n",
    "\n",
    "            # Save basic summary as text (avoiding JSON issues)\n",
    "            summary_txt_path = f\"{config.OUTPUT_PATH}/part_2a2_summary.txt\"\n",
    "            with open(summary_txt_path, \"w\") as f:\n",
    "                f.write(f\"Part 2A2 Summary\\n\")\n",
    "                f.write(f\"Generated: {datetime.now().isoformat()}\\n\")\n",
    "                f.write(f\"Generation Successful: {analysis.get('generation_successful', False)}\\n\")\n",
    "                f.write(f\"Source Items: {analysis.get('matrix_size', {}).get('source_items', 0)}\\n\")\n",
    "                f.write(f\"Total Pairs: {analysis.get('matrix_size', {}).get('total_pairs', 0)}\\n\")\n",
    "                f.write(f\"Matrix File Size: {file_size:.1f} MB\\n\")\n",
    "            fallback_paths[\"summary_txt_path\"] = summary_txt_path\n",
    "            log(f\"   Fallback: part_2a2_summary.txt saved\")\n",
    "\n",
    "            return fallback_paths\n",
    "\n",
    "        except Exception as fallback_error:\n",
    "            log(f\"Fallback save also failed: {fallback_error}\")\n",
    "            raise e\n",
    "\n",
    "# Save all outputs with error handling\n",
    "try:\n",
    "    output_paths = save_click_matrix_outputs(click_to_click_matrix, matrix_analysis, memory_log, validation_results)\n",
    "    log(\"Output saving completed successfully!\")\n",
    "except Exception as save_error:\n",
    "    log(f\"Critical error saving outputs: {save_error}\")\n",
    "    # Create minimal output paths for summary section\n",
    "    output_paths = {\n",
    "        \"matrix_path\": f\"{config.OUTPUT_PATH}/click_to_click_matrix.pkl\",\n",
    "        \"stats_path\": f\"{config.OUTPUT_PATH}/click_matrix_statistics.json\",\n",
    "        \"samples_path\": f\"{config.OUTPUT_PATH}/click_matrix_samples.json\",\n",
    "        \"memory_path\": f\"{config.OUTPUT_PATH}/memory_usage_log.json\",\n",
    "        \"summary_path\": f\"{config.OUTPUT_PATH}/part_2a2_summary.json\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezgmsf-18eGE"
   },
   "source": [
    "## FINAL SUMMARY AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKKTxkJY8e8e",
    "outputId": "0c0339a9-29fe-4dbd-87f6-3aae7fe695b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 17:25:40] \n",
      "================================================================================\n",
      "[2025-08-07 17:25:40] PART 2A2 COMPLETED: CLICK-TO-CLICK MATRIX GENERATION\n",
      "[2025-08-07 17:25:40] ================================================================================\n",
      "[2025-08-07 17:25:40] \n",
      " MATRIX GENERATION SUCCESSFUL\n",
      "[2025-08-07 17:25:40] Source items: 1,839,483\n",
      "[2025-08-07 17:25:40] Total pairs: 63,503,324\n",
      "[2025-08-07 17:25:40] Avg candidates per item: 34.5\n",
      "[2025-08-07 17:25:40] Matrix file size: 556.6 MB\n",
      "[2025-08-07 17:25:40] \n",
      " MEMORY USAGE SUMMARY:\n",
      "[2025-08-07 17:25:40] Peak memory usage: 31.4%\n",
      "[2025-08-07 17:25:40] Average memory usage: 18.1%\n",
      "[2025-08-07 17:25:40] Critical memory events: 0\n",
      "[2025-08-07 17:25:40] \n",
      " QUALITY ASSESSMENT: EXCELLENT\n",
      "[2025-08-07 17:25:40] Has data: yes\n",
      "[2025-08-07 17:25:40] Reasonable size (>100 items): yes\n",
      "[2025-08-07 17:25:40] Good coverage (>1k pairs): yes\n",
      "[2025-08-07 17:25:40] Balanced candidates: yes\n",
      "[2025-08-07 17:25:40] \n",
      " OUTPUT FILES GENERATED:\n",
      "[2025-08-07 17:25:40]     click_to_click_matrix.pkl (556.6 MB)\n",
      "[2025-08-07 17:25:40]     click_matrix_statistics.json\n",
      "[2025-08-07 17:25:40]     click_matrix_samples.json\n",
      "[2025-08-07 17:25:40]     memory_usage_log.json\n",
      "[2025-08-07 17:25:40]     part_2a2_summary.json\n",
      "[2025-08-07 17:25:40] Files location: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-07 17:25:40] \n",
      "ðŸŽ¯ RECOMMENDATIONS:\n",
      "[2025-08-07 17:25:40]     Matrix generation completed successfully\n",
      "[2025-08-07 17:25:40]      Proceed to Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\n",
      "[2025-08-07 17:25:40]     Matrix quality is EXCELLENT - suitable for recommendations\n",
      "[2025-08-07 17:25:40] \n",
      " PERFORMANCE METRICS:\n",
      "[2025-08-07 17:25:40]    Processing speed: 0.0 sessions/second\n",
      "[2025-08-07 17:25:41]    Total runtime: 66.3 minutes\n",
      "[2025-08-07 17:25:41]     Processing speed is slow - consider optimizing session filtering\n",
      "[2025-08-07 17:25:41] \n",
      " PERFORMING FINAL CLEANUP...\n",
      "[2025-08-07 17:25:41]    Cleared matrix from memory (1,839,483 items)\n",
      "[2025-08-07 17:25:41]    Cleared generator from memory\n",
      "[2025-08-07 17:26:15]    Final memory status: LOW\n",
      "[2025-08-07 17:26:15] \n",
      " Part 2A2 processing completed!\n",
      "[2025-08-07 17:26:15] Check all output files in: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-07 17:26:15] Execution summary saved: part_2a2_execution_summary.json\n"
     ]
    }
   ],
   "source": [
    "## FINAL SUMMARY AND NEXT STEPS (Robust Version)\n",
    "\n",
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2A2 COMPLETED: CLICK-TO-CLICK MATRIX GENERATION\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Generation status\n",
    "if generation_successful and matrix_analysis.get(\"matrix_size\"):\n",
    "    log(f\"\\n MATRIX GENERATION SUCCESSFUL\")\n",
    "    matrix_size = matrix_analysis[\"matrix_size\"]\n",
    "    log(f\"Source items: {matrix_size['source_items']:,}\")\n",
    "    log(f\"Total pairs: {matrix_size['total_pairs']:,}\")\n",
    "    log(f\"Avg candidates per item: {matrix_size['avg_candidates_per_item']:.1f}\")\n",
    "\n",
    "    if 'output_paths' in locals() and 'matrix_path' in output_paths:\n",
    "        matrix_file_size = os.path.getsize(output_paths['matrix_path']) / (1024*1024)\n",
    "        log(f\"Matrix file size: {matrix_file_size:.1f} MB\")\n",
    "else:\n",
    "    log(f\"\\n  MATRIX GENERATION INCOMPLETE\")\n",
    "    if click_to_click_matrix:\n",
    "        log(f\"Partial results: {len(click_to_click_matrix):,} source items\")\n",
    "        total_partial_pairs = sum(len(candidates) for candidates in click_to_click_matrix.values())\n",
    "        log(f\"Partial pairs: {total_partial_pairs:,}\")\n",
    "    else:\n",
    "        log(f\"No matrix data generated\")\n",
    "\n",
    "# Memory usage summary\n",
    "if matrix_analysis.get(\"memory_usage\"):\n",
    "    memory_stats = matrix_analysis[\"memory_usage\"]\n",
    "    log(f\"\\n MEMORY USAGE SUMMARY:\")\n",
    "    if \"error\" not in memory_stats:\n",
    "        log(f\"Peak memory usage: {memory_stats.get('peak_memory_percent', 0):.1f}%\")\n",
    "        log(f\"Average memory usage: {memory_stats.get('avg_memory_percent', 0):.1f}%\")\n",
    "        log(f\"Critical memory events: {memory_stats.get('critical_memory_events', 0)}\")\n",
    "\n",
    "        if memory_stats.get('critical_memory_events', 0) > 0:\n",
    "            log(f\"  Memory constraints detected - consider using smaller chunks or more selective filtering\")\n",
    "    else:\n",
    "        log(f\"Memory analysis error: {memory_stats['error']}\")\n",
    "\n",
    "# Quality assessment\n",
    "if matrix_analysis.get(\"quality_metrics\"):\n",
    "    quality = matrix_analysis.get(\"overall_quality\", \"UNKNOWN\")\n",
    "    log(f\"\\n QUALITY ASSESSMENT: {quality}\")\n",
    "\n",
    "    metrics = matrix_analysis[\"quality_metrics\"]\n",
    "    log(f\"Has data: {'yes' if metrics.get('has_data') else 'no'}\")\n",
    "    log(f\"Reasonable size (>100 items): {'yes' if metrics.get('reasonable_size') else 'no'}\")\n",
    "    log(f\"Good coverage (>1k pairs): {'yes' if metrics.get('good_coverage') else 'no'}\")\n",
    "    log(f\"Balanced candidates: {'yes' if metrics.get('balanced_candidates') else 'no'}\")\n",
    "\n",
    "# Output files\n",
    "log(f\"\\n OUTPUT FILES GENERATED:\")\n",
    "try:\n",
    "    if 'output_paths' in locals():\n",
    "        for description, path in output_paths.items():\n",
    "            filename = os.path.basename(path)\n",
    "            if os.path.exists(path):\n",
    "                if path.endswith('.pkl'):\n",
    "                    file_size = os.path.getsize(path) / (1024*1024)\n",
    "                    log(f\"    {filename} ({file_size:.1f} MB)\")\n",
    "                else:\n",
    "                    log(f\"    {filename}\")\n",
    "            else:\n",
    "                log(f\"    {filename} (not found)\")\n",
    "        log(f\"Files location: {config.OUTPUT_PATH}\")\n",
    "    else:\n",
    "        log(\"   No output paths available - files may not have been saved\")\n",
    "except Exception as file_error:\n",
    "    log(f\"   Error checking output files: {file_error}\")\n",
    "\n",
    "# Recommendations based on results\n",
    "log(f\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
    "\n",
    "if generation_successful:\n",
    "    log(f\"    Matrix generation completed successfully\")\n",
    "    log(f\"     Proceed to Part 2A3: Click-to-Buy & Buy-to-Buy Matrix Generation\")\n",
    "    log(f\"    Matrix quality is {matrix_analysis.get('overall_quality', 'UNKNOWN')} - suitable for recommendations\")\n",
    "else:\n",
    "    log(f\"     Matrix generation incomplete due to memory constraints\")\n",
    "    log(f\"    TROUBLESHOOTING OPTIONS:\")\n",
    "    log(f\"      1. Reduce chunk size (currently {validation_results.get('chunk_size', 'unknown'):,})\")\n",
    "    log(f\"      2. Increase MAX_CANDIDATES_PER_ITEM limit (currently {config.MAX_CANDIDATES_PER_ITEM})\")\n",
    "    log(f\"      3. Use more aggressive session filtering (skip sessions >50 items)\")\n",
    "    log(f\"      4. Process in smaller time windows\")\n",
    "\n",
    "    if click_to_click_matrix:\n",
    "        log(f\"     Partial results available - consider using for testing\")\n",
    "        log(f\"     Can proceed with caution to Part 2A3 using partial data\")\n",
    "\n",
    "# Memory optimization suggestions\n",
    "memory_peak = matrix_analysis.get(\"memory_usage\", {}).get(\"peak_memory_percent\", 0)\n",
    "if memory_peak > 90:\n",
    "    log(f\"    MEMORY OPTIMIZATION SUGGESTIONS:\")\n",
    "    log(f\"      - Peak memory usage was {memory_peak:.1f}% - very close to limit\")\n",
    "    log(f\"      - Consider reducing chunk_size from {validation_results.get('chunk_size', 50000):,} to 25,000\")\n",
    "    log(f\"      - Implement more aggressive session filtering\")\n",
    "    log(f\"      - Use shorter time windows for co-visitation\")\n",
    "\n",
    "# Performance metrics\n",
    "try:\n",
    "    if 'generation_time' in locals() and generation_time > 0:\n",
    "        sessions_per_second = validation_results.get(\"processed_sessions\", 0) / generation_time\n",
    "        log(f\"\\n PERFORMANCE METRICS:\")\n",
    "        log(f\"   Processing speed: {sessions_per_second:.1f} sessions/second\")\n",
    "        log(f\"   Total runtime: {generation_time/60:.1f} minutes\")\n",
    "\n",
    "        if sessions_per_second < 100:\n",
    "            log(f\"    Processing speed is slow - consider optimizing session filtering\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Emergency cleanup\n",
    "log(f\"\\n PERFORMING FINAL CLEANUP...\")\n",
    "try:\n",
    "    # Clean up any remaining variables\n",
    "    if 'click_to_click_matrix' in locals():\n",
    "        matrix_items = len(click_to_click_matrix)\n",
    "        del click_to_click_matrix\n",
    "        log(f\"   Cleared matrix from memory ({matrix_items:,} items)\")\n",
    "\n",
    "    if 'generator' in locals():\n",
    "        if hasattr(generator, 'covisitation_counts'):\n",
    "            generator.covisitation_counts.clear()\n",
    "        del generator\n",
    "        log(f\"   Cleared generator from memory\")\n",
    "\n",
    "    # Force aggressive garbage collection\n",
    "    for i in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "    final_memory_status = check_memory()\n",
    "    log(f\"   Final memory status: {final_memory_status}\")\n",
    "\n",
    "except Exception as cleanup_error:\n",
    "    log(f\"   Cleanup error: {cleanup_error}\")\n",
    "\n",
    "log(f\"\\n Part 2A2 processing completed!\")\n",
    "log(f\"Check all output files in: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Save execution summary\n",
    "try:\n",
    "    execution_summary = {\n",
    "        \"notebook\": \"Part 2A2: Click-to-Click Matrix Generation\",\n",
    "        \"completion_timestamp\": datetime.now().isoformat(),\n",
    "        \"generation_successful\": generation_successful,\n",
    "        \"matrix_quality\": matrix_analysis.get(\"overall_quality\", \"UNKNOWN\"),\n",
    "        \"peak_memory_percent\": matrix_analysis.get(\"memory_usage\", {}).get(\"peak_memory_percent\", 0),\n",
    "        \"source_items_generated\": matrix_analysis.get(\"matrix_size\", {}).get(\"source_items\", 0),\n",
    "        \"total_pairs_generated\": matrix_analysis.get(\"matrix_size\", {}).get(\"total_pairs\", 0),\n",
    "        \"processing_time_minutes\": locals().get('generation_time', 0) / 60,\n",
    "        \"recommendations\": \"Proceed to Part 2A3\" if generation_successful else \"Review and optimize parameters\"\n",
    "    }\n",
    "\n",
    "    with open(f\"{config.OUTPUT_PATH}/part_2a2_execution_summary.json\", \"w\") as f:\n",
    "        json.dump(execution_summary, f, indent=2)\n",
    "\n",
    "    log(f\"Execution summary saved: part_2a2_execution_summary.json\")\n",
    "\n",
    "except Exception as summary_error:\n",
    "    log(f\"Could not save execution summary: {summary_error}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
