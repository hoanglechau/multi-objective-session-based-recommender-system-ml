{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8I-Zie_K9g5"
   },
   "source": [
    "# Part 2A4 Matrix Consolidation & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-q3GbY_8KE1Q",
    "outputId": "be7a684e-7d5c-49a7-c4aa-529cfc206816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars==0.20.31\n",
      "  Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: polars\n",
      "  Attempting uninstall: polars\n",
      "    Found existing installation: polars 1.25.2\n",
      "    Uninstalling polars-1.25.2:\n",
      "      Successfully uninstalled polars-1.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 0.20.31 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed polars-0.20.31\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install polars==0.20.31\n",
    "!pip install psutil\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0vRvC7ILB9f",
    "outputId": "fe1e853a-ecb9-475c-d5a4-f5564b1a208e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sJpnkNkgLDtW"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output'\n",
    "\n",
    "    # Consolidation parameters\n",
    "    MIN_CANDIDATES_PER_MATRIX = 5     # Minimum candidates required per matrix type\n",
    "    MAX_CANDIDATES_PER_ITEM = 40      # Maximum candidates to keep per source item\n",
    "    QUALITY_THRESHOLD_COVERAGE = 0.1  # Minimum coverage threshold for quality assessment\n",
    "\n",
    "    # Validation parameters\n",
    "    SAMPLE_SIZE_FOR_VALIDATION = 1000  # Number of items to sample for detailed validation\n",
    "    CROSS_VALIDATION_SAMPLE = 500     # Sample size for cross-matrix validation\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3OQ9TjBLF9H"
   },
   "source": [
    "## LOGGING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTtNbBsvLIh0",
    "outputId": "1cd2c1ab-a1ad-4b16-c5fb-87c3233f541b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:53:45] ================================================================================\n",
      "[2025-08-07 18:53:48] OTTO PART 2A4: MATRIX CONSOLIDATION & VALIDATION STARTED\n",
      "[2025-08-07 18:53:48] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Setup comprehensive logging for matrix consolidation\"\"\"\n",
    "    log_file = f\"{config.OUTPUT_PATH}/matrix_consolidation_log.txt\"\n",
    "\n",
    "    def log_message(message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        print(log_entry)\n",
    "\n",
    "        # Also write to file\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_entry + \"\\n\")\n",
    "\n",
    "    def check_memory_usage():\n",
    "        \"\"\"Check current memory usage\"\"\"\n",
    "        memory = psutil.virtual_memory()\n",
    "        memory_pct = memory.percent\n",
    "        available_gb = memory.available / (1024**3)\n",
    "\n",
    "        if memory_pct > 75:\n",
    "            log_message(f\"Memory usage: {memory_pct:.1f}% used, {available_gb:.1f} GB available\")\n",
    "\n",
    "        return memory_pct\n",
    "\n",
    "    return log_message, check_memory_usage\n",
    "\n",
    "log, check_memory = setup_logging()\n",
    "\n",
    "log(\"=\"*80)\n",
    "log(\"OTTO PART 2A4: MATRIX CONSOLIDATION & VALIDATION STARTED\")\n",
    "log(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSPBRijzLLW5"
   },
   "source": [
    "## INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qw5Mz779LNVO"
   },
   "outputs": [],
   "source": [
    "def validate_input_files():\n",
    "    \"\"\"\n",
    "    Validate that all required input files exist and are accessible\n",
    "\n",
    "    Returns:\n",
    "        dict: File validation results with sizes and status\n",
    "    \"\"\"\n",
    "    log(\"Validating input files from previous notebooks...\")\n",
    "\n",
    "    # Required input files with their sources\n",
    "    required_files = {\n",
    "        \"click_to_click_matrix.pkl\": \"Click-to-click co-visitation matrix from Part 2A2\",\n",
    "        \"click_to_buy_matrix.pkl\": \"Click-to-buy co-visitation matrix from Part 2A3\",\n",
    "        \"buy_to_buy_matrix.pkl\": \"Buy-to-buy co-visitation matrix from Part 2A3\",\n",
    "        \"covisit_data_prepared.parquet\": \"Optimized training data from Part 2A1\",\n",
    "        \"session_analysis.json\": \"Session analysis results from Part 2A1\"\n",
    "    }\n",
    "\n",
    "    # Optional files (may not exist depending on previous runs)\n",
    "    optional_files = {\n",
    "        \"click_matrix_statistics.json\": \"Click matrix statistics from Part 2A2\",\n",
    "        \"buy_matrices_statistics.json\": \"Buy matrices statistics from Part 2A3\",\n",
    "        \"item_stats.parquet\": \"Item statistics from Part 1\"\n",
    "    }\n",
    "\n",
    "    validation_results = {\n",
    "        \"required_files\": {},\n",
    "        \"optional_files\": {},\n",
    "        \"missing_required\": [],\n",
    "        \"missing_optional\": [],\n",
    "        \"total_input_size_mb\": 0\n",
    "    }\n",
    "\n",
    "    # Check required files\n",
    "    log(\"   Checking required files:\")\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if os.path.exists(filepath):\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "            validation_results[\"required_files\"][filename] = {\n",
    "                \"exists\": True,\n",
    "                \"size_mb\": file_size,\n",
    "                \"description\": description\n",
    "            }\n",
    "            validation_results[\"total_input_size_mb\"] += file_size\n",
    "            log(f\"       {filename} - {file_size:.1f} MB\")\n",
    "        else:\n",
    "            validation_results[\"required_files\"][filename] = {\n",
    "                \"exists\": False,\n",
    "                \"size_mb\": 0,\n",
    "                \"description\": description\n",
    "            }\n",
    "            validation_results[\"missing_required\"].append(filename)\n",
    "            log(f\"       {filename} - MISSING\")\n",
    "\n",
    "    # Check optional files\n",
    "    log(\"   Checking optional files:\")\n",
    "    for filename, description in optional_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if os.path.exists(filepath):\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "            validation_results[\"optional_files\"][filename] = {\n",
    "                \"exists\": True,\n",
    "                \"size_mb\": file_size,\n",
    "                \"description\": description\n",
    "            }\n",
    "            validation_results[\"total_input_size_mb\"] += file_size\n",
    "            log(f\"       {filename} - {file_size:.1f} MB\")\n",
    "        else:\n",
    "            validation_results[\"optional_files\"][filename] = {\n",
    "                \"exists\": False,\n",
    "                \"size_mb\": 0,\n",
    "                \"description\": description\n",
    "            }\n",
    "            validation_results[\"missing_optional\"].append(filename)\n",
    "            log(f\"       {filename} - optional, not found\")\n",
    "\n",
    "    # Validation summary\n",
    "    missing_required_count = len(validation_results[\"missing_required\"])\n",
    "    if missing_required_count > 0:\n",
    "        log(f\"   ERROR: {missing_required_count} required files are missing!\")\n",
    "        for missing_file in validation_results[\"missing_required\"]:\n",
    "            log(f\"       {missing_file}\")\n",
    "        log(\"   Please run the previous notebooks (Part 2A1, 2A2, 2A3) to generate required files.\")\n",
    "        raise FileNotFoundError(\"Required input files are missing!\")\n",
    "\n",
    "    log(f\"   SUCCESS: All required files found (Total size: {validation_results['total_input_size_mb']:.1f} MB)\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "def load_co_visitation_matrices():\n",
    "    \"\"\"\n",
    "    Load all co-visitation matrices from previous notebooks\n",
    "\n",
    "    Returns:\n",
    "        tuple: (click_to_click_matrix, click_to_buy_matrix, buy_to_buy_matrix, load_stats)\n",
    "    \"\"\"\n",
    "    log(\"Loading co-visitation matrices...\")\n",
    "\n",
    "    load_stats = {}\n",
    "\n",
    "    try:\n",
    "        # Load click-to-click matrix\n",
    "        log(\"   Loading click-to-click matrix...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/click_to_click_matrix.pkl\", \"rb\") as f:\n",
    "            click_to_click_matrix = pickle.load(f)\n",
    "\n",
    "        ctc_source_items = len(click_to_click_matrix) if click_to_click_matrix else 0\n",
    "        ctc_total_pairs = sum(len(candidates) for candidates in click_to_click_matrix.values()) if click_to_click_matrix else 0\n",
    "\n",
    "        load_stats[\"click_to_click\"] = {\n",
    "            \"source_items\": ctc_source_items,\n",
    "            \"total_pairs\": ctc_total_pairs,\n",
    "            \"loaded_successfully\": True\n",
    "        }\n",
    "\n",
    "        log(f\"       Click-to-click: {ctc_source_items:,} source items, {ctc_total_pairs:,} pairs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"       Failed to load click-to-click matrix: {e}\")\n",
    "        click_to_click_matrix = {}\n",
    "        load_stats[\"click_to_click\"] = {\"source_items\": 0, \"total_pairs\": 0, \"loaded_successfully\": False, \"error\": str(e)}\n",
    "\n",
    "    try:\n",
    "        # Load click-to-buy matrix\n",
    "        log(\"   Loading click-to-buy matrix...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/click_to_buy_matrix.pkl\", \"rb\") as f:\n",
    "            click_to_buy_matrix = pickle.load(f)\n",
    "\n",
    "        ctb_source_items = len(click_to_buy_matrix) if click_to_buy_matrix else 0\n",
    "        ctb_total_pairs = sum(len(candidates) for candidates in click_to_buy_matrix.values()) if click_to_buy_matrix else 0\n",
    "\n",
    "        load_stats[\"click_to_buy\"] = {\n",
    "            \"source_items\": ctb_source_items,\n",
    "            \"total_pairs\": ctb_total_pairs,\n",
    "            \"loaded_successfully\": True\n",
    "        }\n",
    "\n",
    "        log(f\"       Click-to-buy: {ctb_source_items:,} source items, {ctb_total_pairs:,} pairs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"       Failed to load click-to-buy matrix: {e}\")\n",
    "        click_to_buy_matrix = {}\n",
    "        load_stats[\"click_to_buy\"] = {\"source_items\": 0, \"total_pairs\": 0, \"loaded_successfully\": False, \"error\": str(e)}\n",
    "\n",
    "    try:\n",
    "        # Load buy-to-buy matrix\n",
    "        log(\"   Loading buy-to-buy matrix...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/buy_to_buy_matrix.pkl\", \"rb\") as f:\n",
    "            buy_to_buy_matrix = pickle.load(f)\n",
    "\n",
    "        btb_source_items = len(buy_to_buy_matrix) if buy_to_buy_matrix else 0\n",
    "        btb_total_pairs = sum(len(candidates) for candidates in buy_to_buy_matrix.values()) if buy_to_buy_matrix else 0\n",
    "\n",
    "        load_stats[\"buy_to_buy\"] = {\n",
    "            \"source_items\": btb_source_items,\n",
    "            \"total_pairs\": btb_total_pairs,\n",
    "            \"loaded_successfully\": True\n",
    "        }\n",
    "\n",
    "        log(f\"       Buy-to-buy: {btb_source_items:,} source items, {btb_total_pairs:,} pairs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"       Failed to load buy-to-buy matrix: {e}\")\n",
    "        buy_to_buy_matrix = {}\n",
    "        load_stats[\"buy_to_buy\"] = {\"source_items\": 0, \"total_pairs\": 0, \"loaded_successfully\": False, \"error\": str(e)}\n",
    "\n",
    "    # Summary\n",
    "    successful_loads = sum(1 for stats in load_stats.values() if stats[\"loaded_successfully\"])\n",
    "    total_source_items = sum(stats[\"source_items\"] for stats in load_stats.values())\n",
    "    total_pairs = sum(stats[\"total_pairs\"] for stats in load_stats.values())\n",
    "\n",
    "    log(f\"   Matrix loading summary:\")\n",
    "    log(f\"      Successfully loaded: {successful_loads}/3 matrices\")\n",
    "    log(f\"      Total source items: {total_source_items:,}\")\n",
    "    log(f\"      Total pairs: {total_pairs:,}\")\n",
    "\n",
    "    if successful_loads == 0:\n",
    "        raise ValueError(\"No co-visitation matrices could be loaded!\")\n",
    "\n",
    "    return click_to_click_matrix, click_to_buy_matrix, buy_to_buy_matrix, load_stats\n",
    "\n",
    "def load_supporting_data():\n",
    "    \"\"\"\n",
    "    Load supporting data and statistics from previous notebooks\n",
    "\n",
    "    Returns:\n",
    "        dict: Supporting data including session analysis and matrix statistics\n",
    "    \"\"\"\n",
    "    log(\"Loading supporting data and statistics...\")\n",
    "\n",
    "    supporting_data = {}\n",
    "\n",
    "    try:\n",
    "        # Load session analysis\n",
    "        log(\"   Loading session analysis...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/session_analysis.json\", \"r\") as f:\n",
    "            supporting_data[\"session_analysis\"] = json.load(f)\n",
    "        log(f\"       Session analysis loaded\")\n",
    "    except Exception as e:\n",
    "        log(f\"       Failed to load session analysis: {e}\")\n",
    "        supporting_data[\"session_analysis\"] = {}\n",
    "\n",
    "    try:\n",
    "        # Load prepared data for validation\n",
    "        log(\"   Loading prepared training data...\")\n",
    "        prepared_data = pl.read_parquet(f\"{config.OUTPUT_PATH}/covisit_data_prepared.parquet\")\n",
    "\n",
    "        # Extract basic statistics\n",
    "        total_events = len(prepared_data)\n",
    "        unique_items = prepared_data.select(\"aid\").n_unique()\n",
    "        unique_sessions = prepared_data.select(\"session\").n_unique()\n",
    "\n",
    "        supporting_data[\"data_statistics\"] = {\n",
    "            \"total_events\": total_events,\n",
    "            \"unique_items\": unique_items,\n",
    "            \"unique_sessions\": unique_sessions,\n",
    "            \"data_loaded\": True\n",
    "        }\n",
    "\n",
    "        log(f\"       Training data: {total_events:,} events, {unique_items:,} items, {unique_sessions:,} sessions\")\n",
    "\n",
    "        # Keep a sample for validation\n",
    "        supporting_data[\"validation_sample\"] = prepared_data.sample(min(10000, len(prepared_data)), seed=42)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"       Failed to load training data: {e}\")\n",
    "        supporting_data[\"data_statistics\"] = {\"data_loaded\": False, \"error\": str(e)}\n",
    "        supporting_data[\"validation_sample\"] = None\n",
    "\n",
    "    try:\n",
    "        # Load matrix statistics if available\n",
    "        log(\"   Loading matrix statistics...\")\n",
    "\n",
    "        # Click matrix statistics\n",
    "        try:\n",
    "            with open(f\"{config.OUTPUT_PATH}/click_matrix_statistics.json\", \"r\") as f:\n",
    "                supporting_data[\"click_matrix_stats\"] = json.load(f)\n",
    "            log(f\"       Click matrix statistics loaded\")\n",
    "        except:\n",
    "            supporting_data[\"click_matrix_stats\"] = {}\n",
    "\n",
    "        # Buy matrices statistics\n",
    "        try:\n",
    "            with open(f\"{config.OUTPUT_PATH}/buy_matrices_statistics.json\", \"r\") as f:\n",
    "                supporting_data[\"buy_matrices_stats\"] = json.load(f)\n",
    "            log(f\"       Buy matrices statistics loaded\")\n",
    "        except:\n",
    "            supporting_data[\"buy_matrices_stats\"] = {}\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"       Matrix statistics not fully available: {e}\")\n",
    "\n",
    "    log(\"   Supporting data loading completed\")\n",
    "    return supporting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51_SwHvTLdcV"
   },
   "source": [
    "## MATRIX CONSOLIDATION AND STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VDVNJgjtLfct"
   },
   "outputs": [],
   "source": [
    "class MatrixConsolidator:\n",
    "    \"\"\"\n",
    "    Consolidates and standardizes co-visitation matrices from different generation processes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.matrix_types = [\"click_to_click\", \"click_to_buy\", \"buy_to_buy\"]\n",
    "        self.consolidation_stats = {}\n",
    "\n",
    "        log(\"Initializing matrix consolidator...\")\n",
    "        log(f\"   Matrix types to consolidate: {', '.join(self.matrix_types)}\")\n",
    "        log(f\"   Max candidates per item: {config.MAX_CANDIDATES_PER_ITEM}\")\n",
    "\n",
    "    def standardize_matrix_format(self, matrix: Dict, matrix_type: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Standardize matrix format and ensure consistent structure\n",
    "\n",
    "        Args:\n",
    "            matrix: Raw matrix from generation process\n",
    "            matrix_type: Type of matrix (click_to_click, click_to_buy, buy_to_buy)\n",
    "\n",
    "        Returns:\n",
    "            dict: Standardized matrix\n",
    "        \"\"\"\n",
    "        log(f\"   Standardizing {matrix_type} matrix format...\")\n",
    "\n",
    "        if not matrix:\n",
    "            log(f\"      Warning: {matrix_type} matrix is empty\")\n",
    "            return {}\n",
    "\n",
    "        standardized_matrix = {}\n",
    "        original_source_items = len(matrix)\n",
    "        original_total_pairs = sum(len(candidates) for candidates in matrix.values())\n",
    "\n",
    "        removed_items = 0\n",
    "        removed_pairs = 0\n",
    "\n",
    "        for source_item, candidates in matrix.items():\n",
    "            if not candidates:\n",
    "                removed_items += 1\n",
    "                continue\n",
    "\n",
    "            # Ensure candidates are in the correct format (aid, score)\n",
    "            standardized_candidates = []\n",
    "\n",
    "            for candidate in candidates:\n",
    "                if isinstance(candidate, (list, tuple)) and len(candidate) == 2:\n",
    "                    aid, score = candidate\n",
    "                    # Ensure numeric types\n",
    "                    try:\n",
    "                        aid = int(aid)\n",
    "                        score = float(score)\n",
    "                        standardized_candidates.append((aid, score))\n",
    "                    except (ValueError, TypeError):\n",
    "                        removed_pairs += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    removed_pairs += 1\n",
    "                    continue\n",
    "\n",
    "            # Sort by score and limit candidates\n",
    "            if standardized_candidates:\n",
    "                standardized_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                limited_candidates = standardized_candidates[:config.MAX_CANDIDATES_PER_ITEM]\n",
    "\n",
    "                removed_pairs += len(standardized_candidates) - len(limited_candidates)\n",
    "\n",
    "                if len(limited_candidates) >= config.MIN_CANDIDATES_PER_MATRIX:\n",
    "                    standardized_matrix[int(source_item)] = limited_candidates\n",
    "                else:\n",
    "                    removed_items += 1\n",
    "            else:\n",
    "                removed_items += 1\n",
    "\n",
    "        final_source_items = len(standardized_matrix)\n",
    "        final_total_pairs = sum(len(candidates) for candidates in standardized_matrix.values())\n",
    "\n",
    "        standardization_stats = {\n",
    "            \"original_source_items\": original_source_items,\n",
    "            \"final_source_items\": final_source_items,\n",
    "            \"original_total_pairs\": original_total_pairs,\n",
    "            \"final_total_pairs\": final_total_pairs,\n",
    "            \"removed_items\": removed_items,\n",
    "            \"removed_pairs\": removed_pairs,\n",
    "            \"standardization_successful\": True\n",
    "        }\n",
    "\n",
    "        self.consolidation_stats[matrix_type] = standardization_stats\n",
    "\n",
    "        log(f\"       {matrix_type} standardization completed:\")\n",
    "        log(f\"         Source items: {original_source_items:,} -> {final_source_items:,} ({removed_items:,} removed)\")\n",
    "        log(f\"         Total pairs: {original_total_pairs:,} -> {final_total_pairs:,} ({removed_pairs:,} removed)\")\n",
    "\n",
    "        return standardized_matrix\n",
    "\n",
    "    def consolidate_matrices(self, click_to_click: Dict, click_to_buy: Dict, buy_to_buy: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Consolidate all matrices into a unified structure\n",
    "\n",
    "        Args:\n",
    "            click_to_click: Click-to-click matrix\n",
    "            click_to_buy: Click-to-buy matrix\n",
    "            buy_to_buy: Buy-to-buy matrix\n",
    "\n",
    "        Returns:\n",
    "            dict: Consolidated matrices structure\n",
    "        \"\"\"\n",
    "        log(\"Consolidating co-visitation matrices...\")\n",
    "\n",
    "        # Standardize each matrix\n",
    "        standardized_matrices = {}\n",
    "\n",
    "        standardized_matrices[\"click_to_click\"] = self.standardize_matrix_format(click_to_click, \"click_to_click\")\n",
    "        standardized_matrices[\"click_to_buy\"] = self.standardize_matrix_format(click_to_buy, \"click_to_buy\")\n",
    "        standardized_matrices[\"buy_to_buy\"] = self.standardize_matrix_format(buy_to_buy, \"buy_to_buy\")\n",
    "\n",
    "        # Create consolidated structure\n",
    "        consolidated_matrices = {\n",
    "            \"matrices\": standardized_matrices,\n",
    "            \"metadata\": {\n",
    "                \"consolidation_timestamp\": datetime.now().isoformat(),\n",
    "                \"matrix_types\": self.matrix_types,\n",
    "                \"max_candidates_per_item\": config.MAX_CANDIDATES_PER_ITEM,\n",
    "                \"min_candidates_per_matrix\": config.MIN_CANDIDATES_PER_MATRIX,\n",
    "                \"consolidation_stats\": self.consolidation_stats\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Summary statistics\n",
    "        total_source_items = sum(len(matrix) for matrix in standardized_matrices.values())\n",
    "        total_pairs = sum(sum(len(candidates) for candidates in matrix.values())\n",
    "                         for matrix in standardized_matrices.values())\n",
    "\n",
    "        consolidated_matrices[\"summary\"] = {\n",
    "            \"total_source_items\": total_source_items,\n",
    "            \"total_pairs\": total_pairs,\n",
    "            \"matrices_with_data\": sum(1 for matrix in standardized_matrices.values() if matrix),\n",
    "            \"consolidation_successful\": True\n",
    "        }\n",
    "\n",
    "        log(f\"   Consolidation summary:\")\n",
    "        log(f\"      Total source items across all matrices: {total_source_items:,}\")\n",
    "        log(f\"      Total pairs across all matrices: {total_pairs:,}\")\n",
    "        log(f\"      Matrices with data: {consolidated_matrices['summary']['matrices_with_data']}/3\")\n",
    "\n",
    "        return consolidated_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O4lW2yuLi7k"
   },
   "source": [
    "## MATRIX VALIDATION AND QUALITY ASSESSMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p6GoRb5cLlgR"
   },
   "outputs": [],
   "source": [
    "class MatrixValidator:\n",
    "    \"\"\"\n",
    "    Performs comprehensive validation and quality assessment of consolidated matrices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, supporting_data: Dict):\n",
    "        self.supporting_data = supporting_data\n",
    "        self.validation_results = {}\n",
    "\n",
    "        log(\"Initializing matrix validator...\")\n",
    "\n",
    "    def validate_matrix_coverage(self, consolidated_matrices: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze coverage of items across different matrix types\n",
    "\n",
    "        Args:\n",
    "            consolidated_matrices: Consolidated matrices structure\n",
    "\n",
    "        Returns:\n",
    "            dict: Coverage analysis results\n",
    "        \"\"\"\n",
    "        log(\"Analyzing matrix coverage...\")\n",
    "\n",
    "        matrices = consolidated_matrices[\"matrices\"]\n",
    "\n",
    "        # Get all source items from each matrix\n",
    "        all_source_items = {}\n",
    "        for matrix_type, matrix in matrices.items():\n",
    "            all_source_items[matrix_type] = set(matrix.keys()) if matrix else set()\n",
    "\n",
    "        # Get all target items (candidates)\n",
    "        all_target_items = {}\n",
    "        for matrix_type, matrix in matrices.items():\n",
    "            targets = set()\n",
    "            if matrix:\n",
    "                for candidates in matrix.values():\n",
    "                    for aid, score in candidates:\n",
    "                        targets.add(aid)\n",
    "            all_target_items[matrix_type] = targets\n",
    "\n",
    "        # Calculate coverage statistics\n",
    "        all_items_union = set()\n",
    "        for items in all_source_items.values():\n",
    "            all_items_union.update(items)\n",
    "        for items in all_target_items.values():\n",
    "            all_items_union.update(items)\n",
    "\n",
    "        total_items_in_matrices = len(all_items_union)\n",
    "\n",
    "        # Compare with dataset statistics if available\n",
    "        dataset_items = self.supporting_data.get(\"data_statistics\", {}).get(\"unique_items\", 0)\n",
    "        coverage_percentage = (total_items_in_matrices / dataset_items * 100) if dataset_items > 0 else 0\n",
    "\n",
    "        # Matrix overlap analysis\n",
    "        matrix_overlaps = {}\n",
    "        matrix_types = list(all_source_items.keys())\n",
    "\n",
    "        for i, type1 in enumerate(matrix_types):\n",
    "            for j, type2 in enumerate(matrix_types):\n",
    "                if i < j:\n",
    "                    overlap_key = f\"{type1}_vs_{type2}\"\n",
    "                    source_overlap = len(all_source_items[type1].intersection(all_source_items[type2]))\n",
    "                    target_overlap = len(all_target_items[type1].intersection(all_target_items[type2]))\n",
    "\n",
    "                    matrix_overlaps[overlap_key] = {\n",
    "                        \"source_item_overlap\": source_overlap,\n",
    "                        \"target_item_overlap\": target_overlap\n",
    "                    }\n",
    "\n",
    "        coverage_analysis = {\n",
    "            \"total_items_in_matrices\": total_items_in_matrices,\n",
    "            \"dataset_total_items\": dataset_items,\n",
    "            \"coverage_percentage\": coverage_percentage,\n",
    "            \"source_item_counts\": {matrix_type: len(items) for matrix_type, items in all_source_items.items()},\n",
    "            \"target_item_counts\": {matrix_type: len(items) for matrix_type, items in all_target_items.items()},\n",
    "            \"matrix_overlaps\": matrix_overlaps\n",
    "        }\n",
    "\n",
    "        log(f\"   Coverage analysis results:\")\n",
    "        log(f\"      Total items covered: {total_items_in_matrices:,}\")\n",
    "        log(f\"      Dataset coverage: {coverage_percentage:.1f}% ({total_items_in_matrices:,}/{dataset_items:,})\")\n",
    "\n",
    "        for matrix_type, count in coverage_analysis[\"source_item_counts\"].items():\n",
    "            log(f\"      {matrix_type} source items: {count:,}\")\n",
    "\n",
    "        return coverage_analysis\n",
    "\n",
    "    def validate_matrix_quality(self, consolidated_matrices: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Assess the quality of consolidated matrices\n",
    "\n",
    "        Args:\n",
    "            consolidated_matrices: Consolidated matrices structure\n",
    "\n",
    "        Returns:\n",
    "            dict: Quality assessment results\n",
    "        \"\"\"\n",
    "        log(\"Assessing matrix quality...\")\n",
    "\n",
    "        matrices = consolidated_matrices[\"matrices\"]\n",
    "        quality_metrics = {}\n",
    "\n",
    "        for matrix_type, matrix in matrices.items():\n",
    "            if not matrix:\n",
    "                quality_metrics[matrix_type] = {\n",
    "                    \"quality_score\": 0.0,\n",
    "                    \"has_data\": False,\n",
    "                    \"quality_issues\": [\"Matrix is empty\"]\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            log(f\"   Analyzing {matrix_type} matrix quality...\")\n",
    "\n",
    "            # Basic quality metrics\n",
    "            source_items = len(matrix)\n",
    "            candidate_counts = [len(candidates) for candidates in matrix.values()]\n",
    "            total_pairs = sum(candidate_counts)\n",
    "\n",
    "            # Score distribution analysis\n",
    "            all_scores = []\n",
    "            for candidates in matrix.values():\n",
    "                scores = [score for aid, score in candidates]\n",
    "                all_scores.extend(scores)\n",
    "\n",
    "            # Quality indicators\n",
    "            quality_issues = []\n",
    "            quality_score = 1.0\n",
    "\n",
    "            # Check for minimum data requirements\n",
    "            if source_items < 100:\n",
    "                quality_issues.append(f\"Low source item count: {source_items}\")\n",
    "                quality_score *= 0.7\n",
    "\n",
    "            if total_pairs < 1000:\n",
    "                quality_issues.append(f\"Low total pairs count: {total_pairs}\")\n",
    "                quality_score *= 0.8\n",
    "\n",
    "            # Check candidate distribution\n",
    "            if candidate_counts:\n",
    "                avg_candidates = np.mean(candidate_counts)\n",
    "                std_candidates = np.std(candidate_counts)\n",
    "\n",
    "                if avg_candidates < config.MIN_CANDIDATES_PER_MATRIX:\n",
    "                    quality_issues.append(f\"Low average candidates per item: {avg_candidates:.1f}\")\n",
    "                    quality_score *= 0.8\n",
    "\n",
    "                # Check for extremely uneven distribution\n",
    "                if std_candidates > avg_candidates * 2:\n",
    "                    quality_issues.append(\"Highly uneven candidate distribution\")\n",
    "                    quality_score *= 0.9\n",
    "\n",
    "            # Check score distribution\n",
    "            if all_scores:\n",
    "                score_stats = {\n",
    "                    \"min\": min(all_scores),\n",
    "                    \"max\": max(all_scores),\n",
    "                    \"mean\": np.mean(all_scores),\n",
    "                    \"std\": np.std(all_scores)\n",
    "                }\n",
    "\n",
    "                # Check for score validity\n",
    "                if score_stats[\"min\"] < 0:\n",
    "                    quality_issues.append(\"Negative scores found\")\n",
    "                    quality_score *= 0.9\n",
    "\n",
    "                if score_stats[\"std\"] == 0:\n",
    "                    quality_issues.append(\"All scores are identical\")\n",
    "                    quality_score *= 0.8\n",
    "            else:\n",
    "                score_stats = {}\n",
    "                quality_issues.append(\"No valid scores found\")\n",
    "                quality_score *= 0.5\n",
    "\n",
    "            quality_metrics[matrix_type] = {\n",
    "                \"quality_score\": quality_score,\n",
    "                \"has_data\": True,\n",
    "                \"source_items\": source_items,\n",
    "                \"total_pairs\": total_pairs,\n",
    "                \"avg_candidates_per_item\": np.mean(candidate_counts) if candidate_counts else 0,\n",
    "                \"score_statistics\": score_stats,\n",
    "                \"quality_issues\": quality_issues\n",
    "            }\n",
    "\n",
    "            log(f\"      Quality score: {quality_score:.2f}\")\n",
    "            if quality_issues:\n",
    "                log(f\"      Issues found: {', '.join(quality_issues)}\")\n",
    "            else:\n",
    "                log(f\"      No quality issues detected\")\n",
    "\n",
    "        # Overall quality assessment\n",
    "        valid_matrices = sum(1 for metrics in quality_metrics.values() if metrics[\"has_data\"])\n",
    "        avg_quality_score = np.mean([metrics[\"quality_score\"] for metrics in quality_metrics.values()])\n",
    "\n",
    "        overall_assessment = {\n",
    "            \"valid_matrices\": valid_matrices,\n",
    "            \"total_matrices\": len(matrices),\n",
    "            \"average_quality_score\": avg_quality_score,\n",
    "            \"overall_quality\": \"EXCELLENT\" if avg_quality_score >= 0.9 else\n",
    "                              \"GOOD\" if avg_quality_score >= 0.7 else\n",
    "                              \"ACCEPTABLE\" if avg_quality_score >= 0.5 else \"POOR\"\n",
    "        }\n",
    "\n",
    "        quality_assessment = {\n",
    "            \"matrix_quality_metrics\": quality_metrics,\n",
    "            \"overall_assessment\": overall_assessment,\n",
    "            \"quality_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        log(f\"   Overall quality assessment:\")\n",
    "        log(f\"      Valid matrices: {valid_matrices}/{len(matrices)}\")\n",
    "        log(f\"      Average quality score: {avg_quality_score:.2f}\")\n",
    "        log(f\"      Overall quality rating: {overall_assessment['overall_quality']}\")\n",
    "\n",
    "        return quality_assessment\n",
    "\n",
    "    def cross_validate_matrices(self, consolidated_matrices: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform cross-validation between different matrix types\n",
    "\n",
    "        Args:\n",
    "            consolidated_matrices: Consolidated matrices structure\n",
    "\n",
    "        Returns:\n",
    "            dict: Cross-validation results\n",
    "        \"\"\"\n",
    "        log(\"Performing cross-matrix validation...\")\n",
    "\n",
    "        matrices = consolidated_matrices[\"matrices\"]\n",
    "        cross_validation_results = {}\n",
    "\n",
    "        # Sample items for cross-validation\n",
    "        all_source_items = set()\n",
    "        for matrix in matrices.values():\n",
    "            if matrix:\n",
    "                all_source_items.update(matrix.keys())\n",
    "\n",
    "        if len(all_source_items) == 0:\n",
    "            log(\"   No source items available for cross-validation\")\n",
    "            return {\"error\": \"No source items available\"}\n",
    "\n",
    "        sample_items = list(all_source_items)[:config.CROSS_VALIDATION_SAMPLE]\n",
    "        log(f\"   Cross-validating {len(sample_items)} sample items...\")\n",
    "\n",
    "        # Check consistency across matrices\n",
    "        consistency_metrics = {}\n",
    "\n",
    "        for matrix_type, matrix in matrices.items():\n",
    "            if not matrix:\n",
    "                continue\n",
    "\n",
    "            item_consistency = []\n",
    "\n",
    "            for item in sample_items:\n",
    "                if item in matrix:\n",
    "                    candidates = matrix[item]\n",
    "\n",
    "                    # Check for duplicate candidates\n",
    "                    candidate_aids = [aid for aid, score in candidates]\n",
    "                    has_duplicates = len(candidate_aids) != len(set(candidate_aids))\n",
    "\n",
    "                    # Check score ordering\n",
    "                    scores = [score for aid, score in candidates]\n",
    "                    is_sorted = all(scores[i] >= scores[i+1] for i in range(len(scores)-1))\n",
    "\n",
    "                    item_consistency.append({\n",
    "                        \"item\": item,\n",
    "                        \"has_duplicates\": has_duplicates,\n",
    "                        \"is_sorted\": is_sorted,\n",
    "                        \"candidate_count\": len(candidates)\n",
    "                    })\n",
    "\n",
    "            # Calculate consistency metrics\n",
    "            if item_consistency:\n",
    "                duplicate_rate = sum(1 for item in item_consistency if item[\"has_duplicates\"]) / len(item_consistency)\n",
    "                sorting_compliance = sum(1 for item in item_consistency if item[\"is_sorted\"]) / len(item_consistency)\n",
    "                avg_candidates = np.mean([item[\"candidate_count\"] for item in item_consistency])\n",
    "\n",
    "                consistency_metrics[matrix_type] = {\n",
    "                    \"items_validated\": len(item_consistency),\n",
    "                    \"duplicate_rate\": duplicate_rate,\n",
    "                    \"sorting_compliance\": sorting_compliance,\n",
    "                    \"avg_candidates\": avg_candidates\n",
    "                }\n",
    "\n",
    "                log(f\"      {matrix_type}:\")\n",
    "                log(f\"         Items validated: {len(item_consistency)}\")\n",
    "                log(f\"         Duplicate rate: {duplicate_rate:.1%}\")\n",
    "                log(f\"         Sorting compliance: {sorting_compliance:.1%}\")\n",
    "\n",
    "        cross_validation_results = {\n",
    "            \"sample_size\": len(sample_items),\n",
    "            \"consistency_metrics\": consistency_metrics,\n",
    "            \"validation_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        return cross_validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpFTlJOdLvNK"
   },
   "source": [
    "## SAMPLE GENERATION FOR MANUAL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DiHpFovLLxE5"
   },
   "outputs": [],
   "source": [
    "def generate_validation_samples(consolidated_matrices: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate sample relationships for manual validation and inspection\n",
    "\n",
    "    Args:\n",
    "        consolidated_matrices: Consolidated matrices structure\n",
    "\n",
    "    Returns:\n",
    "        dict: Sample relationships for validation\n",
    "    \"\"\"\n",
    "    log(\"Generating validation samples...\")\n",
    "\n",
    "    matrices = consolidated_matrices[\"matrices\"]\n",
    "    validation_samples = {}\n",
    "\n",
    "    for matrix_type, matrix in matrices.items():\n",
    "        if not matrix:\n",
    "            validation_samples[matrix_type] = {\"error\": \"Matrix is empty\"}\n",
    "            continue\n",
    "\n",
    "        log(f\"   Generating samples for {matrix_type}...\")\n",
    "\n",
    "        # Get sample of source items\n",
    "        source_items = list(matrix.keys())\n",
    "        sample_size = min(config.SAMPLE_SIZE_FOR_VALIDATION, len(source_items))\n",
    "        sample_items = np.random.choice(source_items, sample_size, replace=False)\n",
    "\n",
    "        matrix_samples = {}\n",
    "\n",
    "        for item in sample_items:\n",
    "            candidates = matrix[item]\n",
    "\n",
    "            # Include top candidates with their scores\n",
    "            top_candidates = candidates[:10]  # Top 10 for manual inspection\n",
    "\n",
    "            matrix_samples[str(item)] = {\n",
    "                \"total_candidates\": len(candidates),\n",
    "                \"top_candidates\": [{\"aid\": aid, \"score\": score} for aid, score in top_candidates],\n",
    "                \"score_range\": {\n",
    "                    \"max\": max(score for aid, score in candidates),\n",
    "                    \"min\": min(score for aid, score in candidates)\n",
    "                }\n",
    "            }\n",
    "\n",
    "        validation_samples[matrix_type] = {\n",
    "            \"sample_count\": len(matrix_samples),\n",
    "            \"samples\": matrix_samples\n",
    "        }\n",
    "\n",
    "        log(f\"      Generated {len(matrix_samples)} samples for {matrix_type}\")\n",
    "\n",
    "    validation_samples[\"generation_timestamp\"] = datetime.now().isoformat()\n",
    "    validation_samples[\"sample_purpose\"] = \"Manual validation and quality inspection\"\n",
    "\n",
    "    return validation_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53pxnaiwL2BG"
   },
   "source": [
    "## CONFIGURATION GENERATION FOR PART 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RMM2zvDgL9Po"
   },
   "outputs": [],
   "source": [
    "def generate_candidate_generation_config(consolidated_matrices: Dict,\n",
    "                                        quality_assessment: Dict,\n",
    "                                        coverage_analysis: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate configuration for Part 2B candidate generation\n",
    "\n",
    "    Args:\n",
    "        consolidated_matrices: Consolidated matrices structure\n",
    "        quality_assessment: Quality assessment results\n",
    "        coverage_analysis: Coverage analysis results\n",
    "\n",
    "    Returns:\n",
    "        dict: Configuration for candidate generation in Part 2B\n",
    "    \"\"\"\n",
    "    log(\"Generating configuration for Part 2B candidate generation...\")\n",
    "\n",
    "    matrices = consolidated_matrices[\"matrices\"]\n",
    "\n",
    "    # Determine optimal candidate generation strategy based on matrix quality\n",
    "    matrix_strategies = {}\n",
    "\n",
    "    for matrix_type, matrix in matrices.items():\n",
    "        if not matrix:\n",
    "            matrix_strategies[matrix_type] = {\n",
    "                \"enabled\": False,\n",
    "                \"reason\": \"Matrix is empty or unavailable\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        quality_metrics = quality_assessment[\"matrix_quality_metrics\"].get(matrix_type, {})\n",
    "        quality_score = quality_metrics.get(\"quality_score\", 0)\n",
    "\n",
    "        if quality_score >= 0.7:\n",
    "            strategy = {\n",
    "                \"enabled\": True,\n",
    "                \"priority\": \"HIGH\" if quality_score >= 0.9 else \"MEDIUM\",\n",
    "                \"max_candidates\": config.MAX_CANDIDATES_PER_ITEM,\n",
    "                \"min_score_threshold\": None,  # Use all candidates\n",
    "                \"sampling_method\": \"top_k\"\n",
    "            }\n",
    "        elif quality_score >= 0.5:\n",
    "            strategy = {\n",
    "                \"enabled\": True,\n",
    "                \"priority\": \"LOW\",\n",
    "                \"max_candidates\": min(20, config.MAX_CANDIDATES_PER_ITEM),\n",
    "                \"min_score_threshold\": 1.0,  # Filter low-quality candidates\n",
    "                \"sampling_method\": \"filtered_top_k\"\n",
    "            }\n",
    "        else:\n",
    "            strategy = {\n",
    "                \"enabled\": False,\n",
    "                \"reason\": f\"Quality score too low: {quality_score:.2f}\"\n",
    "            }\n",
    "\n",
    "        matrix_strategies[matrix_type] = strategy\n",
    "\n",
    "    # Overall candidate generation parameters\n",
    "    enabled_matrices = [name for name, strategy in matrix_strategies.items() if strategy.get(\"enabled\", False)]\n",
    "\n",
    "    candidate_config = {\n",
    "        \"generation_timestamp\": datetime.now().isoformat(),\n",
    "        \"source_notebook\": \"Part 2A4: Matrix Consolidation & Validation\",\n",
    "        \"target_notebook\": \"Part 2B: Model Training\",\n",
    "\n",
    "        \"matrix_strategies\": matrix_strategies,\n",
    "        \"enabled_matrices\": enabled_matrices,\n",
    "        \"total_enabled_matrices\": len(enabled_matrices),\n",
    "\n",
    "        \"global_parameters\": {\n",
    "            \"max_candidates_per_item\": config.MAX_CANDIDATES_PER_ITEM,\n",
    "            \"candidate_combination_method\": \"weighted_union\",  # How to combine candidates from different matrices\n",
    "            \"default_weights\": {\n",
    "                \"click_to_click\": 0.4,\n",
    "                \"click_to_buy\": 0.35,\n",
    "                \"buy_to_buy\": 0.25\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"quality_based_adjustments\": {\n",
    "            \"use_quality_weighting\": True,\n",
    "            \"quality_scores\": {matrix_type: quality_assessment[\"matrix_quality_metrics\"].get(matrix_type, {}).get(\"quality_score\", 0)\n",
    "                             for matrix_type in matrices.keys()},\n",
    "            \"coverage_percentage\": coverage_analysis.get(\"coverage_percentage\", 0)\n",
    "        },\n",
    "\n",
    "        \"fallback_strategy\": {\n",
    "            \"min_matrices_required\": 1,\n",
    "            \"use_popularity_fallback\": True,\n",
    "            \"popularity_candidates\": 20\n",
    "        }\n",
    "    }\n",
    "\n",
    "    log(f\"   Configuration generated:\")\n",
    "    log(f\"      Enabled matrices: {len(enabled_matrices)}/3\")\n",
    "    log(f\"      Primary matrices: {[name for name, strategy in matrix_strategies.items() if strategy.get('priority') == 'HIGH']}\")\n",
    "    log(f\"      Coverage percentage: {coverage_analysis.get('coverage_percentage', 0):.1f}%\")\n",
    "\n",
    "    return candidate_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn0VpL6cMDgu"
   },
   "source": [
    "## MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4v4vlIm1MGUh",
    "outputId": "7f3a5516-cbed-4c72-d37f-d2bd2d9fc71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:53:48] Validating input files from previous notebooks...\n",
      "[2025-08-07 18:53:48]    Checking required files:\n",
      "[2025-08-07 18:53:48]        click_to_click_matrix.pkl - 556.6 MB\n",
      "[2025-08-07 18:53:48]        click_to_buy_matrix.pkl - 65.3 MB\n",
      "[2025-08-07 18:53:48]        buy_to_buy_matrix.pkl - 32.3 MB\n",
      "[2025-08-07 18:53:48]        covisit_data_prepared.parquet - 1605.4 MB\n",
      "[2025-08-07 18:53:48]        session_analysis.json - 0.0 MB\n",
      "[2025-08-07 18:53:48]    Checking optional files:\n",
      "[2025-08-07 18:53:48]        click_matrix_statistics.json - 0.0 MB\n",
      "[2025-08-07 18:53:48]        buy_matrices_statistics.json - 0.0 MB\n",
      "[2025-08-07 18:53:48]        item_stats.parquet - 0.0 MB\n",
      "[2025-08-07 18:53:48]    SUCCESS: All required files found (Total size: 2259.6 MB)\n",
      "[2025-08-07 18:53:48] Loading co-visitation matrices...\n",
      "[2025-08-07 18:53:48]    Loading click-to-click matrix...\n",
      "[2025-08-07 18:54:16]        Click-to-click: 1,839,483 source items, 63,503,324 pairs\n",
      "[2025-08-07 18:54:16]    Loading click-to-buy matrix...\n",
      "[2025-08-07 18:54:22]        Click-to-buy: 841,226 source items, 6,851,523 pairs\n",
      "[2025-08-07 18:54:22]    Loading buy-to-buy matrix...\n",
      "[2025-08-07 18:54:26]        Buy-to-buy: 311,156 source items, 3,483,335 pairs\n",
      "[2025-08-07 18:54:26]    Matrix loading summary:\n",
      "[2025-08-07 18:54:26]       Successfully loaded: 3/3 matrices\n",
      "[2025-08-07 18:54:26]       Total source items: 2,991,865\n",
      "[2025-08-07 18:54:26]       Total pairs: 73,838,182\n",
      "[2025-08-07 18:54:26] Loading supporting data and statistics...\n",
      "[2025-08-07 18:54:26]    Loading session analysis...\n",
      "[2025-08-07 18:54:26]        Session analysis loaded\n",
      "[2025-08-07 18:54:26]    Loading prepared training data...\n",
      "[2025-08-07 18:54:47]        Training data: 216,384,937 events, 1,855,603 items, 12,899,779 sessions\n",
      "[2025-08-07 18:54:49]    Loading matrix statistics...\n",
      "[2025-08-07 18:54:49]        Click matrix statistics loaded\n",
      "[2025-08-07 18:54:50]        Buy matrices statistics loaded\n",
      "[2025-08-07 18:54:50]    Supporting data loading completed\n",
      "[2025-08-07 18:54:50] \n",
      "============================================================\n",
      "[2025-08-07 18:54:50] MATRIX CONSOLIDATION\n",
      "[2025-08-07 18:54:50] ============================================================\n",
      "[2025-08-07 18:54:50] Initializing matrix consolidator...\n",
      "[2025-08-07 18:54:50]    Matrix types to consolidate: click_to_click, click_to_buy, buy_to_buy\n",
      "[2025-08-07 18:54:50]    Max candidates per item: 40\n",
      "[2025-08-07 18:54:50] Consolidating co-visitation matrices...\n",
      "[2025-08-07 18:54:50]    Standardizing click_to_click matrix format...\n",
      "[2025-08-07 18:55:29]        click_to_click standardization completed:\n",
      "[2025-08-07 18:55:29]          Source items: 1,839,483 -> 1,805,562 (33,921 removed)\n",
      "[2025-08-07 18:55:29]          Total pairs: 63,503,324 -> 63,415,553 (0 removed)\n",
      "[2025-08-07 18:55:29]    Standardizing click_to_buy matrix format...\n",
      "[2025-08-07 18:55:33]        click_to_buy standardization completed:\n",
      "[2025-08-07 18:55:33]          Source items: 841,226 -> 379,726 (461,500 removed)\n",
      "[2025-08-07 18:55:33]          Total pairs: 6,851,523 -> 5,884,056 (0 removed)\n",
      "[2025-08-07 18:55:33]    Standardizing buy_to_buy matrix format...\n",
      "[2025-08-07 18:55:35]        buy_to_buy standardization completed:\n",
      "[2025-08-07 18:55:35]          Source items: 311,156 -> 204,530 (106,626 removed)\n",
      "[2025-08-07 18:55:35]          Total pairs: 3,483,335 -> 3,233,811 (0 removed)\n",
      "[2025-08-07 18:55:35]    Consolidation summary:\n",
      "[2025-08-07 18:55:35]       Total source items across all matrices: 2,389,818\n",
      "[2025-08-07 18:55:35]       Total pairs across all matrices: 72,533,420\n",
      "[2025-08-07 18:55:35]       Matrices with data: 3/3\n",
      "[2025-08-07 18:55:41] \n",
      "============================================================\n",
      "[2025-08-07 18:55:41] MATRIX VALIDATION AND QUALITY ASSESSMENT\n",
      "[2025-08-07 18:55:41] ============================================================\n",
      "[2025-08-07 18:55:41] Initializing matrix validator...\n",
      "[2025-08-07 18:55:41] Analyzing matrix coverage...\n",
      "[2025-08-07 18:56:03]    Coverage analysis results:\n",
      "[2025-08-07 18:56:03]       Total items covered: 1,827,570\n",
      "[2025-08-07 18:56:03]       Dataset coverage: 98.5% (1,827,570/1,855,603)\n",
      "[2025-08-07 18:56:03]       click_to_click source items: 1,805,562\n",
      "[2025-08-07 18:56:03]       click_to_buy source items: 379,726\n",
      "[2025-08-07 18:56:03]       buy_to_buy source items: 204,530\n",
      "[2025-08-07 18:56:03] Assessing matrix quality...\n",
      "[2025-08-07 18:56:03]    Analyzing click_to_click matrix quality...\n",
      "[2025-08-07 18:56:15]       Quality score: 1.00\n",
      "[2025-08-07 18:56:15]       No quality issues detected\n",
      "[2025-08-07 18:56:15]    Analyzing click_to_buy matrix quality...\n",
      "[2025-08-07 18:56:16]       Quality score: 1.00\n",
      "[2025-08-07 18:56:16]       No quality issues detected\n",
      "[2025-08-07 18:56:16]    Analyzing buy_to_buy matrix quality...\n",
      "[2025-08-07 18:56:17]       Quality score: 1.00\n",
      "[2025-08-07 18:56:17]       No quality issues detected\n",
      "[2025-08-07 18:56:17]    Overall quality assessment:\n",
      "[2025-08-07 18:56:17]       Valid matrices: 3/3\n",
      "[2025-08-07 18:56:17]       Average quality score: 1.00\n",
      "[2025-08-07 18:56:17]       Overall quality rating: EXCELLENT\n",
      "[2025-08-07 18:56:17] Performing cross-matrix validation...\n",
      "[2025-08-07 18:56:18]    Cross-validating 500 sample items...\n",
      "[2025-08-07 18:56:18]       click_to_click:\n",
      "[2025-08-07 18:56:18]          Items validated: 499\n",
      "[2025-08-07 18:56:18]          Duplicate rate: 0.0%\n",
      "[2025-08-07 18:56:18]          Sorting compliance: 100.0%\n",
      "[2025-08-07 18:56:18]       click_to_buy:\n",
      "[2025-08-07 18:56:18]          Items validated: 114\n",
      "[2025-08-07 18:56:18]          Duplicate rate: 0.0%\n",
      "[2025-08-07 18:56:18]          Sorting compliance: 100.0%\n",
      "[2025-08-07 18:56:18]       buy_to_buy:\n",
      "[2025-08-07 18:56:18]          Items validated: 62\n",
      "[2025-08-07 18:56:18]          Duplicate rate: 0.0%\n",
      "[2025-08-07 18:56:18]          Sorting compliance: 100.0%\n",
      "[2025-08-07 18:56:18] \n",
      "============================================================\n",
      "[2025-08-07 18:56:18] VALIDATION SAMPLE GENERATION\n",
      "[2025-08-07 18:56:18] ============================================================\n",
      "[2025-08-07 18:56:18] Generating validation samples...\n",
      "[2025-08-07 18:56:18]    Generating samples for click_to_click...\n",
      "[2025-08-07 18:56:18]       Generated 1000 samples for click_to_click\n",
      "[2025-08-07 18:56:18]    Generating samples for click_to_buy...\n",
      "[2025-08-07 18:56:19]       Generated 1000 samples for click_to_buy\n",
      "[2025-08-07 18:56:19]    Generating samples for buy_to_buy...\n",
      "[2025-08-07 18:56:19]       Generated 1000 samples for buy_to_buy\n",
      "[2025-08-07 18:56:19] \n",
      "============================================================\n",
      "[2025-08-07 18:56:19] CANDIDATE GENERATION CONFIGURATION\n",
      "[2025-08-07 18:56:19] ============================================================\n",
      "[2025-08-07 18:56:19] Generating configuration for Part 2B candidate generation...\n",
      "[2025-08-07 18:56:19]    Configuration generated:\n",
      "[2025-08-07 18:56:19]       Enabled matrices: 3/3\n",
      "[2025-08-07 18:56:19]       Primary matrices: ['click_to_click', 'click_to_buy', 'buy_to_buy']\n",
      "[2025-08-07 18:56:19]       Coverage percentage: 98.5%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Validate input files\n",
    "file_validation = validate_input_files()\n",
    "\n",
    "# Step 2: Load co-visitation matrices\n",
    "click_to_click_matrix, click_to_buy_matrix, buy_to_buy_matrix, load_stats = load_co_visitation_matrices()\n",
    "\n",
    "# Step 3: Load supporting data\n",
    "supporting_data = load_supporting_data()\n",
    "\n",
    "# Step 4: Consolidate matrices\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"MATRIX CONSOLIDATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "consolidator = MatrixConsolidator()\n",
    "consolidated_matrices = consolidator.consolidate_matrices(\n",
    "    click_to_click_matrix,\n",
    "    click_to_buy_matrix,\n",
    "    buy_to_buy_matrix\n",
    ")\n",
    "\n",
    "# Clean up individual matrices to free memory\n",
    "del click_to_click_matrix, click_to_buy_matrix, buy_to_buy_matrix\n",
    "gc.collect()\n",
    "\n",
    "# Step 5: Validate and assess quality\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"MATRIX VALIDATION AND QUALITY ASSESSMENT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "validator = MatrixValidator(supporting_data)\n",
    "\n",
    "# Coverage analysis\n",
    "coverage_analysis = validator.validate_matrix_coverage(consolidated_matrices)\n",
    "\n",
    "# Quality assessment\n",
    "quality_assessment = validator.validate_matrix_quality(consolidated_matrices)\n",
    "\n",
    "# Cross-validation\n",
    "cross_validation_results = validator.cross_validate_matrices(consolidated_matrices)\n",
    "\n",
    "# Step 6: Generate validation samples\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"VALIDATION SAMPLE GENERATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "validation_samples = generate_validation_samples(consolidated_matrices)\n",
    "\n",
    "# Step 7: Generate configuration for Part 2B\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CANDIDATE GENERATION CONFIGURATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "candidate_config = generate_candidate_generation_config(\n",
    "    consolidated_matrices,\n",
    "    quality_assessment,\n",
    "    coverage_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU5PjAk4MKF3"
   },
   "source": [
    "## COMPREHENSIVE REPORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j6d7um0MMol",
    "outputId": "d7206097-ca0b-4c88-8eb4-9460ae48dfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:56:19] Generating comprehensive consolidation report...\n",
      "[2025-08-07 18:56:19]    Report generation completed\n",
      "[2025-08-07 18:56:19]       Overall success: True\n",
      "[2025-08-07 18:56:19]       Matrices: 3/3\n",
      "[2025-08-07 18:56:19]       Quality: EXCELLENT\n",
      "[2025-08-07 18:56:19]       Coverage: 98.5%\n"
     ]
    }
   ],
   "source": [
    "def generate_comprehensive_report(consolidated_matrices: Dict,\n",
    "                                coverage_analysis: Dict,\n",
    "                                quality_assessment: Dict,\n",
    "                                cross_validation_results: Dict,\n",
    "                                load_stats: Dict,\n",
    "                                file_validation: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate comprehensive consolidation report\n",
    "\n",
    "    Returns:\n",
    "        dict: Comprehensive report of consolidation process\n",
    "    \"\"\"\n",
    "    log(\"Generating comprehensive consolidation report...\")\n",
    "\n",
    "    # Executive summary\n",
    "    successful_matrices = sum(1 for matrix in consolidated_matrices[\"matrices\"].values() if matrix)\n",
    "    total_source_items = consolidated_matrices[\"summary\"][\"total_source_items\"]\n",
    "    total_pairs = consolidated_matrices[\"summary\"][\"total_pairs\"]\n",
    "    overall_quality = quality_assessment[\"overall_assessment\"][\"overall_quality\"]\n",
    "    coverage_percentage = coverage_analysis.get(\"coverage_percentage\", 0)\n",
    "\n",
    "    executive_summary = {\n",
    "        \"consolidation_successful\": successful_matrices > 0,\n",
    "        \"matrices_consolidated\": f\"{successful_matrices}/3\",\n",
    "        \"total_source_items\": total_source_items,\n",
    "        \"total_pairs\": total_pairs,\n",
    "        \"overall_quality_rating\": overall_quality,\n",
    "        \"dataset_coverage_percentage\": coverage_percentage,\n",
    "        \"ready_for_part_2b\": successful_matrices >= 1 and coverage_percentage > config.QUALITY_THRESHOLD_COVERAGE * 100\n",
    "    }\n",
    "\n",
    "    # Detailed consolidation metrics\n",
    "    consolidation_metrics = {\n",
    "        \"input_validation\": file_validation,\n",
    "        \"matrix_loading\": load_stats,\n",
    "        \"consolidation_stats\": consolidated_matrices[\"metadata\"][\"consolidation_stats\"],\n",
    "        \"matrix_summary\": consolidated_matrices[\"summary\"]\n",
    "    }\n",
    "\n",
    "    # Quality and validation metrics\n",
    "    validation_metrics = {\n",
    "        \"coverage_analysis\": coverage_analysis,\n",
    "        \"quality_assessment\": quality_assessment,\n",
    "        \"cross_validation\": cross_validation_results\n",
    "    }\n",
    "\n",
    "    # Recommendations\n",
    "    recommendations = []\n",
    "\n",
    "    if successful_matrices == 3:\n",
    "        recommendations.append(\"All matrices successfully consolidated - proceed with confidence to Part 2B\")\n",
    "    elif successful_matrices >= 2:\n",
    "        recommendations.append(\"Most matrices consolidated successfully - good for Part 2B with minor limitations\")\n",
    "    elif successful_matrices == 1:\n",
    "        recommendations.append(\"Limited matrix availability - Part 2B possible but with reduced performance\")\n",
    "    else:\n",
    "        recommendations.append(\"No matrices available - re-run previous notebooks to generate matrices\")\n",
    "\n",
    "    if coverage_percentage < 50:\n",
    "        recommendations.append(\"Low dataset coverage - consider reviewing matrix generation parameters\")\n",
    "\n",
    "    if overall_quality in [\"POOR\", \"ACCEPTABLE\"]:\n",
    "        recommendations.append(\"Quality concerns detected - review quality assessment details\")\n",
    "\n",
    "    comprehensive_report = {\n",
    "        \"report_timestamp\": datetime.now().isoformat(),\n",
    "        \"consolidation_version\": \"Part 2A4 v1.0\",\n",
    "        \"executive_summary\": executive_summary,\n",
    "        \"consolidation_metrics\": consolidation_metrics,\n",
    "        \"validation_metrics\": validation_metrics,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"next_steps\": [\n",
    "            \"Review quality assessment and recommendations\",\n",
    "            \"Proceed to Part 2B: Model Training\",\n",
    "            \"Use candidate_generation_config.json for optimal candidate generation\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    log(f\"   Report generation completed\")\n",
    "    log(f\"      Overall success: {executive_summary['consolidation_successful']}\")\n",
    "    log(f\"      Matrices: {executive_summary['matrices_consolidated']}\")\n",
    "    log(f\"      Quality: {executive_summary['overall_quality_rating']}\")\n",
    "    log(f\"      Coverage: {executive_summary['dataset_coverage_percentage']:.1f}%\")\n",
    "\n",
    "    return comprehensive_report\n",
    "\n",
    "# Generate comprehensive report\n",
    "comprehensive_report = generate_comprehensive_report(\n",
    "    consolidated_matrices,\n",
    "    coverage_analysis,\n",
    "    quality_assessment,\n",
    "    cross_validation_results,\n",
    "    load_stats,\n",
    "    file_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_qi70QKMQd6"
   },
   "source": [
    "## SAVE ALL OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpU30TKgMSnw",
    "outputId": "e63a03f3-6a8e-46f0-f9d0-7daf4fc8e828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:56:19] Saving consolidation outputs...\n",
      "[2025-08-07 18:56:55]     consolidated_covisitation_matrices.pkl saved (1122.3 MB)\n",
      "[2025-08-07 18:56:55]     matrix_consolidation_report.json saved\n",
      "[2025-08-07 18:56:55]     matrix_coverage_analysis.json saved\n",
      "[2025-08-07 18:56:55]     matrix_quality_metrics.json saved\n",
      "[2025-08-07 18:56:55]     candidate_generation_config.json saved\n",
      "[2025-08-07 18:56:56]     matrix_samples_validation.json saved\n",
      "[2025-08-07 18:56:56]     consolidation_summary.json saved\n",
      "[2025-08-07 18:56:56]  All consolidation outputs saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_consolidation_outputs(consolidated_matrices: Dict,\n",
    "                              comprehensive_report: Dict,\n",
    "                              coverage_analysis: Dict,\n",
    "                              quality_assessment: Dict,\n",
    "                              candidate_config: Dict,\n",
    "                              validation_samples: Dict):\n",
    "    \"\"\"\n",
    "    Save all consolidation outputs\n",
    "\n",
    "    Args:\n",
    "        consolidated_matrices: Final consolidated matrices\n",
    "        comprehensive_report: Comprehensive consolidation report\n",
    "        coverage_analysis: Coverage analysis results\n",
    "        quality_assessment: Quality assessment results\n",
    "        candidate_config: Configuration for Part 2B\n",
    "        validation_samples: Sample relationships for validation\n",
    "    \"\"\"\n",
    "    log(\"Saving consolidation outputs...\")\n",
    "\n",
    "    output_files = {}\n",
    "\n",
    "    try:\n",
    "        # 1. Save consolidated matrices (main output)\n",
    "        matrices_path = f\"{config.OUTPUT_PATH}/consolidated_covisitation_matrices.pkl\"\n",
    "        with open(matrices_path, \"wb\") as f:\n",
    "            pickle.dump(consolidated_matrices, f)\n",
    "\n",
    "        file_size = os.path.getsize(matrices_path) / (1024*1024)\n",
    "        output_files[\"matrices\"] = {\"path\": matrices_path, \"size_mb\": file_size}\n",
    "        log(f\"    consolidated_covisitation_matrices.pkl saved ({file_size:.1f} MB)\")\n",
    "\n",
    "        # 2. Save comprehensive report\n",
    "        report_path = f\"{config.OUTPUT_PATH}/matrix_consolidation_report.json\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            json.dump(comprehensive_report, f, indent=2)\n",
    "        output_files[\"report\"] = {\"path\": report_path}\n",
    "        log(f\"    matrix_consolidation_report.json saved\")\n",
    "\n",
    "        # 3. Save coverage analysis\n",
    "        coverage_path = f\"{config.OUTPUT_PATH}/matrix_coverage_analysis.json\"\n",
    "        with open(coverage_path, \"w\") as f:\n",
    "            json.dump(coverage_analysis, f, indent=2)\n",
    "        output_files[\"coverage\"] = {\"path\": coverage_path}\n",
    "        log(f\"    matrix_coverage_analysis.json saved\")\n",
    "\n",
    "        # 4. Save quality metrics\n",
    "        quality_path = f\"{config.OUTPUT_PATH}/matrix_quality_metrics.json\"\n",
    "        with open(quality_path, \"w\") as f:\n",
    "            json.dump(quality_assessment, f, indent=2)\n",
    "        output_files[\"quality\"] = {\"path\": quality_path}\n",
    "        log(f\"    matrix_quality_metrics.json saved\")\n",
    "\n",
    "        # 5. Save candidate generation config\n",
    "        config_path = f\"{config.OUTPUT_PATH}/candidate_generation_config.json\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(candidate_config, f, indent=2)\n",
    "        output_files[\"config\"] = {\"path\": config_path}\n",
    "        log(f\"    candidate_generation_config.json saved\")\n",
    "\n",
    "        # 6. Save validation samples\n",
    "        samples_path = f\"{config.OUTPUT_PATH}/matrix_samples_validation.json\"\n",
    "        with open(samples_path, \"w\") as f:\n",
    "            json.dump(validation_samples, f, indent=2)\n",
    "        output_files[\"samples\"] = {\"path\": samples_path}\n",
    "        log(f\"    matrix_samples_validation.json saved\")\n",
    "\n",
    "        # 7. Save executive summary\n",
    "        summary = {\n",
    "            \"notebook\": \"Part 2A4: Matrix Consolidation & Validation\",\n",
    "            \"completion_timestamp\": datetime.now().isoformat(),\n",
    "            \"consolidation_successful\": comprehensive_report[\"executive_summary\"][\"consolidation_successful\"],\n",
    "            \"matrices_processed\": {\n",
    "                \"click_to_click\": len(consolidated_matrices[\"matrices\"][\"click_to_click\"]) > 0,\n",
    "                \"click_to_buy\": len(consolidated_matrices[\"matrices\"][\"click_to_buy\"]) > 0,\n",
    "                \"buy_to_buy\": len(consolidated_matrices[\"matrices\"][\"buy_to_buy\"]) > 0\n",
    "            },\n",
    "            \"key_metrics\": {\n",
    "                \"total_source_items\": comprehensive_report[\"executive_summary\"][\"total_source_items\"],\n",
    "                \"total_pairs\": comprehensive_report[\"executive_summary\"][\"total_pairs\"],\n",
    "                \"quality_rating\": comprehensive_report[\"executive_summary\"][\"overall_quality_rating\"],\n",
    "                \"coverage_percentage\": comprehensive_report[\"executive_summary\"][\"dataset_coverage_percentage\"]\n",
    "            },\n",
    "            \"outputs_generated\": list(output_files.keys()),\n",
    "            \"ready_for_part_2b\": comprehensive_report[\"executive_summary\"][\"ready_for_part_2b\"],\n",
    "            \"next_step\": \"Part 2B: Model Training\"\n",
    "        }\n",
    "\n",
    "        summary_path = f\"{config.OUTPUT_PATH}/consolidation_summary.json\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        output_files[\"summary\"] = {\"path\": summary_path}\n",
    "        log(f\"    consolidation_summary.json saved\")\n",
    "\n",
    "        log(\" All consolidation outputs saved successfully!\")\n",
    "        return output_files\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\" Error saving consolidation outputs: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Save all outputs\n",
    "output_files = save_consolidation_outputs(\n",
    "    consolidated_matrices,\n",
    "    comprehensive_report,\n",
    "    coverage_analysis,\n",
    "    quality_assessment,\n",
    "    candidate_config,\n",
    "    validation_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psSYx5-NMa1j"
   },
   "source": [
    "## FINAL SUMMARY AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmUPTSnEMbg7",
    "outputId": "4dd2e5cf-f8fc-4be6-f278-fd4b60895c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-07 18:56:56] \n",
      "================================================================================\n",
      "[2025-08-07 18:56:56] PART 2A4 COMPLETED: MATRIX CONSOLIDATION & VALIDATION\n",
      "[2025-08-07 18:56:56] ================================================================================\n",
      "[2025-08-07 18:56:56] \n",
      "CONSOLIDATION RESULTS:\n",
      "[2025-08-07 18:56:56]  Matrices consolidated: 3/3\n",
      "[2025-08-07 18:56:56]  Total source items: 2,389,818\n",
      "[2025-08-07 18:56:56]  Total pairs: 72,533,420\n",
      "[2025-08-07 18:56:56]  Quality rating: EXCELLENT\n",
      "[2025-08-07 18:56:56]  Dataset coverage: 98.5%\n",
      "[2025-08-07 18:56:56]  Ready for Part 2B: yes\n",
      "[2025-08-07 18:56:56] \n",
      "MATRIX STATUS:\n",
      "[2025-08-07 18:56:56]  click_to_click: yes (1,805,562 source items)\n",
      "[2025-08-07 18:56:56]  click_to_buy: yes (379,726 source items)\n",
      "[2025-08-07 18:56:56]  buy_to_buy: yes (204,530 source items)\n",
      "[2025-08-07 18:56:56] \n",
      "QUALITY BREAKDOWN:\n",
      "[2025-08-07 18:56:56]  click_to_click: 1.00 (0 issues)\n",
      "[2025-08-07 18:56:56]  click_to_buy: 1.00 (0 issues)\n",
      "[2025-08-07 18:56:56]  buy_to_buy: 1.00 (0 issues)\n",
      "[2025-08-07 18:56:56] \n",
      "OUTPUT FILES GENERATED:\n",
      "[2025-08-07 18:56:56]  consolidated_covisitation_matrices.pkl (1122.3 MB)\n",
      "[2025-08-07 18:56:56]  matrix_consolidation_report.json\n",
      "[2025-08-07 18:56:56]  matrix_coverage_analysis.json\n",
      "[2025-08-07 18:56:56]  matrix_quality_metrics.json\n",
      "[2025-08-07 18:56:56]  candidate_generation_config.json\n",
      "[2025-08-07 18:56:56]  matrix_samples_validation.json\n",
      "[2025-08-07 18:56:56]  consolidation_summary.json\n",
      "[2025-08-07 18:56:56]  Total output size: 1122.3 MB\n",
      "[2025-08-07 18:56:56] \n",
      "RECOMMENDATIONS:\n",
      "[2025-08-07 18:56:56] 1. All matrices successfully consolidated - proceed with confidence to Part 2B\n",
      "[2025-08-07 18:56:56] \n",
      "FINAL STATUS:\n",
      "[2025-08-07 18:56:56]  READY FOR PART 2B - All requirements met\n",
      "[2025-08-07 18:56:56]   Quality: EXCELLENT\n",
      "[2025-08-07 18:56:56]   Coverage: 98.5%\n",
      "[2025-08-07 18:56:56]   Matrices: 3/3\n",
      "[2025-08-07 18:56:59] \n",
      "Memory cleanup completed: 36.5% usage\n",
      "[2025-08-07 18:56:59]  Part 2A4 finished successfully!\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2A4 COMPLETED: MATRIX CONSOLIDATION & VALIDATION\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Final results summary\n",
    "exec_summary = comprehensive_report[\"executive_summary\"]\n",
    "log(f\"\\nCONSOLIDATION RESULTS:\")\n",
    "log(f\" Matrices consolidated: {exec_summary['matrices_consolidated']}\")\n",
    "log(f\" Total source items: {exec_summary['total_source_items']:,}\")\n",
    "log(f\" Total pairs: {exec_summary['total_pairs']:,}\")\n",
    "log(f\" Quality rating: {exec_summary['overall_quality_rating']}\")\n",
    "log(f\" Dataset coverage: {exec_summary['dataset_coverage_percentage']:.1f}%\")\n",
    "log(f\" Ready for Part 2B: {'yes' if exec_summary['ready_for_part_2b'] else 'no'}\")\n",
    "\n",
    "# Matrix-specific status\n",
    "matrices_status = consolidated_matrices[\"matrices\"]\n",
    "log(f\"\\nMATRIX STATUS:\")\n",
    "for matrix_type, matrix in matrices_status.items():\n",
    "    status = \"yes\" if matrix else \"no\"\n",
    "    count = len(matrix) if matrix else 0\n",
    "    log(f\" {matrix_type}: {status} ({count:,} source items)\")\n",
    "\n",
    "# Quality breakdown\n",
    "quality_metrics = quality_assessment[\"matrix_quality_metrics\"]\n",
    "log(f\"\\nQUALITY BREAKDOWN:\")\n",
    "for matrix_type, metrics in quality_metrics.items():\n",
    "    if metrics.get(\"has_data\", False):\n",
    "        score = metrics[\"quality_score\"]\n",
    "        issues = len(metrics.get(\"quality_issues\", []))\n",
    "        log(f\" {matrix_type}: {score:.2f} ({issues} issues)\")\n",
    "    else:\n",
    "        log(f\" {matrix_type}: No data\")\n",
    "\n",
    "# Output files summary\n",
    "log(f\"\\nOUTPUT FILES GENERATED:\")\n",
    "total_size = 0\n",
    "for file_type, file_info in output_files.items():\n",
    "    size_info = f\" ({file_info['size_mb']:.1f} MB)\" if \"size_mb\" in file_info else \"\"\n",
    "    filename = os.path.basename(file_info[\"path\"])\n",
    "    log(f\" {filename}{size_info}\")\n",
    "    if \"size_mb\" in file_info:\n",
    "        total_size += file_info[\"size_mb\"]\n",
    "log(f\" Total output size: {total_size:.1f} MB\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = comprehensive_report[\"recommendations\"]\n",
    "if recommendations:\n",
    "    log(f\"\\nRECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        log(f\"{i}. {rec}\")\n",
    "\n",
    "log(f\"\\nFINAL STATUS:\")\n",
    "if exec_summary[\"ready_for_part_2b\"]:\n",
    "    log(f\" READY FOR PART 2B - All requirements met\")\n",
    "    log(f\"  Quality: {exec_summary['overall_quality_rating']}\")\n",
    "    log(f\"  Coverage: {exec_summary['dataset_coverage_percentage']:.1f}%\")\n",
    "    log(f\"  Matrices: {exec_summary['matrices_consolidated']}\")\n",
    "else:\n",
    "    log(f\" LIMITED READINESS - Part 2B possible with reduced performance\")\n",
    "    log(f\"  Review recommendations for improvement strategies\")\n",
    "\n",
    "# Memory cleanup\n",
    "del consolidated_matrices, supporting_data\n",
    "gc.collect()\n",
    "\n",
    "final_memory = check_memory()\n",
    "log(f\"\\nMemory cleanup completed: {final_memory:.1f}% usage\")\n",
    "log(f\" Part 2A4 finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
