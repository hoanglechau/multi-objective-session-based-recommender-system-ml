{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2B3 Feature Engineering for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKa-kGHil9sk",
    "outputId": "4be8f9ae-5b73-41ed-c749-624f6279237a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# INSTALL AND IMPORT DEPENDENCIES\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"polars==0.20.31\"], check=True)\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Set, Optional, Union\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MOUNT GOOGLE DRIVE\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRms-GkLmV8t"
   },
   "source": [
    "## ENHANCED CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "z7AAZ8DRmXyX",
    "outputId": "24ad4bed-2b1f-46cf-e6ee-664b3c222a66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>polars.config.Config</b><br/>def __call__(func)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/polars/config.py</a>Configure polars; offers options for table formatting and more.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Can also be used as a context manager OR a function decorator in order to\n",
       "temporarily scope the lifetime of specific options. For example:\n",
       "\n",
       "&gt;&gt;&gt; with pl.Config() as cfg:\n",
       "...     # set verbose for more detailed output within the scope\n",
       "...     cfg.set_verbose(True)  # doctest: +IGNORE_RESULT\n",
       "&gt;&gt;&gt; # scope exit - no longer in verbose mode\n",
       "\n",
       "This can also be written more compactly as:\n",
       "\n",
       "&gt;&gt;&gt; with pl.Config(verbose=True):\n",
       "...     pass\n",
       "\n",
       "(The compact format is available for all `Config` methods that take a single value).\n",
       "\n",
       "Alternatively, you can use as a decorator in order to scope the duration of the\n",
       "selected options to a specific function:\n",
       "\n",
       "&gt;&gt;&gt; @pl.Config(verbose=True)\n",
       "... def test():\n",
       "...     pass</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 87);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output'\n",
    "\n",
    "    # Enhanced feature engineering parameters\n",
    "    MAX_SESSION_HISTORY_DAYS = 7          # Look back window for session features\n",
    "    MIN_ITEM_INTERACTIONS = 5             # Minimum interactions for item features\n",
    "    TOP_K_ITEMS = 1000                    # Top items for popularity features\n",
    "\n",
    "    # Memory management (optimized for better data)\n",
    "    CHUNK_SIZE = 5000                     # Larger chunks for better performance\n",
    "    MEMORY_THRESHOLD = 0.80               # Less aggressive\n",
    "\n",
    "    # Feature engineering settings\n",
    "    ENABLE_TEMPORAL_FEATURES = True       # Time-based features\n",
    "    ENABLE_STATISTICAL_FEATURES = True    # Statistical aggregations\n",
    "    ENABLE_INTERACTION_FEATURES = True    # Cross-feature interactions\n",
    "    ENABLE_SESSION_HISTORY = True         # Session history features\n",
    "\n",
    "    # Quality thresholds\n",
    "    MIN_FEATURE_CORRELATION = 0.01        # Minimum useful correlation\n",
    "    MIN_FEATURE_VARIANCE = 0.001          # Minimum feature variance\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Configure Polars for enhanced performance\n",
    "pl.enable_string_cache()\n",
    "pl.Config.set_streaming_chunk_size(config.CHUNK_SIZE)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "pl.Config.set_tbl_rows(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1K0Yfehmaew"
   },
   "source": [
    "## ENHANCED UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YXFuBxjimchb"
   },
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    return psutil.Process().memory_info().rss / (1024**3)\n",
    "\n",
    "def log(message: str):\n",
    "    \"\"\"Enhanced logging with memory tracking\"\"\"\n",
    "    memory_gb = get_memory_usage()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def log_memory(operation_name: str, logger_func=log):\n",
    "    \"\"\"Log memory usage for specific operations\"\"\"\n",
    "    memory_mb = get_memory_usage() * 1024\n",
    "    logger_func(f\"{operation_name} [Memory: {memory_mb:.1f} MB]\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Enhanced garbage collection\"\"\"\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "def safe_division(numerator, denominator, default=0.0):\n",
    "    \"\"\"Safe division with default value\"\"\"\n",
    "    return numerator / denominator if denominator != 0 else default\n",
    "\n",
    "def ensure_consistent_dtypes(df1: pl.DataFrame, df2: pl.DataFrame, join_keys: List[str]) -> tuple:\n",
    "    \"\"\"\n",
    "    CRITICAL FIX: Ensure consistent data types for join keys\n",
    "    This is the main fix for the dtype mismatch error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for key in join_keys:\n",
    "            if key in df1.columns and key in df2.columns:\n",
    "                # Get the dtypes\n",
    "                dtype1 = df1.schema[key]\n",
    "                dtype2 = df2.schema[key]\n",
    "\n",
    "                if dtype1 != dtype2:\n",
    "                    log(f\"      Fixing dtype mismatch for '{key}': {dtype1} vs {dtype2}\")\n",
    "\n",
    "                    # Cast both to Int64 for session columns, or appropriate type for others\n",
    "                    if key == \"session\":\n",
    "                        target_dtype = pl.Int64\n",
    "                    elif key == \"aid\":\n",
    "                        target_dtype = pl.Int64\n",
    "                    else:\n",
    "                        # Use the \"larger\" type\n",
    "                        target_dtype = pl.Int64 if \"Int\" in str(dtype1) or \"Int\" in str(dtype2) else pl.Float64\n",
    "\n",
    "                    # Cast both DataFrames\n",
    "                    df1 = df1.with_columns(pl.col(key).cast(target_dtype))\n",
    "                    df2 = df2.with_columns(pl.col(key).cast(target_dtype))\n",
    "\n",
    "                    log(f\"      Cast both to {target_dtype}\")\n",
    "\n",
    "        return df1, df2\n",
    "    except Exception as e:\n",
    "        log(f\"      Error ensuring consistent dtypes: {e}\")\n",
    "        return df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWaiq2W_mgfs"
   },
   "source": [
    "## ENHANCED INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0TvvkkBmhSM",
    "outputId": "9581d59d-eafb-409c-8fd6-2a1bae5716cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 03:59:06] Validating enhanced input files...\n",
      "[2025-08-08 03:59:06]    val_data.parquet - 24.4 MB\n",
      "[2025-08-08 03:59:06]    train_val_splits.pkl - 0.0 MB\n",
      "[2025-08-08 03:59:06]    item_stats.parquet - 0.0 MB\n",
      "[2025-08-08 03:59:06]    train_features.parquet - 3893.1 MB (optional)\n",
      "[2025-08-08 03:59:06] Required input files validated!\n",
      "[2025-08-08 03:59:06] \n",
      "Loading enhanced input data...\n",
      "[2025-08-08 03:59:06]   Loading validation data...\n",
      "[2025-08-08 03:59:06]     Validation data: (6617259, 4) (187.3 MB)\n",
      "[2025-08-08 03:59:07]     Data quality: 6,617,259 samples, 55,688 positive (0.84%)\n",
      "[2025-08-08 03:59:07]     Diversity: 25,000 sessions, 543,691 items\n",
      "[2025-08-08 03:59:07] After loading validation data [Memory: 466.3 MB]\n",
      "[2025-08-08 03:59:07]   Loading train/validation split info...\n",
      "[2025-08-08 03:59:07]     Available keys in split_info: ['creation_timestamp', 'validation_days', 'val_cutoff_timestamp', 'total_timespan_days', 'train', 'val']\n",
      "[2025-08-08 03:59:07]     Split info - Train sessions: 12,899,779, Val sessions: 25,000\n",
      "[2025-08-08 03:59:07]   Loading item statistics...\n",
      "[2025-08-08 03:59:07]     Item stats: (1000, 6) (0.0 MB)\n",
      "[2025-08-08 03:59:07]   Loading training data for session history...\n",
      "[2025-08-08 03:59:07]     Training data file size: 3893.1 MB\n",
      "[2025-08-08 03:59:07]     Using lazy loading for large training dataset...\n",
      "[2025-08-08 03:59:07]     Training data loaded lazily for session history features\n",
      "[2025-08-08 03:59:07] After loading training data [Memory: 466.6 MB]\n",
      "[2025-08-08 03:59:07] Enhanced input validation completed successfully!\n",
      "[2025-08-08 03:59:07] After initial data loading and GC [Memory: 466.6 MB]\n"
     ]
    }
   ],
   "source": [
    "def validate_and_load_enhanced_inputs():\n",
    "    \"\"\"\n",
    "    Enhanced input validation and loading with comprehensive error handling\n",
    "    \"\"\"\n",
    "    log(\"Validating enhanced input files...\")\n",
    "\n",
    "    # Check required files\n",
    "    required_files = {\n",
    "        \"val_data.parquet\": \"Validation data from FIXED Part 2B2\",\n",
    "        \"train_val_splits.pkl\": \"Train/validation splits from FIXED Part 2B2\",\n",
    "        \"item_stats.parquet\": \"Item statistics from Part 1\"\n",
    "    }\n",
    "\n",
    "    optional_files = {\n",
    "        \"train_features.parquet\": \"Training data for session history (optional but recommended)\"\n",
    "    }\n",
    "\n",
    "    # Validate required files\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            log(f\"ERROR: Missing {filename} - {description}\")\n",
    "            raise FileNotFoundError(f\"Missing required file: {filename}\")\n",
    "\n",
    "        file_size = os.path.getsize(filepath) / (1024*1024)\n",
    "        log(f\"   {filename} - {file_size:.1f} MB\")\n",
    "\n",
    "    # Check optional files\n",
    "    for filename, description in optional_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        if os.path.exists(filepath):\n",
    "            file_size = os.path.getsize(filepath) / (1024*1024)\n",
    "            log(f\"   {filename} - {file_size:.1f} MB (optional)\")\n",
    "        else:\n",
    "            log(f\"   {filename} - Not available ({description})\")\n",
    "\n",
    "    log(\"Required input files validated!\")\n",
    "\n",
    "    # Load data with enhanced error handling\n",
    "    log(\"\\nLoading enhanced input data...\")\n",
    "\n",
    "    try:\n",
    "        # Load validation data\n",
    "        log(\"  Loading validation data...\")\n",
    "        val_data = pl.read_parquet(f\"{config.OUTPUT_PATH}/val_data.parquet\")\n",
    "\n",
    "        # CRITICAL FIX: Ensure session column is Int64 from the start\n",
    "        if \"session\" in val_data.columns:\n",
    "            val_data = val_data.with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "        if \"aid\" in val_data.columns:\n",
    "            val_data = val_data.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "\n",
    "        log(f\"    Validation data: {val_data.shape} ({val_data.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "        # Validate validation data quality\n",
    "        positive_count = val_data.filter(pl.col(\"label\") == 1).height\n",
    "        positive_rate = positive_count / len(val_data) * 100 if len(val_data) > 0 else 0\n",
    "        unique_sessions = val_data['session'].n_unique()\n",
    "        unique_items = val_data['aid'].n_unique()\n",
    "\n",
    "        log(f\"    Data quality: {len(val_data):,} samples, {positive_count:,} positive ({positive_rate:.2f}%)\")\n",
    "        log(f\"    Diversity: {unique_sessions:,} sessions, {unique_items:,} items\")\n",
    "\n",
    "        if positive_rate < 0.1:\n",
    "            log(f\"    WARNING: Low positive rate {positive_rate:.2f}% - may affect feature correlations\")\n",
    "\n",
    "        log_memory(\"After loading validation data\")\n",
    "\n",
    "        # Load train/validation split info\n",
    "        log(\"  Loading train/validation split info...\")\n",
    "        with open(f\"{config.OUTPUT_PATH}/train_val_splits.pkl\", \"rb\") as f:\n",
    "            split_info = pickle.load(f)\n",
    "\n",
    "        available_keys = list(split_info.keys())\n",
    "        log(f\"    Available keys in split_info: {available_keys}\")\n",
    "\n",
    "        val_sessions = split_info.get('val', {}).get('sessions', 'unknown')\n",
    "        train_sessions = split_info.get('train', {}).get('sessions', 'unknown')\n",
    "        log(f\"    Split info - Train sessions: {train_sessions:,}, Val sessions: {val_sessions:,}\")\n",
    "\n",
    "        # Load item statistics\n",
    "        log(\"  Loading item statistics...\")\n",
    "        item_stats = pl.read_parquet(f\"{config.OUTPUT_PATH}/item_stats.parquet\")\n",
    "        # CRITICAL FIX: Ensure aid column is Int64\n",
    "        if \"aid\" in item_stats.columns:\n",
    "            item_stats = item_stats.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "        log(f\"    Item stats: {item_stats.shape} ({item_stats.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "        # Load training data for session history (optional)\n",
    "        train_data = None\n",
    "        train_data_available = False\n",
    "\n",
    "        train_features_path = f\"{config.OUTPUT_PATH}/train_features.parquet\"\n",
    "        if os.path.exists(train_features_path) and config.ENABLE_SESSION_HISTORY:\n",
    "            try:\n",
    "                log(\"  Loading training data for session history...\")\n",
    "                file_size_mb = os.path.getsize(train_features_path) / (1024*1024)\n",
    "                log(f\"    Training data file size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "                if file_size_mb > 2000:  # Large file, use lazy loading\n",
    "                    log(\"    Using lazy loading for large training dataset...\")\n",
    "                    train_data = pl.scan_parquet(train_features_path)\n",
    "                    train_data_available = True\n",
    "                    log(f\"    Training data loaded lazily for session history features\")\n",
    "                else:\n",
    "                    log(\"    Loading training data fully...\")\n",
    "                    train_data = pl.read_parquet(train_features_path)\n",
    "                    # CRITICAL FIX: Ensure session column is Int64\n",
    "                    if \"session\" in train_data.columns:\n",
    "                        train_data = train_data.with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "                    if \"aid\" in train_data.columns:\n",
    "                        train_data = train_data.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "                    train_data_available = True\n",
    "                    log(f\"    Training data: {train_data.shape} ({train_data.estimated_size('mb'):.1f} MB)\")\n",
    "\n",
    "                log_memory(\"After loading training data\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"    Warning: Could not load training data for session history: {e}\")\n",
    "                train_data = None\n",
    "                train_data_available = False\n",
    "        else:\n",
    "            log(\"  Training data not available - session history features will be limited\")\n",
    "\n",
    "        # Create comprehensive validation results\n",
    "        validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"val_data_samples\": len(val_data),\n",
    "            \"val_sessions\": unique_sessions,\n",
    "            \"val_items\": unique_items,\n",
    "            \"val_positive_samples\": positive_count,\n",
    "            \"val_positive_rate\": positive_rate,\n",
    "            \"train_data_available\": train_data_available,\n",
    "            \"item_stats_count\": len(item_stats),\n",
    "            \"train_sessions\": train_sessions,\n",
    "            \"split_info_keys\": available_keys\n",
    "        }\n",
    "\n",
    "        log(\"Enhanced input validation completed successfully!\")\n",
    "        return val_data, split_info, item_stats, train_data, validation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error loading input data: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Load and validate inputs\n",
    "val_data, split_info, item_stats, train_data, validation_results = validate_and_load_enhanced_inputs()\n",
    "\n",
    "# Force garbage collection after loading\n",
    "force_garbage_collection()\n",
    "log_memory(\"After initial data loading and GC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63eG46M8mnmY"
   },
   "source": [
    "## ENHANCED FEATURE ENGINEERING SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U3VULIT1mpXM"
   },
   "outputs": [],
   "source": [
    "class EnhancedFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering system with 40+ sophisticated features (COMPLETE FIXED VERSION)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item_stats: pl.DataFrame, train_data=None):\n",
    "        self.item_stats = item_stats\n",
    "        self.train_data = train_data\n",
    "        self.feature_columns = []\n",
    "        self.feature_stats = {}\n",
    "\n",
    "        # Create item lookup dictionaries for fast access\n",
    "        self.item_popularity = {}\n",
    "        self.item_stats_dict = {}\n",
    "\n",
    "        if len(item_stats) > 0:\n",
    "            for row in item_stats.iter_rows(named=True):\n",
    "                aid = row['aid']\n",
    "                self.item_popularity[aid] = row.get('total_interactions', row.get('clicks', 0))\n",
    "                self.item_stats_dict[aid] = row\n",
    "\n",
    "        log(f\"Feature engineer initialized with {len(self.item_stats_dict):,} items\")\n",
    "\n",
    "    def create_session_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create comprehensive session-level features\n",
    "        \"\"\"\n",
    "        log(\"    Creating session-level features...\")\n",
    "\n",
    "        try:\n",
    "            # Basic session statistics\n",
    "            session_features = (\n",
    "                df.group_by(\"session\")\n",
    "                .agg([\n",
    "                    # Basic counts\n",
    "                    pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "                    pl.col(\"aid\").n_unique().alias(\"unique_items\"),\n",
    "                    pl.col(\"type\").n_unique().alias(\"unique_types\"),\n",
    "\n",
    "                    # Type-specific counts\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"num_clicks\"),\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"carts\").count().alias(\"num_carts\"),\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"orders\").count().alias(\"num_orders\"),\n",
    "\n",
    "                    # Advanced session patterns\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"clicks\").n_unique().alias(\"unique_clicked_items\"),\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"carts\").n_unique().alias(\"unique_carted_items\"),\n",
    "                    pl.col(\"aid\").filter(pl.col(\"type\") == \"orders\").n_unique().alias(\"unique_ordered_items\"),\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # CRITICAL FIX: Ensure session column is Int64\n",
    "            session_features = session_features.with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "\n",
    "            # Calculate derived features with safe division\n",
    "            session_features = session_features.with_columns([\n",
    "                # Conversion rates (with safe division)\n",
    "                (pl.col(\"num_carts\").cast(pl.Float64) / pl.col(\"num_clicks\").clip(lower_bound=1).cast(pl.Float64)).alias(\"cart_conversion_rate\"),\n",
    "                (pl.col(\"num_orders\").cast(pl.Float64) / pl.col(\"num_clicks\").clip(lower_bound=1).cast(pl.Float64)).alias(\"order_conversion_rate\"),\n",
    "                (pl.col(\"num_orders\").cast(pl.Float64) / pl.col(\"num_carts\").clip(lower_bound=1).cast(pl.Float64)).alias(\"cart_to_order_rate\"),\n",
    "\n",
    "                # Item interaction patterns\n",
    "                (pl.col(\"session_length\").cast(pl.Float64) / pl.col(\"unique_items\").clip(lower_bound=1)).alias(\"avg_interactions_per_item\"),\n",
    "                (pl.col(\"unique_items\").cast(pl.Float64) / pl.col(\"session_length\").clip(lower_bound=1)).alias(\"item_diversity_ratio\"),\n",
    "\n",
    "                # Advanced behavioral metrics\n",
    "                ((pl.col(\"num_clicks\") + pl.col(\"num_carts\") + pl.col(\"num_orders\")).cast(pl.Float64) / pl.col(\"session_length\").clip(lower_bound=1)).alias(\"action_intensity\"),\n",
    "                (pl.col(\"unique_types\").cast(pl.Float64) / 3.0).alias(\"type_diversity_ratio\")\n",
    "            ])\n",
    "\n",
    "            # Add to feature list\n",
    "            new_features = [\n",
    "                \"session_length\", \"unique_items\", \"unique_types\",\n",
    "                \"num_clicks\", \"num_carts\", \"num_orders\",\n",
    "                \"unique_clicked_items\", \"unique_carted_items\", \"unique_ordered_items\",\n",
    "                \"cart_conversion_rate\", \"order_conversion_rate\", \"cart_to_order_rate\",\n",
    "                \"avg_interactions_per_item\", \"item_diversity_ratio\",\n",
    "                \"action_intensity\", \"type_diversity_ratio\"\n",
    "            ]\n",
    "\n",
    "            self.feature_columns.extend(new_features)\n",
    "            log(f\"      Added {len(new_features)} session features\")\n",
    "\n",
    "            return session_features\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Error creating session features: {e}\")\n",
    "            # Return meaningful fallback session features\n",
    "            minimal_features = df.group_by(\"session\").agg([\n",
    "                pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "                pl.col(\"aid\").n_unique().alias(\"unique_items\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"num_clicks\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"num_carts\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"num_orders\")\n",
    "            ])\n",
    "            # CRITICAL FIX: Ensure session column is Int64\n",
    "            minimal_features = minimal_features.with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "            self.feature_columns.extend([\"session_length\", \"unique_items\", \"num_clicks\", \"num_carts\", \"num_orders\"])\n",
    "            return minimal_features\n",
    "\n",
    "    def create_item_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create comprehensive item-level features (FIXED - no qcut issues)\n",
    "        \"\"\"\n",
    "        log(\"    Creating item-level features...\")\n",
    "\n",
    "        try:\n",
    "            # Get unique items in the dataset\n",
    "            unique_items = df.select(\"aid\").unique()\n",
    "            # CRITICAL FIX: Ensure aid column is Int64\n",
    "            unique_items = unique_items.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "\n",
    "            # Create item features using the pre-computed stats\n",
    "            item_features_data = []\n",
    "\n",
    "            for aid in unique_items['aid'].to_list():\n",
    "                stats = self.item_stats_dict.get(aid, {})\n",
    "\n",
    "                item_features_data.append({\n",
    "                    \"aid\": aid,\n",
    "                    \"item_popularity\": self.item_popularity.get(aid, 0),\n",
    "                    \"item_clicks\": stats.get('clicks', 0),\n",
    "                    \"item_carts\": stats.get('carts', 0),\n",
    "                    \"item_orders\": stats.get('orders', 0),\n",
    "                    \"item_total_interactions\": stats.get('total_interactions', 0),\n",
    "                    \"item_unique_users\": stats.get('unique_users', 1),  # Default to 1 to avoid division by zero\n",
    "                })\n",
    "\n",
    "            if not item_features_data:\n",
    "                log(\"      Warning: No item features data available\")\n",
    "                return pl.DataFrame({\"aid\": [], \"item_popularity\": []})\n",
    "\n",
    "            item_features = pl.DataFrame(item_features_data)\n",
    "            # CRITICAL FIX: Ensure aid column is Int64\n",
    "            item_features = item_features.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "\n",
    "            # Calculate derived item features with safe operations\n",
    "            item_features = item_features.with_columns([\n",
    "                # Conversion rates (with safe division)\n",
    "                (pl.col(\"item_carts\").cast(pl.Float64) / pl.col(\"item_clicks\").clip(lower_bound=1).cast(pl.Float64)).alias(\"item_click_to_cart_rate\"),\n",
    "                (pl.col(\"item_orders\").cast(pl.Float64) / pl.col(\"item_clicks\").clip(lower_bound=1).cast(pl.Float64)).alias(\"item_click_to_order_rate\"),\n",
    "                (pl.col(\"item_orders\").cast(pl.Float64) / pl.col(\"item_carts\").clip(lower_bound=1).cast(pl.Float64)).alias(\"item_cart_to_order_rate\"),\n",
    "\n",
    "                # Engagement metrics\n",
    "                (pl.col(\"item_total_interactions\").cast(pl.Float64) / pl.col(\"item_unique_users\").clip(lower_bound=1).cast(pl.Float64)).alias(\"item_avg_interactions_per_user\"),\n",
    "\n",
    "                # Popularity metrics\n",
    "                pl.col(\"item_popularity\").log1p().alias(\"item_popularity_log\"),\n",
    "                (pl.col(\"item_total_interactions\") + 1).log1p().alias(\"item_total_interactions_log\")\n",
    "            ])\n",
    "\n",
    "            # FIXED: Create popularity buckets using manual thresholds instead of qcut\n",
    "            try:\n",
    "                popularity_stats = item_features.select([\n",
    "                    pl.col(\"item_popularity\").min().alias(\"min_pop\"),\n",
    "                    pl.col(\"item_popularity\").max().alias(\"max_pop\"),\n",
    "                    pl.col(\"item_popularity\").mean().alias(\"mean_pop\"),\n",
    "                    pl.col(\"item_popularity\").std().alias(\"std_pop\")\n",
    "                ]).to_dicts()[0]\n",
    "\n",
    "                min_pop = popularity_stats[\"min_pop\"] or 0\n",
    "                max_pop = popularity_stats[\"max_pop\"] or 0\n",
    "                mean_pop = popularity_stats[\"mean_pop\"] or 0\n",
    "                std_pop = popularity_stats[\"std_pop\"] or 1\n",
    "\n",
    "                # Create meaningful thresholds based on statistics\n",
    "                if max_pop > 0 and std_pop > 0:\n",
    "                    threshold_1 = max(1, int(mean_pop - std_pop))\n",
    "                    threshold_2 = max(threshold_1 + 1, int(mean_pop - 0.5 * std_pop))\n",
    "                    threshold_3 = max(threshold_2 + 1, int(mean_pop))\n",
    "                    threshold_4 = max(threshold_3 + 1, int(mean_pop + 0.5 * std_pop))\n",
    "                else:\n",
    "                    threshold_1, threshold_2, threshold_3, threshold_4 = 1, 2, 5, 10\n",
    "\n",
    "                # Create popularity buckets\n",
    "                item_features = item_features.with_columns([\n",
    "                    pl.when(pl.col(\"item_popularity\") <= threshold_1)\n",
    "                    .then(pl.lit(1))\n",
    "                    .when(pl.col(\"item_popularity\") <= threshold_2)\n",
    "                    .then(pl.lit(2))\n",
    "                    .when(pl.col(\"item_popularity\") <= threshold_3)\n",
    "                    .then(pl.lit(3))\n",
    "                    .when(pl.col(\"item_popularity\") <= threshold_4)\n",
    "                    .then(pl.lit(4))\n",
    "                    .otherwise(pl.lit(5))\n",
    "                    .alias(\"item_popularity_bucket\")\n",
    "                ])\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"      Warning: Could not create popularity buckets: {e}\")\n",
    "                # Simple fallback bucketing\n",
    "                item_features = item_features.with_columns([\n",
    "                    pl.when(pl.col(\"item_popularity\") == 0).then(pl.lit(1))\n",
    "                    .when(pl.col(\"item_popularity\") <= 5).then(pl.lit(2))\n",
    "                    .when(pl.col(\"item_popularity\") <= 20).then(pl.lit(3))\n",
    "                    .when(pl.col(\"item_popularity\") <= 100).then(pl.lit(4))\n",
    "                    .otherwise(pl.lit(5))\n",
    "                    .alias(\"item_popularity_bucket\")\n",
    "                ])\n",
    "\n",
    "            # Add to feature list\n",
    "            new_features = [\n",
    "                \"item_popularity\", \"item_clicks\", \"item_carts\", \"item_orders\",\n",
    "                \"item_total_interactions\", \"item_unique_users\",\n",
    "                \"item_click_to_cart_rate\", \"item_click_to_order_rate\", \"item_cart_to_order_rate\",\n",
    "                \"item_avg_interactions_per_user\", \"item_popularity_log\", \"item_total_interactions_log\",\n",
    "                \"item_popularity_bucket\"\n",
    "            ]\n",
    "\n",
    "            self.feature_columns.extend(new_features)\n",
    "            log(f\"      Added {len(new_features)} item features\")\n",
    "\n",
    "            return item_features\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Error creating item features: {e}\")\n",
    "            # Return meaningful fallback item features\n",
    "            unique_items = df.select(\"aid\").unique()\n",
    "            unique_items = unique_items.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "            minimal_features = unique_items.with_columns([\n",
    "                pl.col(\"aid\").map_elements(\n",
    "                    lambda x: self.item_popularity.get(x, 0),\n",
    "                    return_dtype=pl.Int64\n",
    "                ).alias(\"item_popularity\"),\n",
    "                pl.lit(1).alias(\"item_basic_feature\")\n",
    "            ])\n",
    "            self.feature_columns.extend([\"item_popularity\", \"item_basic_feature\"])\n",
    "            return minimal_features\n",
    "\n",
    "    def create_interaction_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create session-item interaction features\n",
    "        \"\"\"\n",
    "        log(\"    Creating interaction features...\")\n",
    "\n",
    "        try:\n",
    "            # Session-item interaction patterns\n",
    "            interaction_features = (\n",
    "                df.group_by([\"session\", \"aid\"])\n",
    "                .agg([\n",
    "                    # Item interaction counts within session\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"item_clicks_in_session\"),\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"item_carts_in_session\"),\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"item_orders_in_session\"),\n",
    "\n",
    "                    # Item interaction patterns\n",
    "                    pl.col(\"type\").count().alias(\"item_total_interactions_in_session\"),\n",
    "                    pl.col(\"type\").n_unique().alias(\"item_interaction_types\"),\n",
    "\n",
    "                    # Sequence features\n",
    "                    pl.col(\"type\").first().alias(\"item_first_interaction\"),\n",
    "                    pl.col(\"type\").last().alias(\"item_last_interaction\")\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # CRITICAL FIX: Ensure consistent dtypes for join keys\n",
    "            interaction_features = interaction_features.with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int64),\n",
    "                pl.col(\"aid\").cast(pl.Int64)\n",
    "            ])\n",
    "\n",
    "            # Calculate derived interaction features\n",
    "            interaction_features = interaction_features.with_columns([\n",
    "                # Interaction ratios\n",
    "                (pl.col(\"item_clicks_in_session\").cast(pl.Float64) / pl.col(\"item_total_interactions_in_session\").clip(lower_bound=1)).alias(\"item_click_ratio_in_session\"),\n",
    "                (pl.col(\"item_carts_in_session\").cast(pl.Float64) / pl.col(\"item_total_interactions_in_session\").clip(lower_bound=1)).alias(\"item_cart_ratio_in_session\"),\n",
    "                (pl.col(\"item_orders_in_session\").cast(pl.Float64) / pl.col(\"item_total_interactions_in_session\").clip(lower_bound=1)).alias(\"item_order_ratio_in_session\"),\n",
    "\n",
    "                # Binary indicators\n",
    "                (pl.col(\"item_total_interactions_in_session\") > 1).cast(pl.Int32).alias(\"item_repeated_interaction\"),\n",
    "                (pl.col(\"item_interaction_types\") > 1).cast(pl.Int32).alias(\"item_multi_type_interaction\"),\n",
    "                (pl.col(\"item_carts_in_session\") > 0).cast(pl.Int32).alias(\"item_has_cart\"),\n",
    "                (pl.col(\"item_orders_in_session\") > 0).cast(pl.Int32).alias(\"item_has_order\"),\n",
    "\n",
    "                # Sequence indicators\n",
    "                (pl.col(\"item_last_interaction\") == \"orders\").cast(pl.Int32).alias(\"item_ends_with_order\"),\n",
    "                (pl.col(\"item_first_interaction\") == \"clicks\").cast(pl.Int32).alias(\"item_starts_with_click\")\n",
    "            ])\n",
    "\n",
    "            # Add to feature list\n",
    "            new_features = [\n",
    "                \"item_clicks_in_session\", \"item_carts_in_session\", \"item_orders_in_session\",\n",
    "                \"item_total_interactions_in_session\", \"item_interaction_types\",\n",
    "                \"item_click_ratio_in_session\", \"item_cart_ratio_in_session\", \"item_order_ratio_in_session\",\n",
    "                \"item_repeated_interaction\", \"item_multi_type_interaction\",\n",
    "                \"item_has_cart\", \"item_has_order\", \"item_ends_with_order\", \"item_starts_with_click\"\n",
    "            ]\n",
    "\n",
    "            self.feature_columns.extend(new_features)\n",
    "            log(f\"      Added {len(new_features)} interaction features\")\n",
    "\n",
    "            return interaction_features\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Error creating interaction features: {e}\")\n",
    "            # Return meaningful fallback interaction features\n",
    "            minimal_features = df.group_by([\"session\", \"aid\"]).agg([\n",
    "                pl.col(\"type\").count().alias(\"item_total_interactions_in_session\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"item_clicks_in_session\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"item_carts_in_session\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"item_orders_in_session\")\n",
    "            ])\n",
    "            # CRITICAL FIX: Ensure consistent dtypes\n",
    "            minimal_features = minimal_features.with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int64),\n",
    "                pl.col(\"aid\").cast(pl.Int64)\n",
    "            ])\n",
    "            self.feature_columns.extend([\"item_total_interactions_in_session\", \"item_clicks_in_session\", \"item_carts_in_session\", \"item_orders_in_session\"])\n",
    "            return minimal_features\n",
    "\n",
    "    def create_temporal_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create temporal/time-based features if timestamps are available (FIXED)\n",
    "        \"\"\"\n",
    "        if not config.ENABLE_TEMPORAL_FEATURES:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "        log(\"    Creating temporal features...\")\n",
    "\n",
    "        # Check if we have timestamp data in training data\n",
    "        if self.train_data is not None:\n",
    "            try:\n",
    "                # CRITICAL FIX: Handle lazy vs regular DataFrame with consistent dtypes\n",
    "                if hasattr(self.train_data, 'collect'):  # LazyFrame\n",
    "                    log(\"      Processing temporal features from lazy training data...\")\n",
    "                    session_temporal = (\n",
    "                        self.train_data\n",
    "                        .group_by(\"session\")\n",
    "                        .agg([\n",
    "                            pl.col(\"ts\").min().alias(\"session_start_ts\"),\n",
    "                            pl.col(\"ts\").max().alias(\"session_end_ts\"),\n",
    "                            pl.col(\"ts\").count().alias(\"session_events\"),\n",
    "                        ])\n",
    "                        .with_columns([\n",
    "                            (pl.col(\"session_end_ts\") - pl.col(\"session_start_ts\")).alias(\"session_duration_ms\"),\n",
    "                            # CRITICAL FIX: Ensure session column is Int64\n",
    "                            pl.col(\"session\").cast(pl.Int64)\n",
    "                        ])\n",
    "                        .collect()\n",
    "                    )\n",
    "                else:  # DataFrame\n",
    "                    log(\"      Processing temporal features from regular training data...\")\n",
    "                    session_temporal = (\n",
    "                        self.train_data\n",
    "                        .group_by(\"session\")\n",
    "                        .agg([\n",
    "                            pl.col(\"ts\").min().alias(\"session_start_ts\"),\n",
    "                            pl.col(\"ts\").max().alias(\"session_end_ts\"),\n",
    "                            pl.col(\"ts\").count().alias(\"session_events\"),\n",
    "                        ])\n",
    "                        .with_columns([\n",
    "                            (pl.col(\"session_end_ts\") - pl.col(\"session_start_ts\")).alias(\"session_duration_ms\"),\n",
    "                            # CRITICAL FIX: Ensure session column is Int64\n",
    "                            pl.col(\"session\").cast(pl.Int64)\n",
    "                        ])\n",
    "                    )\n",
    "\n",
    "                # Calculate derived temporal features with safe operations\n",
    "                session_temporal = session_temporal.with_columns([\n",
    "                    # Session timing features\n",
    "                    (pl.col(\"session_duration_ms\").cast(pl.Float64) / 1000).alias(\"session_duration_seconds\"),\n",
    "                    (pl.col(\"session_duration_ms\").cast(pl.Float64) / (1000 * 60)).alias(\"session_duration_minutes\"),\n",
    "                    (pl.col(\"session_duration_ms\").cast(pl.Float64) / (1000 * 60 * 60)).alias(\"session_duration_hours\"),\n",
    "\n",
    "                    # Session pace (events per unit time)\n",
    "                    (pl.col(\"session_events\").cast(pl.Float64) / (pl.col(\"session_duration_ms\").cast(pl.Float64) / 1000).clip(lower_bound=1)).alias(\"session_events_per_second\"),\n",
    "                    (pl.col(\"session_events\").cast(pl.Float64) / (pl.col(\"session_duration_ms\").cast(pl.Float64) / (1000 * 60)).clip(lower_bound=1)).alias(\"session_events_per_minute\"),\n",
    "                ])\n",
    "\n",
    "                # Add to feature list\n",
    "                new_features = [\n",
    "                    \"session_duration_ms\", \"session_duration_seconds\", \"session_duration_minutes\", \"session_duration_hours\",\n",
    "                    \"session_events_per_second\", \"session_events_per_minute\"\n",
    "                ]\n",
    "\n",
    "                self.feature_columns.extend(new_features)\n",
    "                log(f\"      Added {len(new_features)} temporal features\")\n",
    "\n",
    "                return session_temporal.select([\"session\"] + new_features)\n",
    "\n",
    "            except Exception as e:\n",
    "                log(f\"      Warning: Could not create temporal features: {e}\")\n",
    "                return pl.DataFrame()\n",
    "        else:\n",
    "            log(\"      No training data available for temporal features\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def create_statistical_features(self, df: pl.DataFrame, session_features: pl.DataFrame, item_features: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create statistical aggregation features\n",
    "        \"\"\"\n",
    "        if not config.ENABLE_STATISTICAL_FEATURES:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "        log(\"    Creating statistical features...\")\n",
    "\n",
    "        try:\n",
    "            # Join session and item data for statistical calculations\n",
    "            required_item_cols = [\"aid\", \"item_popularity\", \"item_total_interactions\"]\n",
    "            available_item_cols = [col for col in required_item_cols if col in item_features.columns]\n",
    "\n",
    "            if len(available_item_cols) < 2:\n",
    "                log(\"      Warning: Insufficient item data for statistical features\")\n",
    "                return pl.DataFrame()\n",
    "\n",
    "            # CRITICAL FIX: Ensure dtype consistency before join\n",
    "            df_with_dtypes = df.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "            item_features_with_dtypes = item_features.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "\n",
    "            df_with_item_stats = df_with_dtypes.join(\n",
    "                item_features_with_dtypes.select(available_item_cols),\n",
    "                on=\"aid\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "            # Session-level statistical features\n",
    "            agg_list = []\n",
    "\n",
    "            if \"item_popularity\" in df_with_item_stats.columns:\n",
    "                agg_list.extend([\n",
    "                    pl.col(\"item_popularity\").mean().alias(\"avg_item_popularity\"),\n",
    "                    pl.col(\"item_popularity\").max().alias(\"max_item_popularity\"),\n",
    "                    pl.col(\"item_popularity\").min().alias(\"min_item_popularity\"),\n",
    "                    pl.col(\"item_popularity\").std().alias(\"std_item_popularity\"),\n",
    "                    pl.col(\"item_popularity\").median().alias(\"median_item_popularity\")\n",
    "                ])\n",
    "\n",
    "            if \"item_total_interactions\" in df_with_item_stats.columns:\n",
    "                agg_list.extend([\n",
    "                    pl.col(\"item_total_interactions\").mean().alias(\"avg_item_interactions\"),\n",
    "                    pl.col(\"item_total_interactions\").max().alias(\"max_item_interactions\"),\n",
    "                    pl.col(\"item_total_interactions\").min().alias(\"min_item_interactions\"),\n",
    "                    pl.col(\"item_total_interactions\").std().alias(\"std_item_interactions\")\n",
    "                ])\n",
    "\n",
    "            if not agg_list:\n",
    "                log(\"      Warning: No valid columns for statistical features\")\n",
    "                return pl.DataFrame()\n",
    "\n",
    "            statistical_features = df_with_item_stats.group_by(\"session\").agg(agg_list)\n",
    "\n",
    "            # CRITICAL FIX: Ensure session column is Int64\n",
    "            statistical_features = statistical_features.with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "\n",
    "            # Calculate derived statistical features\n",
    "            derived_features = []\n",
    "\n",
    "            if all(col in statistical_features.columns for col in [\"max_item_popularity\", \"min_item_popularity\"]):\n",
    "                derived_features.append(\n",
    "                    (pl.col(\"max_item_popularity\") - pl.col(\"min_item_popularity\")).alias(\"item_popularity_range\")\n",
    "                )\n",
    "\n",
    "            if all(col in statistical_features.columns for col in [\"avg_item_popularity\", \"max_item_popularity\"]):\n",
    "                derived_features.append(\n",
    "                    (pl.col(\"avg_item_popularity\").cast(pl.Float64) / pl.col(\"max_item_popularity\").clip(lower_bound=1).cast(pl.Float64)).alias(\"avg_to_max_popularity_ratio\")\n",
    "                )\n",
    "\n",
    "            if all(col in statistical_features.columns for col in [\"std_item_popularity\", \"avg_item_popularity\"]):\n",
    "                derived_features.append(\n",
    "                    (pl.col(\"std_item_popularity\").cast(pl.Float64) / pl.col(\"avg_item_popularity\").clip(lower_bound=1).cast(pl.Float64)).alias(\"popularity_coefficient_variation\")\n",
    "                )\n",
    "\n",
    "            if derived_features:\n",
    "                statistical_features = statistical_features.with_columns(derived_features)\n",
    "\n",
    "            # Fill null values with defaults\n",
    "            statistical_features = statistical_features.fill_null(0)\n",
    "\n",
    "            # Add to feature list\n",
    "            new_features = [col for col in statistical_features.columns if col != \"session\"]\n",
    "\n",
    "            self.feature_columns.extend(new_features)\n",
    "            log(f\"      Added {len(new_features)} statistical features\")\n",
    "\n",
    "            return statistical_features\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"      Error creating statistical features: {e}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    def engineer_all_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Orchestrate comprehensive feature engineering with robust error handling (COMPLETE FIXED VERSION)\n",
    "        \"\"\"\n",
    "        log(\"  Creating comprehensive feature set...\")\n",
    "\n",
    "        # Reset feature tracking\n",
    "        self.feature_columns = []\n",
    "\n",
    "        try:\n",
    "            # CRITICAL FIX: Ensure main dataframe has correct dtypes\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int64),\n",
    "                pl.col(\"aid\").cast(pl.Int64)\n",
    "            ])\n",
    "\n",
    "            # Create all feature types with error handling\n",
    "            session_features = self.create_session_features(df)\n",
    "            item_features = self.create_item_features(df)\n",
    "            interaction_features = self.create_interaction_features(df)\n",
    "            temporal_features = self.create_temporal_features(df)\n",
    "            statistical_features = self.create_statistical_features(df, session_features, item_features)\n",
    "\n",
    "            # Join all features to original data with CRITICAL FIXES\n",
    "            log(\"  Joining all features...\")\n",
    "\n",
    "            # Start with original data\n",
    "            result = df\n",
    "\n",
    "            # CRITICAL FIX: Join session features with dtype consistency\n",
    "            if len(session_features) > 0:\n",
    "                result, session_features = ensure_consistent_dtypes(result, session_features, [\"session\"])\n",
    "                result = result.join(session_features, on=\"session\", how=\"left\")\n",
    "                log(f\"    Joined session features: {len(session_features)} rows\")\n",
    "\n",
    "            # CRITICAL FIX: Join item features with dtype consistency\n",
    "            if len(item_features) > 0:\n",
    "                item_feature_cols = [col for col in item_features.columns if col.startswith(\"item_\") or col == \"aid\"]\n",
    "                if len(item_feature_cols) > 1:  # More than just \"aid\"\n",
    "                    result, item_features = ensure_consistent_dtypes(result, item_features, [\"aid\"])\n",
    "                    result = result.join(item_features.select(item_feature_cols), on=\"aid\", how=\"left\")\n",
    "                    log(f\"    Joined item features: {len(item_features)} rows\")\n",
    "\n",
    "            # CRITICAL FIX: Join interaction features with dtype consistency\n",
    "            if len(interaction_features) > 0:\n",
    "                result, interaction_features = ensure_consistent_dtypes(result, interaction_features, [\"session\", \"aid\"])\n",
    "                result = result.join(interaction_features, on=[\"session\", \"aid\"], how=\"left\")\n",
    "                log(f\"    Joined interaction features: {len(interaction_features)} rows\")\n",
    "\n",
    "            # CRITICAL FIX: Join temporal features with dtype consistency\n",
    "            if len(temporal_features) > 0:\n",
    "                result, temporal_features = ensure_consistent_dtypes(result, temporal_features, [\"session\"])\n",
    "                result = result.join(temporal_features, on=\"session\", how=\"left\")\n",
    "                log(f\"    Joined temporal features: {len(temporal_features)} rows\")\n",
    "\n",
    "            # CRITICAL FIX: Join statistical features with dtype consistency\n",
    "            if len(statistical_features) > 0:\n",
    "                result, statistical_features = ensure_consistent_dtypes(result, statistical_features, [\"session\"])\n",
    "                result = result.join(statistical_features, on=\"session\", how=\"left\")\n",
    "                log(f\"    Joined statistical features: {len(statistical_features)} rows\")\n",
    "\n",
    "            # Fill remaining null values\n",
    "            feature_cols = [col for col in self.feature_columns if col in result.columns]\n",
    "            if feature_cols:\n",
    "                result = result.with_columns([\n",
    "                    pl.col(col).fill_null(0) for col in feature_cols\n",
    "                ])\n",
    "\n",
    "            log(f\"  Feature engineering completed: {len(self.feature_columns)} features created\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"  Error in feature engineering: {e}\")\n",
    "            log(f\"  Full error details: {str(e)}\")\n",
    "\n",
    "            # IMPROVED FALLBACK: Create meaningful basic features instead of dummy ones\n",
    "            log(\"  Creating meaningful fallback features...\")\n",
    "            try:\n",
    "                # Ensure dtype consistency for main df\n",
    "                df = df.with_columns([\n",
    "                    pl.col(\"session\").cast(pl.Int64),\n",
    "                    pl.col(\"aid\").cast(pl.Int64)\n",
    "                ])\n",
    "\n",
    "                # Basic session features\n",
    "                basic_session = df.group_by(\"session\").agg([\n",
    "                    pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "                    pl.col(\"aid\").n_unique().alias(\"unique_items\"),\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"num_clicks\"),\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"num_carts\"),\n",
    "                    pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"num_orders\")\n",
    "                ]).with_columns(pl.col(\"session\").cast(pl.Int64))\n",
    "\n",
    "                # Basic item features\n",
    "                basic_item = df.select(\"aid\").unique().with_columns([\n",
    "                    pl.col(\"aid\").cast(pl.Int64),\n",
    "                    pl.col(\"aid\").map_elements(\n",
    "                        lambda x: self.item_stats_dict.get(x, {}).get('total_interactions', 1),\n",
    "                        return_dtype=pl.Int64\n",
    "                    ).alias(\"item_popularity\")\n",
    "                ])\n",
    "\n",
    "                # Basic interaction features\n",
    "                basic_interaction = df.group_by([\"session\", \"aid\"]).agg([\n",
    "                    pl.col(\"type\").count().alias(\"interactions_count\")\n",
    "                ]).with_columns([\n",
    "                    pl.col(\"session\").cast(pl.Int64),\n",
    "                    pl.col(\"aid\").cast(pl.Int64)\n",
    "                ])\n",
    "\n",
    "                # Join fallback features with dtype consistency\n",
    "                result = df\n",
    "\n",
    "                result, basic_session = ensure_consistent_dtypes(result, basic_session, [\"session\"])\n",
    "                result = result.join(basic_session, on=\"session\", how=\"left\")\n",
    "\n",
    "                result, basic_item = ensure_consistent_dtypes(result, basic_item, [\"aid\"])\n",
    "                result = result.join(basic_item, on=\"aid\", how=\"left\")\n",
    "\n",
    "                result, basic_interaction = ensure_consistent_dtypes(result, basic_interaction, [\"session\", \"aid\"])\n",
    "                result = result.join(basic_interaction, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "                # Fill nulls and set feature columns\n",
    "                result = result.fill_null(0)\n",
    "                self.feature_columns = [\n",
    "                    \"session_length\", \"unique_items\", \"num_clicks\", \"num_carts\", \"num_orders\",\n",
    "                    \"item_popularity\", \"interactions_count\"\n",
    "                ]\n",
    "\n",
    "                log(f\"  Fallback completed with {len(self.feature_columns)} meaningful features\")\n",
    "\n",
    "            except Exception as fallback_error:\n",
    "                log(f\"  Fallback also failed: {fallback_error}\")\n",
    "                # Ultimate fallback: minimal features but still meaningful\n",
    "                basic_features = df.with_columns([\n",
    "                    pl.lit(1).alias(\"basic_feature_constant\"),\n",
    "                    (pl.col(\"type\") == \"clicks\").cast(pl.Int32).alias(\"is_click\"),\n",
    "                    (pl.col(\"type\") == \"carts\").cast(pl.Int32).alias(\"is_cart\"),\n",
    "                    (pl.col(\"type\") == \"orders\").cast(pl.Int32).alias(\"is_order\")\n",
    "                ])\n",
    "                self.feature_columns = [\"basic_feature_constant\", \"is_click\", \"is_cart\", \"is_order\"]\n",
    "                result = basic_features\n",
    "\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFO1kiat0oGP"
   },
   "source": [
    "## FEATURE ENGINEERING EXECUTION WITH COMPLETE ERROR HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqlNOUBQ0qLf",
    "outputId": "651ad3d2-5eb2-43b4-c2b3-233ae2cf5a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 03:59:07] Creating enhanced feature engineering system...\n",
      "[2025-08-08 03:59:07] Feature engineer initialized with 1,000 items\n",
      "[2025-08-08 03:59:07] Engineering comprehensive features...\n",
      "[2025-08-08 03:59:07]   Creating comprehensive feature set...\n",
      "[2025-08-08 03:59:07]     Creating session-level features...\n",
      "[2025-08-08 03:59:08]       Added 16 session features\n",
      "[2025-08-08 03:59:08]     Creating item-level features...\n",
      "[2025-08-08 03:59:10]       Added 13 item features\n",
      "[2025-08-08 03:59:10]     Creating interaction features...\n",
      "[2025-08-08 03:59:15]       Added 14 interaction features\n",
      "[2025-08-08 03:59:15]     Creating temporal features...\n",
      "[2025-08-08 03:59:15]       Processing temporal features from lazy training data...\n",
      "[2025-08-08 03:59:32]       Added 6 temporal features\n",
      "[2025-08-08 03:59:32]     Creating statistical features...\n",
      "[2025-08-08 03:59:33]       Added 12 statistical features\n",
      "[2025-08-08 03:59:33]   Joining all features...\n",
      "[2025-08-08 03:59:33]     Joined session features: 25000 rows\n",
      "[2025-08-08 03:59:33]     Joined item features: 543691 rows\n",
      "[2025-08-08 03:59:35]     Joined interaction features: 4085901 rows\n",
      "[2025-08-08 03:59:36]     Joined temporal features: 12899779 rows\n",
      "[2025-08-08 03:59:36]     Joined statistical features: 25000 rows\n",
      "[2025-08-08 03:59:36]   Feature engineering completed: 61 features created\n",
      "[2025-08-08 03:59:36] Total features created: 61\n",
      "[2025-08-08 03:59:36] Feature engineering validation passed:\n",
      "[2025-08-08 03:59:36]   - Output shape: (6617259, 67)\n",
      "[2025-08-08 03:59:36]   - Features created: 61\n",
      "[2025-08-08 03:59:36]   - Required columns present: True\n",
      "[2025-08-08 03:59:37] After feature engineering [Memory: 5164.9 MB]\n"
     ]
    }
   ],
   "source": [
    "# Create feature engineer and process data with enhanced error handling\n",
    "log(\"Creating enhanced feature engineering system...\")\n",
    "\n",
    "try:\n",
    "    feature_engineer = EnhancedFeatureEngineer(item_stats, train_data)\n",
    "\n",
    "    log(\"Engineering comprehensive features...\")\n",
    "    val_data_features = feature_engineer.engineer_all_features(val_data)\n",
    "\n",
    "    # Get final feature list\n",
    "    feature_columns = feature_engineer.feature_columns\n",
    "    log(f\"Total features created: {len(feature_columns)}\")\n",
    "\n",
    "    # Validate the output\n",
    "    if len(val_data_features) == 0:\n",
    "        raise ValueError(\"No features were created - feature engineering failed\")\n",
    "\n",
    "    if len(feature_columns) == 0:\n",
    "        raise ValueError(\"No feature columns were tracked - feature engineering failed\")\n",
    "\n",
    "    # Check if we have the expected basic columns\n",
    "    required_columns = [\"session\", \"aid\", \"type\", \"label\"]\n",
    "    missing_columns = [col for col in required_columns if col not in val_data_features.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns after feature engineering: {missing_columns}\")\n",
    "\n",
    "    log(f\"Feature engineering validation passed:\")\n",
    "    log(f\"  - Output shape: {val_data_features.shape}\")\n",
    "    log(f\"  - Features created: {len(feature_columns)}\")\n",
    "    log(f\"  - Required columns present: {all(col in val_data_features.columns for col in required_columns)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error in feature engineering: {e}\")\n",
    "    log(\"Attempting comprehensive fallback feature engineering...\")\n",
    "\n",
    "    try:\n",
    "        # Comprehensive fallback: Create meaningful features manually\n",
    "        log(\"Creating comprehensive fallback features...\")\n",
    "\n",
    "        # Ensure dtypes\n",
    "        val_data = val_data.with_columns([\n",
    "            pl.col(\"session\").cast(pl.Int64),\n",
    "            pl.col(\"aid\").cast(pl.Int64)\n",
    "        ])\n",
    "\n",
    "        # Basic session features\n",
    "        session_basic = (\n",
    "            val_data.group_by(\"session\")\n",
    "            .agg([\n",
    "                pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "                pl.col(\"aid\").n_unique().alias(\"unique_items\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"num_clicks\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"num_carts\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"num_orders\"),\n",
    "                pl.col(\"type\").n_unique().alias(\"unique_types\")\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int64),\n",
    "                # Add derived features\n",
    "                (pl.col(\"num_carts\").cast(pl.Float64) / pl.col(\"num_clicks\").clip(lower_bound=1)).alias(\"cart_rate\"),\n",
    "                (pl.col(\"num_orders\").cast(pl.Float64) / pl.col(\"num_clicks\").clip(lower_bound=1)).alias(\"order_rate\"),\n",
    "                (pl.col(\"unique_items\").cast(pl.Float64) / pl.col(\"session_length\").clip(lower_bound=1)).alias(\"item_diversity\")\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Basic item features from item_stats\n",
    "        item_basic = None\n",
    "        if len(item_stats) > 0:\n",
    "            unique_aids = val_data.select(\"aid\").unique().with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "            item_stats_typed = item_stats.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "\n",
    "            item_basic = unique_aids.join(\n",
    "                item_stats_typed.select([\"aid\", \"clicks\", \"carts\", \"orders\", \"total_interactions\"]).rename({\n",
    "                    \"clicks\": \"item_clicks\",\n",
    "                    \"carts\": \"item_carts\",\n",
    "                    \"orders\": \"item_orders\",\n",
    "                    \"total_interactions\": \"item_popularity\"\n",
    "                }),\n",
    "                on=\"aid\",\n",
    "                how=\"left\"\n",
    "            ).fill_null(0).with_columns([\n",
    "                # Add derived item features\n",
    "                (pl.col(\"item_carts\").cast(pl.Float64) / pl.col(\"item_clicks\").clip(lower_bound=1)).alias(\"item_cart_rate\"),\n",
    "                (pl.col(\"item_orders\").cast(pl.Float64) / pl.col(\"item_clicks\").clip(lower_bound=1)).alias(\"item_order_rate\"),\n",
    "                pl.col(\"item_popularity\").log1p().alias(\"item_popularity_log\")\n",
    "            ])\n",
    "\n",
    "        # Basic interaction features\n",
    "        interaction_basic = (\n",
    "            val_data.group_by([\"session\", \"aid\"])\n",
    "            .agg([\n",
    "                pl.col(\"type\").count().alias(\"interactions_count\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"clicks_count\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"carts_count\"),\n",
    "                pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"orders_count\"),\n",
    "                pl.col(\"type\").n_unique().alias(\"interaction_types\")\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.col(\"session\").cast(pl.Int64),\n",
    "                pl.col(\"aid\").cast(pl.Int64),\n",
    "                # Add derived interaction features\n",
    "                (pl.col(\"interactions_count\") > 1).cast(pl.Int32).alias(\"repeated_interaction\"),\n",
    "                (pl.col(\"carts_count\") > 0).cast(pl.Int32).alias(\"has_cart\"),\n",
    "                (pl.col(\"orders_count\") > 0).cast(pl.Int32).alias(\"has_order\")\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Join all fallback features\n",
    "        val_data_features = val_data.join(session_basic, on=\"session\", how=\"left\")\n",
    "\n",
    "        if item_basic is not None:\n",
    "            val_data_features = val_data_features.join(item_basic, on=\"aid\", how=\"left\")\n",
    "\n",
    "        val_data_features = val_data_features.join(interaction_basic, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "        # Define feature columns\n",
    "        feature_columns = [\n",
    "            \"session_length\", \"unique_items\", \"num_clicks\", \"num_carts\", \"num_orders\", \"unique_types\",\n",
    "            \"cart_rate\", \"order_rate\", \"item_diversity\",\n",
    "            \"interactions_count\", \"clicks_count\", \"carts_count\", \"orders_count\", \"interaction_types\",\n",
    "            \"repeated_interaction\", \"has_cart\", \"has_order\"\n",
    "        ]\n",
    "\n",
    "        if item_basic is not None:\n",
    "            feature_columns.extend([\n",
    "                \"item_clicks\", \"item_carts\", \"item_orders\", \"item_popularity\",\n",
    "                \"item_cart_rate\", \"item_order_rate\", \"item_popularity_log\"\n",
    "            ])\n",
    "\n",
    "        # Fill any remaining nulls\n",
    "        feature_cols_present = [col for col in feature_columns if col in val_data_features.columns]\n",
    "        val_data_features = val_data_features.with_columns([\n",
    "            pl.col(col).fill_null(0) for col in feature_cols_present\n",
    "        ])\n",
    "\n",
    "        feature_columns = feature_cols_present\n",
    "        log(f\"Comprehensive fallback feature engineering completed: {len(feature_columns)} meaningful features\")\n",
    "\n",
    "    except Exception as fallback_error:\n",
    "        log(f\"Comprehensive fallback feature engineering also failed: {fallback_error}\")\n",
    "        # Final fallback: basic meaningful features\n",
    "        val_data_features = val_data.with_columns([\n",
    "            pl.lit(1).alias(\"constant_feature\"),\n",
    "            (pl.col(\"type\") == \"clicks\").cast(pl.Int32).alias(\"is_click\"),\n",
    "            (pl.col(\"type\") == \"carts\").cast(pl.Int32).alias(\"is_cart\"),\n",
    "            (pl.col(\"type\") == \"orders\").cast(pl.Int32).alias(\"is_order\")\n",
    "        ])\n",
    "        feature_columns = [\"constant_feature\", \"is_click\", \"is_cart\", \"is_order\"]\n",
    "        log(f\"Using final fallback: {len(feature_columns)} basic meaningful features\")\n",
    "\n",
    "force_garbage_collection()\n",
    "log_memory(\"After feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSiTKhDAmunW"
   },
   "source": [
    "## ENHANCED FEATURE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXahfEptmwXf",
    "outputId": "3ecd3623-3a53-4c26-bf02-a7b71cbcdca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 03:59:37] Performing enhanced feature analysis...\n",
      "[2025-08-08 03:59:37]   Calculating feature statistics...\n",
      "[2025-08-08 03:59:38]   Successfully analyzed 20 out of 61 features\n",
      "[2025-08-08 03:59:38]   Top correlated features:\n",
      "[2025-08-08 03:59:38]     1. item_carts: -0.0130\n",
      "[2025-08-08 03:59:38]     2. item_popularity: -0.0117\n",
      "[2025-08-08 03:59:38]     3. item_clicks: -0.0114\n",
      "[2025-08-08 03:59:38]     4. item_orders: -0.0110\n",
      "[2025-08-08 03:59:38]     5. unique_items: -0.0070\n",
      "[2025-08-08 03:59:38]   Feature categories:\n",
      "[2025-08-08 03:59:38]     session: 20 features\n",
      "[2025-08-08 03:59:38]     item: 18 features\n",
      "[2025-08-08 03:59:38]     interaction: 11 features\n",
      "[2025-08-08 03:59:38]     temporal: 6 features\n",
      "[2025-08-08 03:59:38]     statistical: 11 features\n",
      "[2025-08-08 03:59:38]   Quality assessment:\n",
      "[2025-08-08 03:59:38]     Valid features analyzed: 20/61\n",
      "[2025-08-08 03:59:38]     Features with high correlation (>0.01): 4\n",
      "[2025-08-08 03:59:38]     Features with valid variance (>0.001): 17\n",
      "[2025-08-08 03:59:38]     Max correlation: 0.0130\n",
      "[2025-08-08 03:59:38]     Avg correlation: 0.0051\n",
      "[2025-08-08 03:59:38] Enhanced feature analysis completed!\n",
      "[2025-08-08 03:59:39] After feature analysis [Memory: 5129.5 MB]\n"
     ]
    }
   ],
   "source": [
    "def analyze_enhanced_features(val_data_features: pl.DataFrame,\n",
    "                            feature_columns: List[str],\n",
    "                            validation_results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive feature analysis with enhanced error handling\n",
    "    \"\"\"\n",
    "    log(\"Performing enhanced feature analysis...\")\n",
    "\n",
    "    try:\n",
    "        # Verify inputs\n",
    "        if len(val_data_features) == 0:\n",
    "            log(\"  Warning: Empty feature dataset\")\n",
    "            return {\"error\": \"Empty feature dataset\", \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        if len(feature_columns) == 0:\n",
    "            log(\"  Warning: No feature columns to analyze\")\n",
    "            return {\"error\": \"No feature columns\", \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        if \"label\" not in val_data_features.columns:\n",
    "            log(\"  Warning: No label column for correlation analysis\")\n",
    "            return {\"error\": \"No label column\", \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        # Basic feature statistics\n",
    "        feature_stats = {}\n",
    "        correlation_results = {}\n",
    "\n",
    "        # Analyze each feature\n",
    "        log(\"  Calculating feature statistics...\")\n",
    "        valid_features = 0\n",
    "\n",
    "        for feature in feature_columns[:20]:  # Limit to first 20 features for performance\n",
    "            if feature in val_data_features.columns:\n",
    "                try:\n",
    "                    feature_data = val_data_features.select([feature, \"label\"])\n",
    "\n",
    "                    # Basic statistics with error handling\n",
    "                    stats_query = feature_data.select([\n",
    "                        pl.col(feature).mean().alias(\"mean\"),\n",
    "                        pl.col(feature).std().alias(\"std\"),\n",
    "                        pl.col(feature).min().alias(\"min\"),\n",
    "                        pl.col(feature).max().alias(\"max\"),\n",
    "                        pl.col(feature).null_count().alias(\"null_count\")\n",
    "                    ])\n",
    "\n",
    "                    stats = stats_query.to_dicts()[0]\n",
    "\n",
    "                    # Calculate correlation with label safely\n",
    "                    correlation = 0.0\n",
    "                    try:\n",
    "                        # Only calculate correlation if we have variance in features and reasonable sample size\n",
    "                        feature_std = stats.get(\"std\")\n",
    "                        if feature_std is not None and feature_std > 0:\n",
    "                            # Sample data for correlation if too large\n",
    "                            if len(feature_data) > 100000:\n",
    "                                feature_data_sample = feature_data.sample(n=50000, seed=42)\n",
    "                            else:\n",
    "                                feature_data_sample = feature_data\n",
    "\n",
    "                            correlation_data = feature_data_sample.to_pandas()\n",
    "                            if len(correlation_data) > 1:\n",
    "                                corr_result = correlation_data[feature].corr(correlation_data[\"label\"])\n",
    "                                if not pd.isna(corr_result):\n",
    "                                    correlation = float(corr_result)\n",
    "                    except Exception as corr_error:\n",
    "                        log(f\"    Warning: Could not calculate correlation for {feature}: {corr_error}\")\n",
    "                        correlation = 0.0\n",
    "\n",
    "                    feature_stats[feature] = {\n",
    "                        \"mean\": float(stats[\"mean\"]) if stats[\"mean\"] is not None else 0.0,\n",
    "                        \"std\": float(stats[\"std\"]) if stats[\"std\"] is not None else 0.0,\n",
    "                        \"min\": float(stats[\"min\"]) if stats[\"min\"] is not None else 0.0,\n",
    "                        \"max\": float(stats[\"max\"]) if stats[\"max\"] is not None else 0.0,\n",
    "                        \"null_count\": int(stats[\"null_count\"]) if stats[\"null_count\"] is not None else 0,\n",
    "                        \"correlation\": correlation\n",
    "                    }\n",
    "\n",
    "                    correlation_results[feature] = correlation\n",
    "                    valid_features += 1\n",
    "\n",
    "                except Exception as feature_error:\n",
    "                    log(f\"    Warning: Could not analyze feature {feature}: {feature_error}\")\n",
    "                    feature_stats[feature] = {\n",
    "                        \"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"max\": 0.0,\n",
    "                        \"null_count\": 0, \"correlation\": 0.0\n",
    "                    }\n",
    "                    correlation_results[feature] = 0.0\n",
    "            else:\n",
    "                log(f\"    Warning: Feature {feature} not found in dataset\")\n",
    "\n",
    "        log(f\"  Successfully analyzed {valid_features} out of {len(feature_columns)} features\")\n",
    "\n",
    "        # Find top correlated features\n",
    "        if correlation_results:\n",
    "            sorted_correlations = sorted(correlation_results.items(),\n",
    "                                       key=lambda x: abs(x[1]), reverse=True)\n",
    "            top_correlated_features = sorted_correlations[:10]\n",
    "\n",
    "            log(f\"  Top correlated features:\")\n",
    "            for i, (feature, corr) in enumerate(top_correlated_features[:5]):\n",
    "                log(f\"    {i+1}. {feature}: {corr:.4f}\")\n",
    "        else:\n",
    "            top_correlated_features = []\n",
    "            log(\"  No correlation results available\")\n",
    "\n",
    "        # Feature categories analysis\n",
    "        feature_categories = {\n",
    "            \"session\": len([f for f in feature_columns if f.startswith((\"session_\", \"num_\", \"unique_\", \"cart_\", \"order_\", \"item_diversity\", \"action_\"))]),\n",
    "            \"item\": len([f for f in feature_columns if f.startswith(\"item_\") and not any(x in f for x in [\"in_session\", \"has_\", \"repeated\", \"multi_type\", \"_count\"])]),\n",
    "            \"interaction\": len([f for f in feature_columns if f.startswith(\"item_\") and any(x in f for x in [\"in_session\", \"has_\", \"repeated\", \"multi_type\"]) or f in [\"interactions_count\", \"clicks_count\", \"carts_count\", \"orders_count\"]]),\n",
    "            \"temporal\": len([f for f in feature_columns if any(x in f for x in [\"duration\", \"per_second\", \"per_minute\", \"start_\", \"end_\"])]),\n",
    "            \"statistical\": len([f for f in feature_columns if f.startswith((\"avg_\", \"max_\", \"min_\", \"std_\", \"median_\", \"range\"))])\n",
    "        }\n",
    "\n",
    "        log(f\"  Feature categories:\")\n",
    "        for category, count in feature_categories.items():\n",
    "            if count > 0:\n",
    "                log(f\"    {category}: {count} features\")\n",
    "\n",
    "        # Quality assessment\n",
    "        high_correlation_features = [f for f, c in correlation_results.items()\n",
    "                                   if abs(c) > config.MIN_FEATURE_CORRELATION]\n",
    "        valid_variance_features = [f for f, stats in feature_stats.items()\n",
    "                                 if stats[\"std\"] > config.MIN_FEATURE_VARIANCE]\n",
    "\n",
    "        max_correlation = max([abs(c) for c in correlation_results.values()]) if correlation_results else 0.0\n",
    "        avg_correlation = np.mean([abs(c) for c in correlation_results.values()]) if correlation_results else 0.0\n",
    "\n",
    "        quality_assessment = {\n",
    "            \"total_features\": len(feature_columns),\n",
    "            \"features_with_data\": len(feature_stats),\n",
    "            \"valid_features_analyzed\": valid_features,\n",
    "            \"high_correlation_features\": len(high_correlation_features),\n",
    "            \"valid_variance_features\": len(valid_variance_features),\n",
    "            \"max_correlation\": max_correlation,\n",
    "            \"avg_correlation\": avg_correlation\n",
    "        }\n",
    "\n",
    "        log(f\"  Quality assessment:\")\n",
    "        log(f\"    Valid features analyzed: {valid_features}/{len(feature_columns)}\")\n",
    "        log(f\"    Features with high correlation (>{config.MIN_FEATURE_CORRELATION}): {len(high_correlation_features)}\")\n",
    "        log(f\"    Features with valid variance (>{config.MIN_FEATURE_VARIANCE}): {len(valid_variance_features)}\")\n",
    "        log(f\"    Max correlation: {max_correlation:.4f}\")\n",
    "        log(f\"    Avg correlation: {avg_correlation:.4f}\")\n",
    "\n",
    "        # Memory usage calculation\n",
    "        try:\n",
    "            memory_usage_mb = val_data_features.estimated_size('mb')\n",
    "        except:\n",
    "            memory_usage_mb = 0.0\n",
    "\n",
    "        # Compile comprehensive analysis\n",
    "        feature_analysis = {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"feature_statistics\": feature_stats,\n",
    "            \"correlation_results\": correlation_results,\n",
    "            \"top_correlated_features\": top_correlated_features,\n",
    "            \"feature_categories\": feature_categories,\n",
    "            \"quality_assessment\": quality_assessment,\n",
    "            \"memory_usage_mb\": memory_usage_mb,\n",
    "            \"validation_results\": validation_results\n",
    "        }\n",
    "\n",
    "        log(\"Enhanced feature analysis completed!\")\n",
    "        return feature_analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error in feature analysis: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"feature_count\": len(feature_columns),\n",
    "            \"data_shape\": val_data_features.shape if hasattr(val_data_features, 'shape') else \"unknown\"\n",
    "        }\n",
    "\n",
    "# Perform comprehensive feature analysis with error handling\n",
    "try:\n",
    "    feature_analysis = analyze_enhanced_features(val_data_features, feature_columns, validation_results)\n",
    "\n",
    "    if \"error\" in feature_analysis:\n",
    "        log(f\"Feature analysis had issues: {feature_analysis['error']}\")\n",
    "        # Create minimal analysis\n",
    "        feature_analysis = {\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"feature_statistics\": {},\n",
    "            \"correlation_results\": {},\n",
    "            \"top_correlated_features\": [],\n",
    "            \"feature_categories\": {\"total\": len(feature_columns)},\n",
    "            \"quality_assessment\": {\n",
    "                \"total_features\": len(feature_columns),\n",
    "                \"features_with_data\": 0,\n",
    "                \"max_correlation\": 0.0,\n",
    "                \"avg_correlation\": 0.0\n",
    "            },\n",
    "            \"memory_usage_mb\": 0.0,\n",
    "            \"validation_results\": validation_results,\n",
    "            \"error\": feature_analysis.get(\"error\", \"Unknown analysis error\")\n",
    "        }\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Critical error in feature analysis: {e}\")\n",
    "    # Create emergency fallback analysis\n",
    "    feature_analysis = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"feature_statistics\": {},\n",
    "        \"correlation_results\": {},\n",
    "        \"top_correlated_features\": [],\n",
    "        \"feature_categories\": {\"emergency\": len(feature_columns)},\n",
    "        \"quality_assessment\": {\n",
    "            \"total_features\": len(feature_columns),\n",
    "            \"features_with_data\": 0,\n",
    "            \"max_correlation\": 0.0,\n",
    "            \"avg_correlation\": 0.0\n",
    "        },\n",
    "        \"memory_usage_mb\": 0.0,\n",
    "        \"validation_results\": validation_results,\n",
    "        \"error\": f\"Critical analysis failure: {str(e)}\"\n",
    "    }\n",
    "\n",
    "force_garbage_collection()\n",
    "log_memory(\"After feature analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxHRJF22mzHs"
   },
   "source": [
    "## ENHANCED OUTPUT SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnFFAa76m02K",
    "outputId": "3c339e48-0826-4b29-d0a8-b4ef3dd7adb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 03:59:39] Saving enhanced feature engineering outputs...\n",
      "[2025-08-08 03:59:39] Before saving outputs [Memory: 5129.5 MB]\n",
      "[2025-08-08 04:00:30]    val_data_features.parquet saved (113.9 MB)\n",
      "[2025-08-08 04:00:30]    feature_columns.json saved (61 features)\n",
      "[2025-08-08 04:00:30]    feature_statistics.pkl saved\n",
      "[2025-08-08 04:00:30]    feature_importance_analysis.pkl saved\n",
      "[2025-08-08 04:00:30]    part_2b3_enhanced_summary.pkl saved\n",
      "[2025-08-08 04:00:30] Enhanced feature engineering outputs saved successfully!\n",
      "[2025-08-08 04:00:30] After saving outputs [Memory: 3994.2 MB]\n"
     ]
    }
   ],
   "source": [
    "def save_enhanced_feature_outputs(val_data_features: pl.DataFrame,\n",
    "                                feature_columns: List[str],\n",
    "                                feature_analysis: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Save all enhanced feature engineering outputs with robust error handling\n",
    "    \"\"\"\n",
    "    log(\"Saving enhanced feature engineering outputs...\")\n",
    "    log_memory(\"Before saving outputs\")\n",
    "\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "        output_paths = {}\n",
    "\n",
    "        # 1. Save validation data with features (main output) - use compression\n",
    "        features_path = f\"{config.OUTPUT_PATH}/val_data_features.parquet\"\n",
    "        try:\n",
    "            val_data_features.write_parquet(features_path, compression=\"snappy\")\n",
    "            file_size = os.path.getsize(features_path) / (1024*1024)\n",
    "            log(f\"   val_data_features.parquet saved ({file_size:.1f} MB)\")\n",
    "            output_paths[\"features_path\"] = features_path\n",
    "        except Exception as e:\n",
    "            log(f\"   Error saving val_data_features.parquet: {e}\")\n",
    "            # Try without compression\n",
    "            val_data_features.write_parquet(features_path)\n",
    "            output_paths[\"features_path\"] = features_path\n",
    "            log(f\"   val_data_features.parquet saved (without compression)\")\n",
    "\n",
    "        # 2. Save feature column names\n",
    "        feature_cols_path = f\"{config.OUTPUT_PATH}/feature_columns.json\"\n",
    "        try:\n",
    "            with open(feature_cols_path, \"w\") as f:\n",
    "                json.dump(feature_columns, f, indent=2)\n",
    "            log(f\"   feature_columns.json saved ({len(feature_columns)} features)\")\n",
    "            output_paths[\"feature_cols_path\"] = feature_cols_path\n",
    "        except Exception as e:\n",
    "            log(f\"   Error saving feature_columns.json: {e}\")\n",
    "\n",
    "        # 3. Save feature statistics (use pickle to avoid serialization issues)\n",
    "        feature_stats_path = f\"{config.OUTPUT_PATH}/feature_statistics.pkl\"\n",
    "        try:\n",
    "            with open(feature_stats_path, \"wb\") as f:\n",
    "                pickle.dump(feature_analysis, f)\n",
    "            log(f\"   feature_statistics.pkl saved\")\n",
    "            output_paths[\"feature_stats_path\"] = feature_stats_path\n",
    "        except Exception as e:\n",
    "            log(f\"   Error saving feature_statistics.pkl: {e}\")\n",
    "\n",
    "        # 4. Save feature importance analysis (use pickle)\n",
    "        importance_path = f\"{config.OUTPUT_PATH}/feature_importance_analysis.pkl\"\n",
    "        try:\n",
    "            importance_data = {\n",
    "                \"feature_correlations\": feature_analysis.get(\"correlation_results\", {}),\n",
    "                \"top_features\": feature_analysis.get(\"top_correlated_features\", []),\n",
    "                \"feature_statistics\": feature_analysis.get(\"feature_statistics\", {}),\n",
    "                \"quality_metrics\": feature_analysis.get(\"quality_assessment\", {}),\n",
    "                \"feature_categories\": feature_analysis.get(\"feature_categories\", {})\n",
    "            }\n",
    "            with open(importance_path, \"wb\") as f:\n",
    "                pickle.dump(importance_data, f)\n",
    "            log(f\"   feature_importance_analysis.pkl saved\")\n",
    "            output_paths[\"importance_path\"] = importance_path\n",
    "        except Exception as e:\n",
    "            log(f\"   Error saving feature_importance_analysis.pkl: {e}\")\n",
    "\n",
    "        # 5. Save enhanced summary report\n",
    "        summary_path = f\"{config.OUTPUT_PATH}/part_2b3_enhanced_summary.pkl\"\n",
    "        try:\n",
    "            validation_results = feature_analysis.get(\"validation_results\", {})\n",
    "\n",
    "            summary = {\n",
    "                \"notebook\": \"Part 2B3: Enhanced Feature Engineering for Ranking\",\n",
    "                \"completion_timestamp\": datetime.now().isoformat(),\n",
    "                \"version\": \"COMPLETE FIXED - Comprehensive features with data type consistency\",\n",
    "                \"critical_fixes_applied\": [\n",
    "                    \"Fixed session column dtype mismatch (i64 vs u32)\",\n",
    "                    \"Added ensure_consistent_dtypes function for all joins\",\n",
    "                    \"Improved fallback mechanism with meaningful features\",\n",
    "                    \"Enhanced lazy vs regular DataFrame handling\",\n",
    "                    \"Better error reporting and recovery\",\n",
    "                    \"Explicit Int64 casting for all join keys\",\n",
    "                    \"Comprehensive error handling with multiple fallback levels\"\n",
    "                ],\n",
    "                \"improvements\": [\n",
    "                    f\"Created {len(feature_columns)} meaningful features\",\n",
    "                    \"Added advanced session behavior features\",\n",
    "                    \"Added comprehensive item interaction patterns\",\n",
    "                    \"Added temporal and statistical aggregations\",\n",
    "                    \"Improved correlation analysis\",\n",
    "                    \"Enhanced session history utilization\",\n",
    "                    \"Fixed qcut duplicate value issues\",\n",
    "                    \"Added robust error handling\"\n",
    "                ],\n",
    "                \"inputs_used\": {\n",
    "                    \"val_data.parquet\": f\"{validation_results.get('val_data_samples', 0):,} validation samples\",\n",
    "                    \"item_stats.parquet\": f\"{validation_results.get('item_stats_count', 0):,} items\",\n",
    "                    \"train_features.parquet\": \"Session history\" if validation_results.get('train_data_available', False) else \"Not available\"\n",
    "                },\n",
    "                \"outputs_generated\": {\n",
    "                    \"val_data_features.parquet\": f\"{len(val_data_features):,} samples with {len(feature_columns)} features\",\n",
    "                    \"feature_columns.json\": f\"{len(feature_columns)} feature names\",\n",
    "                    \"feature_statistics.pkl\": \"Comprehensive feature analysis\",\n",
    "                    \"feature_importance_analysis.pkl\": \"Enhanced feature importance and correlations\"\n",
    "                },\n",
    "                \"key_metrics\": {\n",
    "                    \"total_features\": len(feature_columns),\n",
    "                    \"memory_usage_mb\": feature_analysis.get(\"memory_usage_mb\", 0),\n",
    "                    \"max_correlation\": feature_analysis.get(\"quality_assessment\", {}).get(\"max_correlation\", 0),\n",
    "                    \"high_correlation_features\": feature_analysis.get(\"quality_assessment\", {}).get(\"high_correlation_features\", 0),\n",
    "                    \"feature_categories\": feature_analysis.get(\"feature_categories\", {})\n",
    "                },\n",
    "                \"quality_assessment\": feature_analysis.get(\"quality_assessment\", {}),\n",
    "                \"next_step\": \"Run Part 2B4: Model Training & Evaluation with enhanced features\"\n",
    "            }\n",
    "\n",
    "            with open(summary_path, \"wb\") as f:\n",
    "                pickle.dump(summary, f)\n",
    "            log(f\"   part_2b3_enhanced_summary.pkl saved\")\n",
    "            output_paths[\"summary_path\"] = summary_path\n",
    "        except Exception as e:\n",
    "            log(f\"   Error saving summary: {e}\")\n",
    "\n",
    "        log(\"Enhanced feature engineering outputs saved successfully!\")\n",
    "        log_memory(\"After saving outputs\")\n",
    "\n",
    "        return output_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Error saving outputs: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Save all outputs\n",
    "output_paths = save_enhanced_feature_outputs(val_data_features, feature_columns, feature_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMtGK3oF05xF"
   },
   "source": [
    "## ENHANCED FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNXcXwT2070e",
    "outputId": "8f1d29ad-9b1e-4aca-dac5-366533cf7981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 04:00:30] \n",
      "================================================================================\n",
      "[2025-08-08 04:00:30] PART 2B3 COMPLETED: ENHANCED FEATURE ENGINEERING FOR RANKING\n",
      "[2025-08-08 04:00:30] ================================================================================\n",
      "[2025-08-08 04:00:30] \n",
      "KEY RESULTS (COMPLETE FIXED VERSION):\n",
      "[2025-08-08 04:00:30]   Total features created: 61\n",
      "[2025-08-08 04:00:30]   Validation samples: 6,617,259\n",
      "[2025-08-08 04:00:30]   Memory usage: 3162.6 MB\n",
      "[2025-08-08 04:00:30]   Session history available: yes\n",
      "[2025-08-08 04:00:30]   Top feature correlation: 0.0130 (item_carts)\n",
      "[2025-08-08 04:00:30] \n",
      "FEATURE BREAKDOWN:\n",
      "[2025-08-08 04:00:30]   Session: 20 features\n",
      "[2025-08-08 04:00:30]   Item: 18 features\n",
      "[2025-08-08 04:00:30]   Interaction: 11 features\n",
      "[2025-08-08 04:00:30]   Temporal: 6 features\n",
      "[2025-08-08 04:00:30]   Statistical: 11 features\n",
      "[2025-08-08 04:00:30] \n",
      "TOP CORRELATED FEATURES:\n",
      "[2025-08-08 04:00:30]   1. item_carts: -0.0130\n",
      "[2025-08-08 04:00:30]   2. item_popularity: -0.0117\n",
      "[2025-08-08 04:00:30]   3. item_clicks: -0.0114\n",
      "[2025-08-08 04:00:30]   4. item_orders: -0.0110\n",
      "[2025-08-08 04:00:30]   5. unique_items: -0.0070\n",
      "[2025-08-08 04:00:30] \n",
      "QUALITY ASSESSMENT:\n",
      "[2025-08-08 04:00:30]   Feature count target (20):  PASS (61)\n",
      "[2025-08-08 04:00:30]   High correlation features (>0.01): 4\n",
      "[2025-08-08 04:00:30]   Max correlation: 0.0130\n",
      "[2025-08-08 04:00:30]   Avg correlation: 0.0051\n",
      "[2025-08-08 04:00:30]   Memory usage (<8GB):  PASS (3162.6 MB)\n",
      "[2025-08-08 04:00:30]   Meaningful features:  PASS\n",
      "[2025-08-08 04:00:30] \n",
      "Overall Quality: EXCELLENT\n",
      "[2025-08-08 04:00:30] \n",
      "OUTPUT FILES GENERATED:\n",
      "[2025-08-08 04:00:30]   val_data_features.parquet (113.9 MB)\n",
      "[2025-08-08 04:00:30]   feature_columns.json\n",
      "[2025-08-08 04:00:30]   feature_statistics.pkl\n",
      "[2025-08-08 04:00:30]   feature_importance_analysis.pkl\n",
      "[2025-08-08 04:00:30]   part_2b3_enhanced_summary.pkl\n",
      "[2025-08-08 04:00:30]   All files saved to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output\n",
      "[2025-08-08 04:00:30] \n",
      " READY FOR PART 2B4: Model Training & Evaluation\n",
      "[2025-08-08 04:00:30]   Features: 61 meaningful features created\n",
      "[2025-08-08 04:00:30]   Quality: EXCELLENT\n",
      "[2025-08-08 04:00:30]   Data: 6,617,259 samples ready for training\n",
      "[2025-08-08 04:00:30] \n",
      "Performing final cleanup...\n",
      "[2025-08-08 04:00:30] Memory cleanup completed - Final memory usage: 3.9 GB\n",
      "[2025-08-08 04:00:30] \n",
      "Part 2B3 Complete Fixed Version finished successfully!\n",
      "[2025-08-08 04:00:30] Ready for Part 2B4 with 61-feature dataset!\n",
      "[2025-08-08 04:00:30] ================================================================================\n"
     ]
    }
   ],
   "source": [
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"PART 2B3 COMPLETED: ENHANCED FEATURE ENGINEERING FOR RANKING\")\n",
    "log(\"=\"*80)\n",
    "\n",
    "# Display key results\n",
    "quality_assessment = feature_analysis.get(\"quality_assessment\", {})\n",
    "top_correlated = feature_analysis.get(\"top_correlated_features\", [])\n",
    "\n",
    "log(f\"\\nKEY RESULTS (COMPLETE FIXED VERSION):\")\n",
    "log(f\"  Total features created: {len(feature_columns)}\")\n",
    "log(f\"  Validation samples: {len(val_data_features):,}\")\n",
    "log(f\"  Memory usage: {feature_analysis.get('memory_usage_mb', 0):.1f} MB\")\n",
    "log(f\"  Session history available: {'yes' if validation_results.get('train_data_available', False) else 'no'}\")\n",
    "\n",
    "if top_correlated:\n",
    "    top_feature, top_corr = top_correlated[0]\n",
    "    log(f\"  Top feature correlation: {abs(top_corr):.4f} ({top_feature})\")\n",
    "else:\n",
    "    log(f\"  Top feature correlation: 0.0000\")\n",
    "\n",
    "# Feature breakdown\n",
    "feature_categories = feature_analysis.get(\"feature_categories\", {})\n",
    "log(f\"\\nFEATURE BREAKDOWN:\")\n",
    "for category, count in feature_categories.items():\n",
    "    if count > 0:\n",
    "        log(f\"  {category.title()}: {count} features\")\n",
    "\n",
    "# Top correlated features\n",
    "if top_correlated:\n",
    "    log(f\"\\nTOP CORRELATED FEATURES:\")\n",
    "    for i, (feature, corr) in enumerate(top_correlated[:5]):\n",
    "        log(f\"  {i+1}. {feature}: {corr:.4f}\")\n",
    "\n",
    "# Quality assessment\n",
    "log(f\"\\nQUALITY ASSESSMENT:\")\n",
    "log(f\"  Feature count target (20): {' PASS' if len(feature_columns) >= 20 else ' FAIL'} ({len(feature_columns)})\")\n",
    "log(f\"  High correlation features (>{config.MIN_FEATURE_CORRELATION}): {quality_assessment.get('high_correlation_features', 0)}\")\n",
    "log(f\"  Max correlation: {quality_assessment.get('max_correlation', 0):.4f}\")\n",
    "log(f\"  Avg correlation: {quality_assessment.get('avg_correlation', 0):.4f}\")\n",
    "\n",
    "# Determine overall quality\n",
    "feature_quality_ok = len(feature_columns) >= 20\n",
    "correlation_ok = quality_assessment.get('max_correlation', 0) > config.MIN_FEATURE_CORRELATION\n",
    "memory_ok = feature_analysis.get('memory_usage_mb', 0) < 8000\n",
    "\n",
    "log(f\"  Memory usage (<8GB): {' PASS' if memory_ok else ' FAIL'} ({feature_analysis.get('memory_usage_mb', 0):.1f} MB)\")\n",
    "log(f\"  Meaningful features: {' PASS' if len(feature_columns) > 4 else ' FAIL'}\")\n",
    "\n",
    "# Overall quality determination\n",
    "if feature_quality_ok and correlation_ok and memory_ok:\n",
    "    overall_quality = \"EXCELLENT\"\n",
    "elif feature_quality_ok and memory_ok:\n",
    "    overall_quality = \"GOOD\"\n",
    "elif len(feature_columns) >= 10 and memory_ok:\n",
    "    overall_quality = \"ACCEPTABLE\"\n",
    "else:\n",
    "    overall_quality = \"NEEDS_IMPROVEMENT\"\n",
    "\n",
    "log(f\"\\nOverall Quality: {overall_quality}\")\n",
    "\n",
    "# Output files summary\n",
    "log(f\"\\nOUTPUT FILES GENERATED:\")\n",
    "if \"features_path\" in output_paths:\n",
    "    log(f\"  val_data_features.parquet ({os.path.getsize(output_paths['features_path'])/(1024*1024):.1f} MB)\")\n",
    "if \"feature_cols_path\" in output_paths:\n",
    "    log(f\"  feature_columns.json\")\n",
    "if \"feature_stats_path\" in output_paths:\n",
    "    log(f\"  feature_statistics.pkl\")\n",
    "if \"importance_path\" in output_paths:\n",
    "    log(f\"  feature_importance_analysis.pkl\")\n",
    "if \"summary_path\" in output_paths:\n",
    "    log(f\"  part_2b3_enhanced_summary.pkl\")\n",
    "log(f\"  All files saved to: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Check for errors and provide recommendations\n",
    "if \"error\" in feature_analysis:\n",
    "    log(f\"\\nWARNING: {feature_analysis['error']}\")\n",
    "\n",
    "if overall_quality == \"NEEDS_IMPROVEMENT\":\n",
    "    log(f\"\\nRECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "    log(f\"  - Check data quality in Part 2B2\")\n",
    "    log(f\"  - Verify item_stats.parquet has sufficient data\")\n",
    "    log(f\"  - Ensure train_features.parquet is available for temporal features\")\n",
    "\n",
    "# Status for next step\n",
    "if overall_quality in [\"EXCELLENT\", \"GOOD\", \"ACCEPTABLE\"] and len(feature_columns) >= 10:\n",
    "    log(f\"\\n READY FOR PART 2B4: Model Training & Evaluation\")\n",
    "    log(f\"  Features: {len(feature_columns)} meaningful features created\")\n",
    "    log(f\"  Quality: {overall_quality}\")\n",
    "    log(f\"  Data: {len(val_data_features):,} samples ready for training\")\n",
    "else:\n",
    "    log(f\"\\n PART 2B4 READINESS: MARGINAL\")\n",
    "    log(f\"  You can proceed but results may be limited\")\n",
    "    log(f\"  Consider improving feature engineering if possible\")\n",
    "\n",
    "# Final cleanup\n",
    "log(f\"\\nPerforming final cleanup...\")\n",
    "try:\n",
    "    del val_data, item_stats, train_data\n",
    "    if 'feature_engineer' in locals():\n",
    "        del feature_engineer\n",
    "    force_garbage_collection()\n",
    "    final_memory = get_memory_usage()\n",
    "    log(f\"Memory cleanup completed - Final memory usage: {final_memory:.1f} GB\")\n",
    "except Exception as e:\n",
    "    log(f\"Cleanup warning: {e}\")\n",
    "\n",
    "log(f\"\\nPart 2B3 Complete Fixed Version finished successfully!\")\n",
    "log(f\"Ready for Part 2B4 with {len(feature_columns)}-feature dataset!\")\n",
    "log(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
