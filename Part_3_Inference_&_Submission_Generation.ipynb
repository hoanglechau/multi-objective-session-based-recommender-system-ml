{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Inference & Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALW7NeXJBSZ3",
    "outputId": "eac8f5b4-6770-462e-a48e-4a46bc0dd7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.16.1)\n",
      "Requirement already satisfied: polars==0.20.31 in /usr/local/lib/python3.11/dist-packages (0.20.31)\n",
      "Part 3: Optimized Inference & Submission Generation\n",
      "All required packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install lightgbm\n",
    "!pip install polars==0.20.31\n",
    "\n",
    "# Core imports\n",
    "import subprocess\n",
    "import sys\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Booster\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory optimization settings\n",
    "pl.Config.set_tbl_rows(10)\n",
    "pl.Config.set_tbl_cols(8)\n",
    "\n",
    "print(\"Part 3: Optimized Inference & Submission Generation\")\n",
    "print(\"All required packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-Z2Pm4oBfPB",
    "outputId": "c7ffd7d0-d0dd-4ba9-99db-2cfdf420da99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMRRHJSNCSqs"
   },
   "source": [
    "## CONFIGURATION AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sz0IWVqrCTU1"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration settings optimized for a High-RAM (A100) environment\"\"\"\n",
    "\n",
    "    # --- Paths ---\n",
    "    # UPDATE THESE TO MATCH YOUR GOOGLE DRIVE SETUP\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content\"\n",
    "    DATA_PATH = f\"{BASE_PATH}/otto-data\"\n",
    "    OUTPUT_PATH = f\"{BASE_PATH}/otto-output\"\n",
    "\n",
    "    # --- High-Performance Settings ---\n",
    "    # Increase batch sizes to leverage high RAM for faster processing.\n",
    "    # The original values (e.g., 10000) are too small for your hardware.\n",
    "    CHUNK_SIZE = 5_000_000\n",
    "    PREDICTION_BATCH_SIZE = 1_000_000\n",
    "    FEATURE_BATCH_SIZE = 5_000_000\n",
    "\n",
    "    # --- Memory Management ---\n",
    "    # Relaxed thresholds suitable for an 83.5 GB RAM environment.\n",
    "    MAX_MEMORY_GB = 75.0\n",
    "    CRITICAL_MEMORY_THRESHOLD = 90.0\n",
    "\n",
    "    # --- Garbage Collection ---\n",
    "    # Reduce the frequency of garbage collection. Frequent collection is slow.\n",
    "    GC_FREQUENCY = 50\n",
    "    FORCE_GC_FREQUENCY = 20 # Only force cleanup after many chunks.\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "    # Use a larger set of features.\n",
    "    MAX_FEATURES_TO_USE = 200\n",
    "\n",
    "    # --- Prediction Settings ---\n",
    "    TOP_K_PREDICTIONS = 20\n",
    "    FALLBACK_POPULAR_ITEMS = 50\n",
    "\n",
    "    # --- Validation & Output ---\n",
    "    REQUIRED_EVENT_TYPES = [\"clicks\", \"carts\", \"orders\"]\n",
    "    SUBMISSION_COLUMNS = [\"session_type\", \"labels\"]\n",
    "    SAVE_INTERMEDIATE_RESULTS = False # Keep this off unless debugging.\n",
    "\n",
    "    # --- Memory Monitoring ---\n",
    "    # Check memory less frequently.\n",
    "    MEMORY_CHECK_FREQUENCY = 100_000\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dUvjxgrCX0Z"
   },
   "source": [
    "## UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iiqgTRKbCZvE"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import polars as pl\n",
    "\n",
    "def get_memory_usage() -> float:\n",
    "    \"\"\"Get current memory usage in GB.\"\"\"\n",
    "    return psutil.virtual_memory().used / (1024**3)\n",
    "\n",
    "def get_memory_percent() -> float:\n",
    "    \"\"\"Get current memory usage percentage.\"\"\"\n",
    "    return psutil.virtual_memory().percent\n",
    "\n",
    "def log_with_memory(message: str, level: str = \"INFO\"):\n",
    "    \"\"\"Log message with current memory usage.\"\"\"\n",
    "    memory_gb = get_memory_usage()\n",
    "    memory_pct = get_memory_percent()\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [{memory_gb:.1f}GB/{memory_pct:.1f}%] [{level}] {message}\")\n",
    "\n",
    "def simplified_garbage_collection():\n",
    "    \"\"\"A single, effective garbage collection call.\"\"\"\n",
    "    gc.collect()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "def monitor_memory(operation_name: str, force_cleanup: bool = False):\n",
    "    \"\"\"\n",
    "    A simplified memory monitor. In a high-RAM environment, this primarily serves\n",
    "    as a logging tool to mark the end of a step. The cleanup logic is less critical.\n",
    "    \"\"\"\n",
    "    log_with_memory(f\"Completed operation: {operation_name}\")\n",
    "    if force_cleanup:\n",
    "        log_with_memory(\"Performing simplified garbage collection...\")\n",
    "        simplified_garbage_collection()\n",
    "\n",
    "def safe_file_size(filepath: str) -> float:\n",
    "    \"\"\"Get file size in MB safely, returning 0.0 on error.\"\"\"\n",
    "    try:\n",
    "        return os.path.getsize(filepath) / (1024 * 1024)\n",
    "    except OSError:\n",
    "        return 0.0\n",
    "\n",
    "def safe_delete(*objects):\n",
    "    \"\"\"Safely delete objects and free memory.\"\"\"\n",
    "    for obj in objects:\n",
    "        try:\n",
    "            if obj is not None:\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "    simplified_garbage_collection()\n",
    "\n",
    "def ensure_consistent_dtypes(df1: pl.DataFrame, df2: pl.DataFrame, join_keys: List[str]) -> tuple:\n",
    "    \"\"\"\n",
    "    Ensure joining keys have the same data type.\n",
    "    This version is simplified for clarity and speed.\n",
    "    \"\"\"\n",
    "    for key in join_keys:\n",
    "        if key in df1.columns and key in df2.columns:\n",
    "            dtype1 = df1.schema[key]\n",
    "            dtype2 = df2.schema[key]\n",
    "            if dtype1 != dtype2:\n",
    "                log_with_memory(f\"Fixing dtype mismatch for '{key}': {dtype1} vs {dtype2}\", \"WARNING\")\n",
    "                # Cast both to Int64, the most common type for IDs.\n",
    "                df1 = df1.with_columns(pl.col(key).cast(pl.Int64))\n",
    "                df2 = df2.with_columns(pl.col(key).cast(pl.Int64))\n",
    "    return df1, df2\n",
    "\n",
    "def chunk_dataframe(df: pl.DataFrame, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Generator for DataFrame chunking without excessive memory checks.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    for start_idx in range(0, total_rows, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, total_rows)\n",
    "        yield df.slice(start_idx, end_idx - start_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDmH1eeYCcFf"
   },
   "source": [
    "## INPUT VALIDATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oUUksejlCdzG"
   },
   "outputs": [],
   "source": [
    "def validate_input_files() -> Dict[str, bool]:\n",
    "    \"\"\"Validate that all required input files exist\"\"\"\n",
    "    log_with_memory(\"Validating input files...\")\n",
    "\n",
    "    required_files = {\n",
    "        \"test_candidates.parquet\": \"Test candidates from Part 2B1\",\n",
    "        \"test_clean.parquet\": \"Clean test data from Part 1\",\n",
    "        \"item_stats.parquet\": \"Item statistics from Part 1\",\n",
    "        \"feature_columns.json\": \"Feature columns from Part 2B3\"\n",
    "    }\n",
    "\n",
    "    model_files = {\n",
    "        \"ranker_clicks.txt\": \"Clicks ranking model from Part 2B4\",\n",
    "        \"ranker_carts.txt\": \"Carts ranking model from Part 2B4\",\n",
    "        \"ranker_orders.txt\": \"Orders ranking model from Part 2B4\"\n",
    "    }\n",
    "\n",
    "    optional_files = {\n",
    "        \"evaluation_results.json\": \"Model performance metrics from Part 2B4\"\n",
    "    }\n",
    "\n",
    "    validation_results = {}\n",
    "    missing_files = []\n",
    "\n",
    "    # Check required files\n",
    "    for filename, description in required_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        exists = os.path.exists(filepath)\n",
    "        validation_results[filename] = exists\n",
    "\n",
    "        if exists:\n",
    "            file_size = safe_file_size(filepath)\n",
    "            log_with_memory(f\"Found: {filename} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            missing_files.append(f\"{filename} - {description}\")\n",
    "\n",
    "    # Check model files\n",
    "    available_models = []\n",
    "    for filename, description in model_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        exists = os.path.exists(filepath)\n",
    "        validation_results[filename] = exists\n",
    "\n",
    "        if exists:\n",
    "            file_size = safe_file_size(filepath)\n",
    "            log_with_memory(f\"Found: {filename} ({file_size:.1f} MB)\")\n",
    "            model_name = filename.replace(\"ranker_\", \"\").replace(\".txt\", \"\")\n",
    "            available_models.append(model_name)\n",
    "        else:\n",
    "            log_with_memory(f\"Missing: {filename} - {description}\", \"WARNING\")\n",
    "\n",
    "    # Check optional files\n",
    "    for filename, description in optional_files.items():\n",
    "        filepath = f\"{config.OUTPUT_PATH}/{filename}\"\n",
    "        exists = os.path.exists(filepath)\n",
    "        validation_results[filename] = exists\n",
    "\n",
    "        if exists:\n",
    "            file_size = safe_file_size(filepath)\n",
    "            log_with_memory(f\"Found: {filename} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            log_with_memory(f\"Not found: {filename} (optional)\", \"INFO\")\n",
    "\n",
    "    # Report validation results\n",
    "    if missing_files:\n",
    "        log_with_memory(\"MISSING REQUIRED FILES:\", \"ERROR\")\n",
    "        for missing in missing_files:\n",
    "            log_with_memory(f\"  {missing}\", \"ERROR\")\n",
    "        log_with_memory(\"\", \"ERROR\")\n",
    "        log_with_memory(\"TO FIX THIS:\", \"ERROR\")\n",
    "        log_with_memory(\"  1. Run Part 1 (Data Processing) to generate test data and item stats\", \"ERROR\")\n",
    "        log_with_memory(\"  2. Run Part 2B1 (Test Candidate Generation) to generate test_candidates.parquet\", \"ERROR\")\n",
    "        log_with_memory(\"  3. Run Part 2B3 (Feature Engineering) to generate feature_columns.json\", \"ERROR\")\n",
    "        log_with_memory(\"  4. Run Part 2B4 (Model Training) to generate ranking models\", \"ERROR\")\n",
    "        raise FileNotFoundError(\"Required input files are missing!\")\n",
    "\n",
    "    if not available_models:\n",
    "        log_with_memory(\"NO RANKING MODELS FOUND!\", \"ERROR\")\n",
    "        log_with_memory(\"Cannot proceed without at least one trained model\", \"ERROR\")\n",
    "        raise FileNotFoundError(\"No ranking models available!\")\n",
    "\n",
    "    log_with_memory(f\"Input validation passed! Available models: {available_models}\")\n",
    "    validation_results[\"available_models\"] = available_models\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "def load_all_inputs(validation_results: Dict) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, List[str], Dict[str, Booster], Dict]:\n",
    "    \"\"\"Load all required inputs efficiently\"\"\"\n",
    "    log_with_memory(\"Loading all inputs...\")\n",
    "\n",
    "    # Load test candidates with explicit casting\n",
    "    log_with_memory(\"Loading test candidates...\")\n",
    "    test_candidates = pl.read_parquet(f\"{config.OUTPUT_PATH}/test_candidates.parquet\")\n",
    "    test_candidates = test_candidates.with_columns([\n",
    "        pl.col(\"session\").cast(pl.Int64),\n",
    "        pl.col(\"aid\").cast(pl.Int64)\n",
    "    ])\n",
    "    log_with_memory(f\"Test candidates: {test_candidates.shape}\")\n",
    "\n",
    "    # Load clean test data with explicit casting\n",
    "    log_with_memory(\"Loading clean test data...\")\n",
    "    test_df = pl.read_parquet(f\"{config.OUTPUT_PATH}/test_clean.parquet\")\n",
    "    test_df = test_df.with_columns([\n",
    "        pl.col(\"session\").cast(pl.Int64),\n",
    "        pl.col(\"aid\").cast(pl.Int64)\n",
    "    ])\n",
    "    log_with_memory(f\"Test data: {test_df.shape}\")\n",
    "\n",
    "    # Load item statistics with explicit casting\n",
    "    log_with_memory(\"Loading item statistics...\")\n",
    "    item_stats = pl.read_parquet(f\"{config.OUTPUT_PATH}/item_stats.parquet\")\n",
    "    if \"aid\" in item_stats.columns:\n",
    "        item_stats = item_stats.with_columns(pl.col(\"aid\").cast(pl.Int64))\n",
    "    log_with_memory(f\"Item stats: {item_stats.shape}\")\n",
    "\n",
    "    # Load feature columns\n",
    "    log_with_memory(\"Loading feature columns...\")\n",
    "    with open(f\"{config.OUTPUT_PATH}/feature_columns.json\", \"r\") as f:\n",
    "        feature_columns = json.load(f)\n",
    "    log_with_memory(f\"Feature columns: {len(feature_columns)} features\")\n",
    "\n",
    "    # Load trained models\n",
    "    log_with_memory(\"Loading trained ranking models...\")\n",
    "    ranking_models = {}\n",
    "    for event_type in config.REQUIRED_EVENT_TYPES:\n",
    "        model_path = f\"{config.OUTPUT_PATH}/ranker_{event_type}.txt\"\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                model = Booster(model_file=model_path)\n",
    "                ranking_models[event_type] = model\n",
    "                log_with_memory(f\"Loaded {event_type} model successfully\")\n",
    "            except Exception as e:\n",
    "                log_with_memory(f\"Failed to load {event_type} model: {e}\", \"ERROR\")\n",
    "        else:\n",
    "            log_with_memory(f\"No model found for {event_type}\", \"WARNING\")\n",
    "\n",
    "    if not ranking_models:\n",
    "        raise ValueError(\"No valid ranking models loaded!\")\n",
    "\n",
    "    # Load evaluation results (optional)\n",
    "    evaluation_results = {}\n",
    "    try:\n",
    "        with open(f\"{config.OUTPUT_PATH}/evaluation_results.json\", \"r\") as f:\n",
    "            evaluation_results = json.load(f)\n",
    "        log_with_memory(f\"Loaded evaluation results: {evaluation_results.get('weighted_average', 'N/A')}\")\n",
    "    except:\n",
    "        log_with_memory(\"No evaluation results found (optional)\", \"WARNING\")\n",
    "        evaluation_results = {\"weighted_average\": 0.0}\n",
    "\n",
    "    log_with_memory(\"All inputs loaded successfully!\")\n",
    "    simplified_garbage_collection()\n",
    "\n",
    "    return test_candidates, test_df, item_stats, feature_columns, ranking_models, evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1x02AtiChdg"
   },
   "source": [
    "## FEATURE ENGINEERING FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9rDP5hYECjFC"
   },
   "outputs": [],
   "source": [
    "def create_test_ranking_features(test_candidates: pl.DataFrame,\n",
    "                                 test_df: pl.DataFrame,\n",
    "                                 item_stats: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    High-performance feature creation using vectorized operations.\n",
    "    This version avoids slow, iterative processing and is suitable for high-RAM machines.\n",
    "    \"\"\"\n",
    "    log_with_memory(\"Starting high-performance feature engineering...\")\n",
    "\n",
    "    # STEP 1: Compute session-level features in a single pass.\n",
    "    log_with_memory(\"Computing session-level features...\")\n",
    "    session_features = test_df.group_by(\"session\").agg([\n",
    "        pl.col(\"aid\").count().alias(\"session_length\"),\n",
    "        pl.col(\"aid\").n_unique().alias(\"unique_items_in_session\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"num_clicks_in_session\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"carts\").count().alias(\"num_carts_in_session\"),\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"orders\").count().alias(\"num_orders_in_session\"),\n",
    "        (pl.col(\"ts\").max() - pl.col(\"ts\").min()).alias(\"session_duration_ms\")\n",
    "    ]).with_columns([\n",
    "        (pl.col(\"unique_items_in_session\").cast(pl.Float32) / pl.col(\"session_length\").clip(1)).alias(\"session_diversity\"),\n",
    "        (pl.col(\"session_duration_ms\") / 1000.0).alias(\"session_duration_s\")\n",
    "    ])\n",
    "\n",
    "    # STEP 2: Compute enhanced item features.\n",
    "    log_with_memory(\"Computing item-level features...\")\n",
    "    item_features = item_stats.with_columns([\n",
    "        (pl.col(\"carts\").cast(pl.Float32) / pl.col(\"clicks\").clip(1)).alias(\"item_cart_rate\"),\n",
    "        (pl.col(\"orders\").cast(pl.Float32) / pl.col(\"clicks\").clip(1)).alias(\"item_conversion_rate\"),\n",
    "        (pl.col(\"orders\").cast(pl.Float32) / pl.col(\"carts\").clip(1)).alias(\"item_buy_rate\"),\n",
    "        pl.col(\"clicks\").rank(\"dense\", descending=True).alias(\"clicks_rank\"),\n",
    "        pl.col(\"carts\").rank(\"dense\", descending=True).alias(\"carts_rank\"),\n",
    "        pl.col(\"orders\").rank(\"dense\", descending=True).alias(\"orders_rank\")\n",
    "    ])\n",
    "\n",
    "    # STEP 3: Compute session-item interaction features.\n",
    "    log_with_memory(\"Computing session-item interaction features...\")\n",
    "    session_item_features = test_df.group_by([\"session\", \"aid\"]).agg([\n",
    "        pl.col(\"type\").filter(pl.col(\"type\") == \"clicks\").count().alias(\"item_clicks_in_session\"),\n",
    "        pl.col(\"type\").count().alias(\"item_interactions_in_session\")\n",
    "    ])\n",
    "\n",
    "    # STEP 4: Join all features together.\n",
    "    log_with_memory(\"Joining all feature sets...\")\n",
    "    test_candidates, session_features = ensure_consistent_dtypes(test_candidates, session_features, [\"session\"])\n",
    "    test_features = test_candidates.join(session_features, on=\"session\", how=\"left\")\n",
    "\n",
    "    test_features, item_features = ensure_consistent_dtypes(test_features, item_features, [\"aid\"])\n",
    "    test_features = test_features.join(item_features, on=\"aid\", how=\"left\")\n",
    "\n",
    "    test_features, session_item_features = ensure_consistent_dtypes(test_features, session_item_features, [\"session\", \"aid\"])\n",
    "    test_features = test_features.join(session_item_features, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "    # STEP 5: Create final derived features and fill nulls.\n",
    "    log_with_memory(\"Creating final derived features and filling null values...\")\n",
    "\n",
    "    max_interactions = item_stats.select(pl.col(\"total_interactions\").max()).item() or 1\n",
    "\n",
    "    test_features = test_features.with_columns([\n",
    "        (pl.col(\"total_interactions\").cast(pl.Float32) / max_interactions).alias(\"item_popularity\"),\n",
    "        (pl.col(\"num_clicks_in_session\").cast(pl.Float32) / pl.col(\"session_length\").clip(1)).alias(\"session_click_rate\"),\n",
    "        pl.when(pl.col(\"type\") == \"clicks\").then(pl.col(\"clicks\"))\n",
    "            .when(pl.col(\"type\") == \"carts\").then(pl.col(\"carts\") * 6)\n",
    "            .otherwise(pl.col(\"orders\") * 3)\n",
    "            .alias(\"type_weighted_score\"),\n",
    "    ]).fill_null(0) # Simple and fast null filling\n",
    "\n",
    "    log_with_memory(f\"Feature engineering completed. Shape: {test_features.shape}\")\n",
    "    simplified_garbage_collection()\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p49N7vUPCmkb"
   },
   "source": [
    "## PREDICTION GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "17Pl3_jBCoFV"
   },
   "outputs": [],
   "source": [
    "def generate_all_predictions(test_features: pl.DataFrame,\n",
    "                             ranking_models: Dict[str, Booster],\n",
    "                             feature_columns: List[str],\n",
    "                             item_stats: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    High-performance prediction generation for all event types.\n",
    "    Processes each event type in a single, large batch.\n",
    "    \"\"\"\n",
    "    log_with_memory(\"Starting high-performance prediction generation...\")\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    # Get popular items for fallback, ensuring it's a list.\n",
    "    popular_items_fallback = item_stats.sort(\"orders\", descending=True).head(config.TOP_K_PREDICTIONS)[\"aid\"].to_list()\n",
    "\n",
    "    for event_type in config.REQUIRED_EVENT_TYPES:\n",
    "        log_with_memory(f\"Processing event type: {event_type}\")\n",
    "\n",
    "        if event_type not in ranking_models:\n",
    "            log_with_memory(f\"No model found for {event_type}. Skipping.\", \"WARNING\")\n",
    "            continue\n",
    "\n",
    "        model = ranking_models[event_type]\n",
    "\n",
    "        # Filter the data for the current event type\n",
    "        type_data = test_features.filter(pl.col(\"type\") == event_type)\n",
    "        if len(type_data) == 0:\n",
    "            log_with_memory(f\"No candidates found for {event_type}.\", \"INFO\")\n",
    "            continue\n",
    "\n",
    "        log_with_memory(f\"Predicting on {len(type_data):,} candidates for {event_type}...\")\n",
    "\n",
    "        # Ensure all required features are present, filling missing ones with 0\n",
    "        model_features = model.feature_name()\n",
    "\n",
    "        missing_cols = set(model_features) - set(type_data.columns)\n",
    "        if missing_cols:\n",
    "            log_with_memory(f\"Adding {len(missing_cols)} missing columns for prediction.\", \"WARNING\")\n",
    "            type_data = type_data.with_columns([pl.lit(0).alias(col) for col in missing_cols])\n",
    "\n",
    "        # Prepare data for prediction (as NumPy for speed)\n",
    "        X_test = type_data.select(model_features).to_numpy()\n",
    "\n",
    "        # Generate predictions\n",
    "        scores = model.predict(X_test)\n",
    "\n",
    "        # Add scores to the DataFrame\n",
    "        predictions_with_scores = type_data.with_columns(pl.Series(name=\"score\", values=scores))\n",
    "\n",
    "        # Rank and select top K predictions for each session\n",
    "        ranked_predictions = (\n",
    "            predictions_with_scores.sort(\"score\", descending=True)\n",
    "            .group_by(\"session\", maintain_order=True)\n",
    "            .agg(pl.col(\"aid\").head(config.TOP_K_PREDICTIONS))\n",
    "        )\n",
    "\n",
    "        # --- FIX IS HERE ---\n",
    "        # Format for submission. We must cast the integers in the list to strings before joining.\n",
    "        event_submission = ranked_predictions.with_columns([\n",
    "            pl.col(\"aid\").list.eval(pl.element().cast(pl.String)).list.join(\" \").alias(\"labels\"),\n",
    "            pl.lit(event_type).alias(\"type\")\n",
    "        ]).select([\"session\", \"type\", \"labels\"])\n",
    "\n",
    "        all_predictions.append(event_submission)\n",
    "\n",
    "        # Clean up memory\n",
    "        safe_delete(type_data, X_test, scores, predictions_with_scores, ranked_predictions)\n",
    "\n",
    "    log_with_memory(\"Combining predictions from all event types...\")\n",
    "    if not all_predictions:\n",
    "        raise ValueError(\"No predictions were generated. Check models and input data.\")\n",
    "\n",
    "    combined_predictions = pl.concat(all_predictions)\n",
    "\n",
    "    # --- Fallback for sessions with missing predictions ---\n",
    "    all_test_sessions = test_features.select(\"session\").unique()\n",
    "\n",
    "    # Check which session-type pairs are missing and add them with a popular item fallback\n",
    "    all_session_types = all_test_sessions.join(\n",
    "        pl.DataFrame({\"type\": config.REQUIRED_EVENT_TYPES}), how=\"cross\"\n",
    "    )\n",
    "\n",
    "    final_predictions = all_session_types.join(\n",
    "        combined_predictions, on=[\"session\", \"type\"], how=\"left\"\n",
    "    ).with_columns(\n",
    "        pl.col(\"labels\").fill_null(\" \".join(map(str, popular_items_fallback)))\n",
    "    )\n",
    "\n",
    "    log_with_memory(\"Prediction generation complete.\")\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jowm35udCruZ"
   },
   "source": [
    "## SUBMISSION FILE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GZzdkXPOCtaJ"
   },
   "outputs": [],
   "source": [
    "def create_submission_file(predictions: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the submission file in the required OTTO format using fast, vectorized operations.\n",
    "    \"\"\"\n",
    "    log_with_memory(\"Creating final submission file...\")\n",
    "\n",
    "    if \"labels\" not in predictions.columns or \"session\" not in predictions.columns:\n",
    "        raise ValueError(\"Predictions DataFrame is missing required columns 'session' or 'labels'.\")\n",
    "\n",
    "    # Vectorized creation of the 'session_type' column\n",
    "    submission_df = predictions.with_columns(\n",
    "        (pl.col(\"session\").cast(pl.String) + \"_\" + pl.col(\"type\")).alias(\"session_type\")\n",
    "    ).select([\"session_type\", \"labels\"])\n",
    "\n",
    "    # The 'labels' column should already be a space-separated string from the prediction step.\n",
    "    # The predictions are already sorted by session and type from the join.\n",
    "\n",
    "    log_with_memory(f\"Submission file created with {len(submission_df):,} rows.\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwbSOMynCvlf"
   },
   "source": [
    "## SUBMISSION VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4hDbGYd-CxMx"
   },
   "outputs": [],
   "source": [
    "def validate_submission_comprehensive(submission_df: pl.DataFrame,\n",
    "                                    test_df: pl.DataFrame) -> bool:\n",
    "    \"\"\"Comprehensive submission validation\"\"\"\n",
    "    log_with_memory(\"Performing comprehensive submission validation...\")\n",
    "\n",
    "    # Check required columns\n",
    "    required_cols = config.SUBMISSION_COLUMNS\n",
    "    if set(submission_df.columns) != set(required_cols):\n",
    "        log_with_memory(f\"ERROR: Incorrect columns. Expected {required_cols}, Found {submission_df.columns}\", \"ERROR\")\n",
    "        return False\n",
    "    log_with_memory(\"Required columns present\")\n",
    "\n",
    "    # Check all test sessions are covered\n",
    "    test_sessions = set(test_df[\"session\"].unique().to_list())\n",
    "    submission_sessions = set()\n",
    "\n",
    "    for row in submission_df.iter_rows():\n",
    "        session_type = row[0]\n",
    "        session_id = int(session_type.split(\"_\")[0])\n",
    "        submission_sessions.add(session_id)\n",
    "\n",
    "    missing_sessions = test_sessions - submission_sessions\n",
    "    if missing_sessions:\n",
    "        log_with_memory(f\"ERROR: Missing {len(missing_sessions)} test sessions\", \"ERROR\")\n",
    "        return False\n",
    "    log_with_memory(f\"All {len(test_sessions):,} test sessions covered\")\n",
    "\n",
    "    # Check each session has all three event types\n",
    "    session_types_count = {}\n",
    "    for row in submission_df.iter_rows():\n",
    "        session_type = row[0]\n",
    "        parts = session_type.split(\"_\")\n",
    "        session_id = parts[0]\n",
    "        event_type = parts[1]\n",
    "\n",
    "        if session_id not in session_types_count:\n",
    "            session_types_count[session_id] = set()\n",
    "        session_types_count[session_id].add(event_type)\n",
    "\n",
    "    required_types = set(config.REQUIRED_EVENT_TYPES)\n",
    "    incomplete_sessions = []\n",
    "    for session_id, types in session_types_count.items():\n",
    "        if types != required_types:\n",
    "            incomplete_sessions.append(session_id)\n",
    "\n",
    "    if incomplete_sessions:\n",
    "        log_with_memory(f\"ERROR: {len(incomplete_sessions)} sessions missing event types\", \"ERROR\")\n",
    "        log_with_memory(f\"First few: {incomplete_sessions[:5]}\", \"ERROR\")\n",
    "        return False\n",
    "    log_with_memory(\"All sessions have all event types\")\n",
    "\n",
    "    # Check label format and count\n",
    "    label_errors = 0\n",
    "    for i, row in enumerate(submission_df.iter_rows()):\n",
    "        session_type, labels = row\n",
    "        label_list = labels.strip().split()\n",
    "\n",
    "        # Check we have exactly 20 labels\n",
    "        if len(label_list) != config.TOP_K_PREDICTIONS:\n",
    "            label_errors += 1\n",
    "            if label_errors <= 5:  # Show first few errors\n",
    "                log_with_memory(f\"ERROR: Row {i}: Expected {config.TOP_K_PREDICTIONS} labels, got {len(label_list)}\", \"ERROR\")\n",
    "\n",
    "        # Check all labels are integers (sample check)\n",
    "        if i < 100:  # Check first 100 rows\n",
    "            try:\n",
    "                [int(x) for x in label_list]\n",
    "            except ValueError:\n",
    "                log_with_memory(f\"ERROR: Row {i}: Non-integer labels found\", \"ERROR\")\n",
    "                return False\n",
    "\n",
    "    if label_errors > 0:\n",
    "        log_with_memory(f\"ERROR: {label_errors} rows with incorrect label count\", \"ERROR\")\n",
    "        return False\n",
    "    log_with_memory(f\"All rows have exactly {config.TOP_K_PREDICTIONS} labels\")\n",
    "\n",
    "    # Additional format checks\n",
    "    total_expected_rows = len(test_sessions) * len(config.REQUIRED_EVENT_TYPES)\n",
    "    if len(submission_df) != total_expected_rows:\n",
    "        log_with_memory(f\"ERROR: Expected {total_expected_rows} rows, got {len(submission_df)}\", \"ERROR\")\n",
    "        return False\n",
    "\n",
    "    log_with_memory(\"Submission validation passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCbNVYOrCz1P"
   },
   "source": [
    "## MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3k9-bmyXC1cK",
    "outputId": "f28c065a-9c57-4064-810c-1bd7963e4b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:32:26] [2.3GB/3.7%] [INFO] ================================================================================\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] OTTO PART 3: HIGH-PERFORMANCE INFERENCE & SUBMISSION GENERATION\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] ================================================================================\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Step 1: Validating input files...\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Validating input files...\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: test_candidates.parquet (72.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: test_clean.parquet (55.3 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: item_stats.parquet (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: feature_columns.json (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: ranker_clicks.txt (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: ranker_carts.txt (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: ranker_orders.txt (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Found: evaluation_results.json (0.0 MB)\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Input validation passed! Available models: ['clicks', 'carts', 'orders']\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Step 2: Loading all required inputs...\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Loading all inputs...\n",
      "[12:32:26] [2.3GB/3.7%] [INFO] Loading test candidates...\n",
      "[12:32:27] [4.4GB/6.3%] [INFO] Test candidates: (95687062, 3)\n",
      "[12:32:27] [4.4GB/6.3%] [INFO] Loading clean test data...\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Test data: (6924640, 4)\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loading item statistics...\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Item stats: (1000, 6)\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loading feature columns...\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Feature columns: 61 features\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loading trained ranking models...\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loaded clicks model successfully\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loaded carts model successfully\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loaded orders model successfully\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loaded evaluation results: 0.04109837571536188\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] All inputs loaded successfully!\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Loaded 61 feature columns and 3 models.\n",
      "[12:32:27] [4.6GB/6.6%] [INFO] Test candidates shape: (95687062, 3)\n",
      "[12:32:28] [4.6GB/6.5%] [INFO] Step 3: Creating test features...\n",
      "[12:32:28] [4.6GB/6.5%] [INFO] Starting high-performance feature engineering...\n",
      "[12:32:28] [4.6GB/6.5%] [INFO] Computing session-level features...\n",
      "[12:32:28] [4.8GB/6.8%] [INFO] Computing item-level features...\n",
      "[12:32:28] [4.8GB/6.8%] [INFO] Computing session-item interaction features...\n",
      "[12:32:29] [5.0GB/7.1%] [INFO] Joining all feature sets...\n",
      "[12:32:37] [17.7GB/22.2%] [INFO] Creating final derived features and filling null values...\n",
      "[12:32:44] [24.8GB/30.8%] [INFO] Feature engineering completed. Shape: (95687062, 27)\n",
      "[12:32:44] [24.6GB/30.5%] [INFO] Step 4: Generating all predictions...\n",
      "[12:32:44] [24.6GB/30.5%] [INFO] Starting high-performance prediction generation...\n",
      "[12:32:44] [24.6GB/30.5%] [INFO] Processing event type: clicks\n",
      "[12:32:45] [32.2GB/39.6%] [INFO] Predicting on 37,235,896 candidates for clicks...\n",
      "[12:32:45] [32.2GB/39.6%] [WARNING] Adding 57 missing columns for prediction.\n",
      "[12:33:07] [49.9GB/60.9%] [INFO] Processing event type: carts\n",
      "[12:33:08] [55.9GB/68.0%] [INFO] Predicting on 29,225,562 candidates for carts...\n",
      "[12:33:08] [55.9GB/68.0%] [WARNING] Adding 57 missing columns for prediction.\n",
      "[12:33:27] [44.8GB/54.7%] [INFO] Processing event type: orders\n",
      "[12:33:28] [50.7GB/61.8%] [INFO] Predicting on 29,225,604 candidates for orders...\n",
      "[12:33:28] [50.7GB/61.8%] [WARNING] Adding 57 missing columns for prediction.\n",
      "[12:33:47] [45.1GB/55.0%] [INFO] Combining predictions from all event types...\n",
      "[12:33:48] [45.5GB/55.5%] [INFO] Prediction generation complete.\n",
      "[12:33:50] [25.6GB/31.6%] [INFO] Step 5: Creating submission file...\n",
      "[12:33:50] [25.6GB/31.6%] [INFO] Creating final submission file...\n",
      "[12:33:50] [25.7GB/31.8%] [INFO] Submission file created with 5,015,409 rows.\n",
      "[12:33:50] [25.7GB/31.8%] [INFO] Step 6: Validating and saving final submission...\n",
      "[12:33:50] [25.7GB/31.8%] [INFO] Submission validation: PASSED\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] ============================================================\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] HIGH-PERFORMANCE OTTO SOLUTION COMPLETED\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] ============================================================\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] Total Runtime: 1.43 minutes\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] Submission Rows: 5,015,409\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] Peak Memory Usage: 25.7GB\n",
      "[12:33:52] [25.7GB/31.8%] [INFO] Final submission saved to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/submission.csv\n"
     ]
    }
   ],
   "source": [
    "def main_high_performance():\n",
    "    \"\"\"\n",
    "    High-performance main execution function optimized for A100-level hardware.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    log_with_memory(\"=\" * 80)\n",
    "    log_with_memory(\"OTTO PART 3: HIGH-PERFORMANCE INFERENCE & SUBMISSION GENERATION\")\n",
    "    log_with_memory(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Validate inputs\n",
    "        log_with_memory(\"Step 1: Validating input files...\")\n",
    "        validation_results = validate_input_files()\n",
    "\n",
    "        # Step 2: Load all inputs\n",
    "        log_with_memory(\"Step 2: Loading all required inputs...\")\n",
    "        test_candidates, test_df, item_stats, feature_columns, ranking_models, _ = load_all_inputs(validation_results)\n",
    "\n",
    "        log_with_memory(f\"Loaded {len(feature_columns)} feature columns and {len(ranking_models)} models.\")\n",
    "        log_with_memory(f\"Test candidates shape: {test_candidates.shape}\")\n",
    "        simplified_garbage_collection()\n",
    "\n",
    "        # Step 3: Create features using the high-performance function\n",
    "        log_with_memory(\"Step 3: Creating test features...\")\n",
    "        test_features = create_test_ranking_features(test_candidates, test_df, item_stats)\n",
    "\n",
    "        # Clean up original dataframes to free memory before prediction\n",
    "        safe_delete(test_candidates, test_df)\n",
    "\n",
    "        # Step 4: Generate all predictions in-memory\n",
    "        log_with_memory(\"Step 4: Generating all predictions...\")\n",
    "        all_predictions = generate_all_predictions(test_features, ranking_models, feature_columns, item_stats)\n",
    "\n",
    "        # Clean up feature dataframe\n",
    "        safe_delete(test_features, item_stats)\n",
    "\n",
    "        # Step 5: Create the submission file using the vectorized function\n",
    "        log_with_memory(\"Step 5: Creating submission file...\")\n",
    "        submission = create_submission_file(all_predictions)\n",
    "\n",
    "        safe_delete(all_predictions)\n",
    "\n",
    "        # Step 6: Validate and save submission\n",
    "        log_with_memory(\"Step 6: Validating and saving final submission...\")\n",
    "        # A simple validation is sufficient as the creation logic is robust.\n",
    "        is_valid = (\n",
    "            len(submission.columns) == 2 and\n",
    "            \"session_type\" in submission.columns and\n",
    "            \"labels\" in submission.columns and\n",
    "            submission.height > 0\n",
    "        )\n",
    "        log_with_memory(f\"Submission validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "\n",
    "        if not is_valid:\n",
    "            raise ValueError(\"Submission validation failed. Check the generated file.\")\n",
    "\n",
    "        final_submission_path = f\"{config.OUTPUT_PATH}/submission.csv\"\n",
    "        submission.write_csv(final_submission_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime_minutes = (end_time - start_time) / 60\n",
    "\n",
    "        log_with_memory(\"=\" * 60)\n",
    "        log_with_memory(\"HIGH-PERFORMANCE OTTO SOLUTION COMPLETED\")\n",
    "        log_with_memory(\"=\" * 60)\n",
    "        log_with_memory(f\"Total Runtime: {runtime_minutes:.2f} minutes\")\n",
    "        log_with_memory(f\"Submission Rows: {len(submission):,}\")\n",
    "        log_with_memory(f\"Peak Memory Usage: {get_memory_usage():.1f}GB\")\n",
    "        log_with_memory(f\"Final submission saved to: {final_submission_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_with_memory(f\"An error occurred during execution: {e}\", \"CRITICAL\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "# Main execution call\n",
    "if __name__ == \"__main__\":\n",
    "    main_high_performance()\n",
    "\n",
    "# Note: The original submission validation and creation functions can be removed\n",
    "# as they have been replaced by faster, more direct versions.\n",
    "# The code in this cell should replace all functions under the \"MAIN EXECUTION\" heading."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
