{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVhfNZcTrznm"
   },
   "source": [
    "# Part 1A Data Preparation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsWggU61tw7N",
    "outputId": "f12edd1c-f757-45b3-bdfb-6f9f3e21e62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars==0.20.31 in /usr/local/lib/python3.11/dist-packages (0.20.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install polars==0.20.31\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0anRR_krdtA",
    "outputId": "7c7c21f5-d7d5-45a0-b114-3d5d132b959f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yprrefr2ryEn"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAMz9516ryhx",
    "outputId": "cfc14572-4557-4809-ce4e-4257ef7679a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration complete\n",
      "Dataset zip path: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-recommender-system.zip\n",
      "Extract path: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data-extracted/\n",
      "Data path: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/\n",
      "Output path: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths according to your Google Drive structure\n",
    "class DataConfig:\n",
    "    # Path where you uploaded the OTTO dataset zip file\n",
    "    DRIVE_BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content'\n",
    "\n",
    "    # Dataset paths (update these based on your folder structure)\n",
    "    DATASET_ZIP_PATH = f'{DRIVE_BASE_PATH}/otto-recommender-system.zip'\n",
    "    EXTRACT_PATH = f'{DRIVE_BASE_PATH}/otto-data-extracted/'\n",
    "\n",
    "    # Final processed data paths\n",
    "    DATA_PATH = f'{DRIVE_BASE_PATH}/otto-data/'\n",
    "    OUTPUT_PATH = f'{DRIVE_BASE_PATH}/otto-output/'\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "config = DataConfig()\n",
    "\n",
    "print(\"Configuration complete\")\n",
    "print(f\"Dataset zip path: {config.DATASET_ZIP_PATH}\")\n",
    "print(f\"Extract path: {config.EXTRACT_PATH}\")\n",
    "print(f\"Data path: {config.DATA_PATH}\")\n",
    "print(f\"Output path: {config.OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEJ6oNyds0qL"
   },
   "source": [
    "## Dataset Download and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAhDm3OVs1Hp",
    "outputId": "4f2c2e2e-75bd-46fe-9091-259a3b9444e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if dataset exists...\n",
      "✅ Dataset found at /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-recommender-system.zip\n",
      "✅ Dataset already extracted\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_data():\n",
    "    \"\"\"\n",
    "    Extract and prepare OTTO dataset files\n",
    "    \"\"\"\n",
    "    print(\"Checking if dataset exists...\")\n",
    "\n",
    "    # Check if zip file exists\n",
    "    if not os.path.exists(config.DATASET_ZIP_PATH):\n",
    "        print(f\"❌ Dataset not found at {config.DATASET_ZIP_PATH}\")\n",
    "        print(\"Please upload the otto-recommender-system.zip file to your Google Drive\")\n",
    "        print(\"You can download it from: https://www.kaggle.com/competitions/otto-recommender-system/data\")\n",
    "        return False\n",
    "\n",
    "    print(f\"✅ Dataset found at {config.DATASET_ZIP_PATH}\")\n",
    "\n",
    "    # Extract if not already extracted\n",
    "    train_path = os.path.join(config.EXTRACT_PATH, 'train.jsonl')\n",
    "    test_path = os.path.join(config.EXTRACT_PATH, 'test.jsonl')\n",
    "\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        print(\"✅ Dataset already extracted\")\n",
    "        return True\n",
    "\n",
    "    print(\"Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(config.DATASET_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(config.EXTRACT_PATH)\n",
    "        print(\"✅ Dataset extracted successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "# Download and extract data\n",
    "extraction_success = download_and_extract_data()\n",
    "\n",
    "if not extraction_success:\n",
    "    raise RuntimeError(\"Failed to extract dataset. Please check the file path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1XEa_BVs6Zn"
   },
   "source": [
    "## Data Format Conversion and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5Lfpa3ns6zK",
    "outputId": "c0f66653-ee50-4b7a-b965-8f1bcbd0a811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MEMORY-OPTIMIZED DATA CONVERSION\n",
      "============================================================\n",
      "Attempting chunk-based conversion...\n",
      "Converting Training Data from JSONL to optimized format (Memory Optimized)...\n",
      "Processing /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data-extracted/train.jsonl in memory-efficient chunks...\n",
      "  Processed chunk 1: 500,000 total rows\n",
      "  Processed chunk 2: 1,000,000 total rows\n",
      "  Processed chunk 3: 1,500,000 total rows\n",
      "  Processed chunk 4: 2,000,000 total rows\n",
      "  Processed chunk 5: 2,500,000 total rows\n",
      "  Read 50,000 lines from file...\n",
      "  Processed chunk 6: 3,000,000 total rows\n",
      "  Processed chunk 7: 3,500,000 total rows\n",
      "  Processed chunk 8: 4,000,000 total rows\n",
      "  Processed chunk 9: 4,500,000 total rows\n",
      "  Processed chunk 10: 5,000,000 total rows\n",
      "  Read 100,000 lines from file...\n",
      "  Processed chunk 11: 5,500,000 total rows\n",
      "  Processed chunk 12: 6,000,000 total rows\n",
      "  Processed chunk 13: 6,500,000 total rows\n",
      "  Processed chunk 14: 7,000,000 total rows\n",
      "  Processed chunk 15: 7,500,000 total rows\n",
      "  Read 150,000 lines from file...\n",
      "  Processed chunk 16: 8,000,000 total rows\n",
      "  Processed chunk 17: 8,500,000 total rows\n",
      "  Processed chunk 18: 9,000,000 total rows\n",
      "  Processed chunk 19: 9,500,000 total rows\n",
      "  Processed chunk 20: 10,000,000 total rows\n",
      "  Read 200,000 lines from file...\n",
      "  Processed chunk 21: 10,500,000 total rows\n",
      "  Processed chunk 22: 11,000,000 total rows\n",
      "  Processed chunk 23: 11,500,000 total rows\n",
      "  Processed chunk 24: 12,000,000 total rows\n",
      "  Processed chunk 25: 12,500,000 total rows\n",
      "  Read 250,000 lines from file...\n",
      "  Processed chunk 26: 13,000,000 total rows\n",
      "  Processed chunk 27: 13,500,000 total rows\n",
      "  Processed chunk 28: 14,000,000 total rows\n",
      "  Processed chunk 29: 14,500,000 total rows\n",
      "  Read 300,000 lines from file...\n",
      "  Processed chunk 30: 15,000,000 total rows\n",
      "  Processed chunk 31: 15,500,000 total rows\n",
      "  Processed chunk 32: 16,000,000 total rows\n",
      "  Processed chunk 33: 16,500,000 total rows\n",
      "  Read 350,000 lines from file...\n",
      "  Processed chunk 34: 17,000,000 total rows\n",
      "  Processed chunk 35: 17,500,000 total rows\n",
      "  Processed chunk 36: 18,000,000 total rows\n",
      "  Processed chunk 37: 18,500,000 total rows\n",
      "  Read 400,000 lines from file...\n",
      "  Processed chunk 38: 19,000,000 total rows\n",
      "  Processed chunk 39: 19,500,000 total rows\n",
      "  Processed chunk 40: 20,000,000 total rows\n",
      "  Processed chunk 41: 20,500,000 total rows\n",
      "  Read 450,000 lines from file...\n",
      "  Processed chunk 42: 21,000,000 total rows\n",
      "  Processed chunk 43: 21,500,000 total rows\n",
      "  Processed chunk 44: 22,000,000 total rows\n",
      "  Processed chunk 45: 22,500,000 total rows\n",
      "  Read 500,000 lines from file...\n",
      "  Processed chunk 46: 23,000,000 total rows\n",
      "  Processed chunk 47: 23,500,000 total rows\n",
      "  Processed chunk 48: 24,000,000 total rows\n",
      "  Read 550,000 lines from file...\n",
      "  Processed chunk 49: 24,500,000 total rows\n",
      "  Processed chunk 50: 25,000,000 total rows\n",
      "  Processed chunk 51: 25,500,000 total rows\n",
      "  Processed chunk 52: 26,000,000 total rows\n",
      "  Read 600,000 lines from file...\n",
      "  Processed chunk 53: 26,500,000 total rows\n",
      "  Processed chunk 54: 27,000,000 total rows\n",
      "  Processed chunk 55: 27,500,000 total rows\n",
      "  Read 650,000 lines from file...\n",
      "  Processed chunk 56: 28,000,000 total rows\n",
      "  Processed chunk 57: 28,500,000 total rows\n",
      "  Processed chunk 58: 29,000,000 total rows\n",
      "  Processed chunk 59: 29,500,000 total rows\n",
      "  Read 700,000 lines from file...\n",
      "  Processed chunk 60: 30,000,000 total rows\n",
      "  Processed chunk 61: 30,500,000 total rows\n",
      "  Processed chunk 62: 31,000,000 total rows\n",
      "  Read 750,000 lines from file...\n",
      "  Processed chunk 63: 31,500,000 total rows\n",
      "  Processed chunk 64: 32,000,000 total rows\n",
      "  Processed chunk 65: 32,500,000 total rows\n",
      "  Processed chunk 66: 33,000,000 total rows\n",
      "  Read 800,000 lines from file...\n",
      "  Processed chunk 67: 33,500,000 total rows\n",
      "  Processed chunk 68: 34,000,000 total rows\n",
      "  Processed chunk 69: 34,500,000 total rows\n",
      "  Read 850,000 lines from file...\n",
      "  Processed chunk 70: 35,000,000 total rows\n",
      "  Processed chunk 71: 35,500,000 total rows\n",
      "  Processed chunk 72: 36,000,000 total rows\n",
      "  Processed chunk 73: 36,500,000 total rows\n",
      "  Read 900,000 lines from file...\n",
      "  Processed chunk 74: 37,000,000 total rows\n",
      "  Processed chunk 75: 37,500,000 total rows\n",
      "  Processed chunk 76: 38,000,000 total rows\n",
      "  Read 950,000 lines from file...\n",
      "  Processed chunk 77: 38,500,000 total rows\n",
      "  Processed chunk 78: 39,000,000 total rows\n",
      "  Processed chunk 79: 39,500,000 total rows\n",
      "  Read 1,000,000 lines from file...\n",
      "  Processed chunk 80: 40,000,000 total rows\n",
      "  Processed chunk 81: 40,500,000 total rows\n",
      "  Processed chunk 82: 41,000,000 total rows\n",
      "  Read 1,050,000 lines from file...\n",
      "  Processed chunk 83: 41,500,000 total rows\n",
      "  Processed chunk 84: 42,000,000 total rows\n",
      "  Processed chunk 85: 42,500,000 total rows\n",
      "  Processed chunk 86: 43,000,000 total rows\n",
      "  Read 1,100,000 lines from file...\n",
      "  Processed chunk 87: 43,500,000 total rows\n",
      "  Processed chunk 88: 44,000,000 total rows\n",
      "  Processed chunk 89: 44,500,000 total rows\n",
      "  Read 1,150,000 lines from file...\n",
      "  Processed chunk 90: 45,000,000 total rows\n",
      "  Processed chunk 91: 45,500,000 total rows\n",
      "  Processed chunk 92: 46,000,000 total rows\n",
      "  Read 1,200,000 lines from file...\n",
      "  Processed chunk 93: 46,500,000 total rows\n",
      "  Processed chunk 94: 47,000,000 total rows\n",
      "  Processed chunk 95: 47,500,000 total rows\n",
      "  Read 1,250,000 lines from file...\n",
      "  Processed chunk 96: 48,000,000 total rows\n",
      "  Processed chunk 97: 48,500,000 total rows\n",
      "  Processed chunk 98: 49,000,000 total rows\n",
      "  Read 1,300,000 lines from file...\n",
      "  Processed chunk 99: 49,500,000 total rows\n",
      "  Processed chunk 100: 50,000,000 total rows\n",
      "  Processed chunk 101: 50,500,000 total rows\n",
      "  Processed chunk 102: 51,000,000 total rows\n",
      "  Read 1,350,000 lines from file...\n",
      "  Processed chunk 103: 51,500,000 total rows\n",
      "  Processed chunk 104: 52,000,000 total rows\n",
      "  Processed chunk 105: 52,500,000 total rows\n",
      "  Read 1,400,000 lines from file...\n",
      "  Processed chunk 106: 53,000,000 total rows\n",
      "  Processed chunk 107: 53,500,000 total rows\n",
      "  Read 1,450,000 lines from file...\n",
      "  Processed chunk 108: 54,000,000 total rows\n",
      "  Processed chunk 109: 54,500,000 total rows\n",
      "  Processed chunk 110: 55,000,000 total rows\n",
      "  Read 1,500,000 lines from file...\n",
      "  Processed chunk 111: 55,500,000 total rows\n",
      "  Processed chunk 112: 56,000,000 total rows\n",
      "  Processed chunk 113: 56,500,000 total rows\n",
      "  Read 1,550,000 lines from file...\n",
      "  Processed chunk 114: 57,000,000 total rows\n",
      "  Processed chunk 115: 57,500,000 total rows\n",
      "  Processed chunk 116: 58,000,000 total rows\n",
      "  Read 1,600,000 lines from file...\n",
      "  Processed chunk 117: 58,500,000 total rows\n",
      "  Processed chunk 118: 59,000,000 total rows\n",
      "  Read 1,650,000 lines from file...\n",
      "  Processed chunk 119: 59,500,000 total rows\n",
      "  Processed chunk 120: 60,000,000 total rows\n",
      "  Processed chunk 121: 60,500,000 total rows\n",
      "  Read 1,700,000 lines from file...\n",
      "  Processed chunk 122: 61,000,000 total rows\n",
      "  Processed chunk 123: 61,500,000 total rows\n",
      "  Processed chunk 124: 62,000,000 total rows\n",
      "  Read 1,750,000 lines from file...\n",
      "  Processed chunk 125: 62,500,000 total rows\n",
      "  Processed chunk 126: 63,000,000 total rows\n",
      "  Read 1,800,000 lines from file...\n",
      "  Processed chunk 127: 63,500,000 total rows\n",
      "  Processed chunk 128: 64,000,000 total rows\n",
      "  Processed chunk 129: 64,500,000 total rows\n",
      "  Read 1,850,000 lines from file...\n",
      "  Processed chunk 130: 65,000,000 total rows\n",
      "  Processed chunk 131: 65,500,000 total rows\n",
      "  Read 1,900,000 lines from file...\n",
      "  Processed chunk 132: 66,000,000 total rows\n",
      "  Processed chunk 133: 66,500,000 total rows\n",
      "  Processed chunk 134: 67,000,000 total rows\n",
      "  Read 1,950,000 lines from file...\n",
      "  Processed chunk 135: 67,500,000 total rows\n",
      "  Processed chunk 136: 68,000,000 total rows\n",
      "  Read 2,000,000 lines from file...\n",
      "  Processed chunk 137: 68,500,000 total rows\n",
      "  Processed chunk 138: 69,000,000 total rows\n",
      "  Processed chunk 139: 69,500,000 total rows\n",
      "  Read 2,050,000 lines from file...\n",
      "  Processed chunk 140: 70,000,000 total rows\n",
      "  Processed chunk 141: 70,500,000 total rows\n",
      "  Read 2,100,000 lines from file...\n",
      "  Processed chunk 142: 71,000,000 total rows\n",
      "  Processed chunk 143: 71,500,000 total rows\n",
      "  Read 2,150,000 lines from file...\n",
      "  Processed chunk 144: 72,000,000 total rows\n",
      "  Processed chunk 145: 72,500,000 total rows\n",
      "  Processed chunk 146: 73,000,000 total rows\n",
      "  Read 2,200,000 lines from file...\n",
      "  Processed chunk 147: 73,500,000 total rows\n",
      "  Processed chunk 148: 74,000,000 total rows\n",
      "  Processed chunk 149: 74,500,000 total rows\n",
      "  Read 2,250,000 lines from file...\n",
      "  Processed chunk 150: 75,000,000 total rows\n",
      "  Processed chunk 151: 75,500,000 total rows\n",
      "  Read 2,300,000 lines from file...\n",
      "  Processed chunk 152: 76,000,000 total rows\n",
      "  Processed chunk 153: 76,500,000 total rows\n",
      "  Read 2,350,000 lines from file...\n",
      "  Processed chunk 154: 77,000,000 total rows\n",
      "  Processed chunk 155: 77,500,000 total rows\n",
      "  Read 2,400,000 lines from file...\n",
      "  Processed chunk 156: 78,000,000 total rows\n",
      "  Processed chunk 157: 78,500,000 total rows\n",
      "  Processed chunk 158: 79,000,000 total rows\n",
      "  Read 2,450,000 lines from file...\n",
      "  Processed chunk 159: 79,500,000 total rows\n",
      "  Processed chunk 160: 80,000,000 total rows\n",
      "  Read 2,500,000 lines from file...\n",
      "  Processed chunk 161: 80,500,000 total rows\n",
      "  Processed chunk 162: 81,000,000 total rows\n",
      "  Read 2,550,000 lines from file...\n",
      "  Processed chunk 163: 81,500,000 total rows\n",
      "  Processed chunk 164: 82,000,000 total rows\n",
      "  Read 2,600,000 lines from file...\n",
      "  Processed chunk 165: 82,500,000 total rows\n",
      "  Processed chunk 166: 83,000,000 total rows\n",
      "  Processed chunk 167: 83,500,000 total rows\n",
      "  Read 2,650,000 lines from file...\n",
      "  Processed chunk 168: 84,000,000 total rows\n",
      "  Processed chunk 169: 84,500,000 total rows\n",
      "  Read 2,700,000 lines from file...\n",
      "  Processed chunk 170: 85,000,000 total rows\n",
      "  Processed chunk 171: 85,500,000 total rows\n",
      "  Read 2,750,000 lines from file...\n",
      "  Processed chunk 172: 86,000,000 total rows\n",
      "  Processed chunk 173: 86,500,000 total rows\n",
      "  Read 2,800,000 lines from file...\n",
      "  Processed chunk 174: 87,000,000 total rows\n",
      "  Processed chunk 175: 87,500,000 total rows\n",
      "  Read 2,850,000 lines from file...\n",
      "  Processed chunk 176: 88,000,000 total rows\n",
      "  Processed chunk 177: 88,500,000 total rows\n",
      "  Read 2,900,000 lines from file...\n",
      "  Processed chunk 178: 89,000,000 total rows\n",
      "  Processed chunk 179: 89,500,000 total rows\n",
      "  Read 2,950,000 lines from file...\n",
      "  Processed chunk 180: 90,000,000 total rows\n",
      "  Processed chunk 181: 90,500,000 total rows\n",
      "  Read 3,000,000 lines from file...\n",
      "  Processed chunk 182: 91,000,000 total rows\n",
      "  Processed chunk 183: 91,500,000 total rows\n",
      "  Read 3,050,000 lines from file...\n",
      "  Processed chunk 184: 92,000,000 total rows\n",
      "  Processed chunk 185: 92,500,000 total rows\n",
      "  Processed chunk 186: 93,000,000 total rows\n",
      "  Read 3,100,000 lines from file...\n",
      "  Processed chunk 187: 93,500,000 total rows\n",
      "  Read 3,150,000 lines from file...\n",
      "  Processed chunk 188: 94,000,000 total rows\n",
      "  Processed chunk 189: 94,500,000 total rows\n",
      "  Read 3,200,000 lines from file...\n",
      "  Processed chunk 190: 95,000,000 total rows\n",
      "  Processed chunk 191: 95,500,000 total rows\n",
      "  Read 3,250,000 lines from file...\n",
      "  Processed chunk 192: 96,000,000 total rows\n",
      "  Processed chunk 193: 96,500,000 total rows\n",
      "  Read 3,300,000 lines from file...\n",
      "  Processed chunk 194: 97,000,000 total rows\n",
      "  Processed chunk 195: 97,500,000 total rows\n",
      "  Read 3,350,000 lines from file...\n",
      "  Processed chunk 196: 98,000,000 total rows\n",
      "  Processed chunk 197: 98,500,000 total rows\n",
      "  Read 3,400,000 lines from file...\n",
      "  Processed chunk 198: 99,000,000 total rows\n",
      "  Processed chunk 199: 99,500,000 total rows\n",
      "  Read 3,450,000 lines from file...\n",
      "  Processed chunk 200: 100,000,000 total rows\n",
      "  Processed chunk 201: 100,500,000 total rows\n",
      "  Read 3,500,000 lines from file...\n",
      "  Processed chunk 202: 101,000,000 total rows\n",
      "  Processed chunk 203: 101,500,000 total rows\n",
      "  Read 3,550,000 lines from file...\n",
      "  Processed chunk 204: 102,000,000 total rows\n",
      "  Read 3,600,000 lines from file...\n",
      "  Processed chunk 205: 102,500,000 total rows\n",
      "  Processed chunk 206: 103,000,000 total rows\n",
      "  Read 3,650,000 lines from file...\n",
      "  Processed chunk 207: 103,500,000 total rows\n",
      "  Processed chunk 208: 104,000,000 total rows\n",
      "  Read 3,700,000 lines from file...\n",
      "  Processed chunk 209: 104,500,000 total rows\n",
      "  Processed chunk 210: 105,000,000 total rows\n",
      "  Read 3,750,000 lines from file...\n",
      "  Processed chunk 211: 105,500,000 total rows\n",
      "  Processed chunk 212: 106,000,000 total rows\n",
      "  Read 3,800,000 lines from file...\n",
      "  Processed chunk 213: 106,500,000 total rows\n",
      "  Processed chunk 214: 107,000,000 total rows\n",
      "  Read 3,850,000 lines from file...\n",
      "  Processed chunk 215: 107,500,000 total rows\n",
      "  Processed chunk 216: 108,000,000 total rows\n",
      "  Read 3,900,000 lines from file...\n",
      "  Processed chunk 217: 108,500,000 total rows\n",
      "  Read 3,950,000 lines from file...\n",
      "  Processed chunk 218: 109,000,000 total rows\n",
      "  Processed chunk 219: 109,500,000 total rows\n",
      "  Read 4,000,000 lines from file...\n",
      "  Processed chunk 220: 110,000,000 total rows\n",
      "  Processed chunk 221: 110,500,000 total rows\n",
      "  Read 4,050,000 lines from file...\n",
      "  Processed chunk 222: 111,000,000 total rows\n",
      "  Processed chunk 223: 111,500,000 total rows\n",
      "  Read 4,100,000 lines from file...\n",
      "  Processed chunk 224: 112,000,000 total rows\n",
      "  Processed chunk 225: 112,500,000 total rows\n",
      "  Read 4,150,000 lines from file...\n",
      "  Processed chunk 226: 113,000,000 total rows\n",
      "  Processed chunk 227: 113,500,000 total rows\n",
      "  Read 4,200,000 lines from file...\n",
      "  Processed chunk 228: 114,000,000 total rows\n",
      "  Read 4,250,000 lines from file...\n",
      "  Processed chunk 229: 114,500,000 total rows\n",
      "  Processed chunk 230: 115,000,000 total rows\n",
      "  Read 4,300,000 lines from file...\n",
      "  Processed chunk 231: 115,500,000 total rows\n",
      "  Processed chunk 232: 116,000,000 total rows\n",
      "  Read 4,350,000 lines from file...\n",
      "  Processed chunk 233: 116,500,000 total rows\n",
      "  Processed chunk 234: 117,000,000 total rows\n",
      "  Read 4,400,000 lines from file...\n",
      "  Processed chunk 235: 117,500,000 total rows\n",
      "  Processed chunk 236: 118,000,000 total rows\n",
      "  Read 4,450,000 lines from file...\n",
      "  Processed chunk 237: 118,500,000 total rows\n",
      "  Read 4,500,000 lines from file...\n",
      "  Processed chunk 238: 119,000,000 total rows\n",
      "  Processed chunk 239: 119,500,000 total rows\n",
      "  Read 4,550,000 lines from file...\n",
      "  Processed chunk 240: 120,000,000 total rows\n",
      "  Processed chunk 241: 120,500,000 total rows\n",
      "  Read 4,600,000 lines from file...\n",
      "  Processed chunk 242: 121,000,000 total rows\n",
      "  Processed chunk 243: 121,500,000 total rows\n",
      "  Read 4,650,000 lines from file...\n",
      "  Processed chunk 244: 122,000,000 total rows\n",
      "  Read 4,700,000 lines from file...\n",
      "  Processed chunk 245: 122,500,000 total rows\n",
      "  Processed chunk 246: 123,000,000 total rows\n",
      "  Read 4,750,000 lines from file...\n",
      "  Processed chunk 247: 123,500,000 total rows\n",
      "  Processed chunk 248: 124,000,000 total rows\n",
      "  Read 4,800,000 lines from file...\n",
      "  Processed chunk 249: 124,500,000 total rows\n",
      "  Processed chunk 250: 125,000,000 total rows\n",
      "  Read 4,850,000 lines from file...\n",
      "  Processed chunk 251: 125,500,000 total rows\n",
      "  Read 4,900,000 lines from file...\n",
      "  Processed chunk 252: 126,000,000 total rows\n",
      "  Processed chunk 253: 126,500,000 total rows\n",
      "  Read 4,950,000 lines from file...\n",
      "  Processed chunk 254: 127,000,000 total rows\n",
      "  Processed chunk 255: 127,500,000 total rows\n",
      "  Read 5,000,000 lines from file...\n",
      "  Processed chunk 256: 128,000,000 total rows\n",
      "  Processed chunk 257: 128,500,000 total rows\n",
      "  Read 5,050,000 lines from file...\n",
      "  Processed chunk 258: 129,000,000 total rows\n",
      "  Read 5,100,000 lines from file...\n",
      "  Processed chunk 259: 129,500,000 total rows\n",
      "  Processed chunk 260: 130,000,000 total rows\n",
      "  Read 5,150,000 lines from file...\n",
      "  Processed chunk 261: 130,500,000 total rows\n",
      "  Read 5,200,000 lines from file...\n",
      "  Processed chunk 262: 131,000,000 total rows\n",
      "  Processed chunk 263: 131,500,000 total rows\n",
      "  Read 5,250,000 lines from file...\n",
      "  Processed chunk 264: 132,000,000 total rows\n",
      "  Processed chunk 265: 132,500,000 total rows\n",
      "  Read 5,300,000 lines from file...\n",
      "  Processed chunk 266: 133,000,000 total rows\n",
      "  Read 5,350,000 lines from file...\n",
      "  Processed chunk 267: 133,500,000 total rows\n",
      "  Processed chunk 268: 134,000,000 total rows\n",
      "  Read 5,400,000 lines from file...\n",
      "  Processed chunk 269: 134,500,000 total rows\n",
      "  Processed chunk 270: 135,000,000 total rows\n",
      "  Read 5,450,000 lines from file...\n",
      "  Processed chunk 271: 135,500,000 total rows\n",
      "  Read 5,500,000 lines from file...\n",
      "  Processed chunk 272: 136,000,000 total rows\n",
      "  Processed chunk 273: 136,500,000 total rows\n",
      "  Read 5,550,000 lines from file...\n",
      "  Processed chunk 274: 137,000,000 total rows\n",
      "  Read 5,600,000 lines from file...\n",
      "  Processed chunk 275: 137,500,000 total rows\n",
      "  Processed chunk 276: 138,000,000 total rows\n",
      "  Read 5,650,000 lines from file...\n",
      "  Processed chunk 277: 138,500,000 total rows\n",
      "  Read 5,700,000 lines from file...\n",
      "  Processed chunk 278: 139,000,000 total rows\n",
      "  Processed chunk 279: 139,500,000 total rows\n",
      "  Read 5,750,000 lines from file...\n",
      "  Processed chunk 280: 140,000,000 total rows\n",
      "  Read 5,800,000 lines from file...\n",
      "  Processed chunk 281: 140,500,000 total rows\n",
      "  Processed chunk 282: 141,000,000 total rows\n",
      "  Read 5,850,000 lines from file...\n",
      "  Processed chunk 283: 141,500,000 total rows\n",
      "  Read 5,900,000 lines from file...\n",
      "  Processed chunk 284: 142,000,000 total rows\n",
      "  Processed chunk 285: 142,500,000 total rows\n",
      "  Read 5,950,000 lines from file...\n",
      "  Processed chunk 286: 143,000,000 total rows\n",
      "  Read 6,000,000 lines from file...\n",
      "  Processed chunk 287: 143,500,000 total rows\n",
      "  Processed chunk 288: 144,000,000 total rows\n",
      "  Read 6,050,000 lines from file...\n",
      "  Processed chunk 289: 144,500,000 total rows\n",
      "  Read 6,100,000 lines from file...\n",
      "  Processed chunk 290: 145,000,000 total rows\n",
      "  Read 6,150,000 lines from file...\n",
      "  Processed chunk 291: 145,500,000 total rows\n",
      "  Processed chunk 292: 146,000,000 total rows\n",
      "  Read 6,200,000 lines from file...\n",
      "  Processed chunk 293: 146,500,000 total rows\n",
      "  Read 6,250,000 lines from file...\n",
      "  Processed chunk 294: 147,000,000 total rows\n",
      "  Processed chunk 295: 147,500,000 total rows\n",
      "  Read 6,300,000 lines from file...\n",
      "  Processed chunk 296: 148,000,000 total rows\n",
      "  Read 6,350,000 lines from file...\n",
      "  Processed chunk 297: 148,500,000 total rows\n",
      "  Read 6,400,000 lines from file...\n",
      "  Processed chunk 298: 149,000,000 total rows\n",
      "  Processed chunk 299: 149,500,000 total rows\n",
      "  Read 6,450,000 lines from file...\n",
      "  Processed chunk 300: 150,000,000 total rows\n",
      "  Read 6,500,000 lines from file...\n",
      "  Processed chunk 301: 150,500,000 total rows\n",
      "  Read 6,550,000 lines from file...\n",
      "  Processed chunk 302: 151,000,000 total rows\n",
      "  Processed chunk 303: 151,500,000 total rows\n",
      "  Read 6,600,000 lines from file...\n",
      "  Processed chunk 304: 152,000,000 total rows\n",
      "  Read 6,650,000 lines from file...\n",
      "  Processed chunk 305: 152,500,000 total rows\n",
      "  Read 6,700,000 lines from file...\n",
      "  Processed chunk 306: 153,000,000 total rows\n",
      "  Processed chunk 307: 153,500,000 total rows\n",
      "  Read 6,750,000 lines from file...\n",
      "  Processed chunk 308: 154,000,000 total rows\n",
      "  Read 6,800,000 lines from file...\n",
      "  Processed chunk 309: 154,500,000 total rows\n",
      "  Read 6,850,000 lines from file...\n",
      "  Processed chunk 310: 155,000,000 total rows\n",
      "  Processed chunk 311: 155,500,000 total rows\n",
      "  Read 6,900,000 lines from file...\n",
      "  Processed chunk 312: 156,000,000 total rows\n",
      "  Read 6,950,000 lines from file...\n",
      "  Processed chunk 313: 156,500,000 total rows\n",
      "  Read 7,000,000 lines from file...\n",
      "  Processed chunk 314: 157,000,000 total rows\n",
      "  Read 7,050,000 lines from file...\n",
      "  Processed chunk 315: 157,500,000 total rows\n",
      "  Processed chunk 316: 158,000,000 total rows\n",
      "  Read 7,100,000 lines from file...\n",
      "  Processed chunk 317: 158,500,000 total rows\n",
      "  Read 7,150,000 lines from file...\n",
      "  Processed chunk 318: 159,000,000 total rows\n",
      "  Read 7,200,000 lines from file...\n",
      "  Processed chunk 319: 159,500,000 total rows\n",
      "  Read 7,250,000 lines from file...\n",
      "  Processed chunk 320: 160,000,000 total rows\n",
      "  Processed chunk 321: 160,500,000 total rows\n",
      "  Read 7,300,000 lines from file...\n",
      "  Processed chunk 322: 161,000,000 total rows\n",
      "  Read 7,350,000 lines from file...\n",
      "  Processed chunk 323: 161,500,000 total rows\n",
      "  Read 7,400,000 lines from file...\n",
      "  Processed chunk 324: 162,000,000 total rows\n",
      "  Read 7,450,000 lines from file...\n",
      "  Processed chunk 325: 162,500,000 total rows\n",
      "  Read 7,500,000 lines from file...\n",
      "  Processed chunk 326: 163,000,000 total rows\n",
      "  Read 7,550,000 lines from file...\n",
      "  Processed chunk 327: 163,500,000 total rows\n",
      "  Processed chunk 328: 164,000,000 total rows\n",
      "  Read 7,600,000 lines from file...\n",
      "  Processed chunk 329: 164,500,000 total rows\n",
      "  Read 7,650,000 lines from file...\n",
      "  Processed chunk 330: 165,000,000 total rows\n",
      "  Read 7,700,000 lines from file...\n",
      "  Processed chunk 331: 165,500,000 total rows\n",
      "  Read 7,750,000 lines from file...\n",
      "  Processed chunk 332: 166,000,000 total rows\n",
      "  Read 7,800,000 lines from file...\n",
      "  Processed chunk 333: 166,500,000 total rows\n",
      "  Processed chunk 334: 167,000,000 total rows\n",
      "  Read 7,850,000 lines from file...\n",
      "  Processed chunk 335: 167,500,000 total rows\n",
      "  Read 7,900,000 lines from file...\n",
      "  Processed chunk 336: 168,000,000 total rows\n",
      "  Read 7,950,000 lines from file...\n",
      "  Processed chunk 337: 168,500,000 total rows\n",
      "  Read 8,000,000 lines from file...\n",
      "  Processed chunk 338: 169,000,000 total rows\n",
      "  Read 8,050,000 lines from file...\n",
      "  Processed chunk 339: 169,500,000 total rows\n",
      "  Read 8,100,000 lines from file...\n",
      "  Processed chunk 340: 170,000,000 total rows\n",
      "  Read 8,150,000 lines from file...\n",
      "  Processed chunk 341: 170,500,000 total rows\n",
      "  Processed chunk 342: 171,000,000 total rows\n",
      "  Read 8,200,000 lines from file...\n",
      "  Processed chunk 343: 171,500,000 total rows\n",
      "  Read 8,250,000 lines from file...\n",
      "  Processed chunk 344: 172,000,000 total rows\n",
      "  Read 8,300,000 lines from file...\n",
      "  Processed chunk 345: 172,500,000 total rows\n",
      "  Read 8,350,000 lines from file...\n",
      "  Processed chunk 346: 173,000,000 total rows\n",
      "  Read 8,400,000 lines from file...\n",
      "  Processed chunk 347: 173,500,000 total rows\n",
      "  Read 8,450,000 lines from file...\n",
      "  Processed chunk 348: 174,000,000 total rows\n",
      "  Read 8,500,000 lines from file...\n",
      "  Processed chunk 349: 174,500,000 total rows\n",
      "  Read 8,550,000 lines from file...\n",
      "  Processed chunk 350: 175,000,000 total rows\n",
      "  Read 8,600,000 lines from file...\n",
      "  Processed chunk 351: 175,500,000 total rows\n",
      "  Read 8,650,000 lines from file...\n",
      "  Processed chunk 352: 176,000,000 total rows\n",
      "  Processed chunk 353: 176,500,000 total rows\n",
      "  Read 8,700,000 lines from file...\n",
      "  Processed chunk 354: 177,000,000 total rows\n",
      "  Read 8,750,000 lines from file...\n",
      "  Processed chunk 355: 177,500,000 total rows\n",
      "  Read 8,800,000 lines from file...\n",
      "  Processed chunk 356: 178,000,000 total rows\n",
      "  Read 8,850,000 lines from file...\n",
      "  Processed chunk 357: 178,500,000 total rows\n",
      "  Read 8,900,000 lines from file...\n",
      "  Processed chunk 358: 179,000,000 total rows\n",
      "  Read 8,950,000 lines from file...\n",
      "  Processed chunk 359: 179,500,000 total rows\n",
      "  Read 9,000,000 lines from file...\n",
      "  Processed chunk 360: 180,000,000 total rows\n",
      "  Read 9,050,000 lines from file...\n",
      "  Processed chunk 361: 180,500,000 total rows\n",
      "  Read 9,100,000 lines from file...\n",
      "  Processed chunk 362: 181,000,000 total rows\n",
      "  Read 9,150,000 lines from file...\n",
      "  Processed chunk 363: 181,500,000 total rows\n",
      "  Read 9,200,000 lines from file...\n",
      "  Processed chunk 364: 182,000,000 total rows\n",
      "  Read 9,250,000 lines from file...\n",
      "  Processed chunk 365: 182,500,000 total rows\n",
      "  Read 9,300,000 lines from file...\n",
      "  Processed chunk 366: 183,000,000 total rows\n",
      "  Read 9,350,000 lines from file...\n",
      "  Processed chunk 367: 183,500,000 total rows\n",
      "  Processed chunk 368: 184,000,000 total rows\n",
      "  Read 9,400,000 lines from file...\n",
      "  Processed chunk 369: 184,500,000 total rows\n",
      "  Read 9,450,000 lines from file...\n",
      "  Processed chunk 370: 185,000,000 total rows\n",
      "  Read 9,500,000 lines from file...\n",
      "  Processed chunk 371: 185,500,000 total rows\n",
      "  Read 9,550,000 lines from file...\n",
      "  Processed chunk 372: 186,000,000 total rows\n",
      "  Read 9,600,000 lines from file...\n",
      "  Processed chunk 373: 186,500,000 total rows\n",
      "  Read 9,650,000 lines from file...\n",
      "  Processed chunk 374: 187,000,000 total rows\n",
      "  Read 9,700,000 lines from file...\n",
      "  Processed chunk 375: 187,500,000 total rows\n",
      "  Read 9,750,000 lines from file...\n",
      "  Processed chunk 376: 188,000,000 total rows\n",
      "  Read 9,800,000 lines from file...\n",
      "  Processed chunk 377: 188,500,000 total rows\n",
      "  Read 9,850,000 lines from file...\n",
      "  Processed chunk 378: 189,000,000 total rows\n",
      "  Read 9,900,000 lines from file...\n",
      "  Processed chunk 379: 189,500,000 total rows\n",
      "  Read 9,950,000 lines from file...\n",
      "  Processed chunk 380: 190,000,000 total rows\n",
      "  Read 10,000,000 lines from file...\n",
      "  Processed chunk 381: 190,500,000 total rows\n",
      "  Read 10,050,000 lines from file...\n",
      "  Processed chunk 382: 191,000,000 total rows\n",
      "  Read 10,100,000 lines from file...\n",
      "  Processed chunk 383: 191,500,000 total rows\n",
      "  Read 10,150,000 lines from file...\n",
      "  Processed chunk 384: 192,000,000 total rows\n",
      "  Read 10,200,000 lines from file...\n",
      "  Processed chunk 385: 192,500,000 total rows\n",
      "  Read 10,250,000 lines from file...\n",
      "  Processed chunk 386: 193,000,000 total rows\n",
      "  Read 10,300,000 lines from file...\n",
      "  Processed chunk 387: 193,500,000 total rows\n",
      "  Read 10,350,000 lines from file...\n",
      "  Processed chunk 388: 194,000,000 total rows\n",
      "  Read 10,400,000 lines from file...\n",
      "  Processed chunk 389: 194,500,000 total rows\n",
      "  Read 10,450,000 lines from file...\n",
      "  Processed chunk 390: 195,000,000 total rows\n",
      "  Read 10,500,000 lines from file...\n",
      "  Processed chunk 391: 195,500,000 total rows\n",
      "  Read 10,550,000 lines from file...\n",
      "  Processed chunk 392: 196,000,000 total rows\n",
      "  Read 10,600,000 lines from file...\n",
      "  Processed chunk 393: 196,500,000 total rows\n",
      "  Read 10,650,000 lines from file...\n",
      "  Processed chunk 394: 197,000,000 total rows\n",
      "  Read 10,700,000 lines from file...\n",
      "  Processed chunk 395: 197,500,000 total rows\n",
      "  Read 10,750,000 lines from file...\n",
      "  Processed chunk 396: 198,000,000 total rows\n",
      "  Read 10,800,000 lines from file...\n",
      "  Processed chunk 397: 198,500,000 total rows\n",
      "  Read 10,850,000 lines from file...\n",
      "  Processed chunk 398: 199,000,000 total rows\n",
      "  Read 10,900,000 lines from file...\n",
      "  Processed chunk 399: 199,500,000 total rows\n",
      "  Read 10,950,000 lines from file...\n",
      "  Processed chunk 400: 200,000,000 total rows\n",
      "  Read 11,000,000 lines from file...\n",
      "  Processed chunk 401: 200,500,000 total rows\n",
      "  Read 11,050,000 lines from file...\n",
      "  Processed chunk 402: 201,000,000 total rows\n",
      "  Read 11,100,000 lines from file...\n",
      "  Processed chunk 403: 201,500,000 total rows\n",
      "  Read 11,150,000 lines from file...\n",
      "  Processed chunk 404: 202,000,000 total rows\n",
      "  Read 11,200,000 lines from file...\n",
      "  Processed chunk 405: 202,500,000 total rows\n",
      "  Read 11,250,000 lines from file...\n",
      "  Processed chunk 406: 203,000,000 total rows\n",
      "  Read 11,300,000 lines from file...\n",
      "  Processed chunk 407: 203,500,000 total rows\n",
      "  Read 11,350,000 lines from file...\n",
      "  Processed chunk 408: 204,000,000 total rows\n",
      "  Read 11,400,000 lines from file...\n",
      "  Processed chunk 409: 204,500,000 total rows\n",
      "  Read 11,450,000 lines from file...\n",
      "  Processed chunk 410: 205,000,000 total rows\n",
      "  Read 11,500,000 lines from file...\n",
      "  Processed chunk 411: 205,500,000 total rows\n",
      "  Read 11,550,000 lines from file...\n",
      "  Read 11,600,000 lines from file...\n",
      "  Processed chunk 412: 206,000,000 total rows\n",
      "  Read 11,650,000 lines from file...\n",
      "  Processed chunk 413: 206,500,000 total rows\n",
      "  Read 11,700,000 lines from file...\n",
      "  Processed chunk 414: 207,000,000 total rows\n",
      "  Read 11,750,000 lines from file...\n",
      "  Processed chunk 415: 207,500,000 total rows\n",
      "  Read 11,800,000 lines from file...\n",
      "  Processed chunk 416: 208,000,000 total rows\n",
      "  Read 11,850,000 lines from file...\n",
      "  Processed chunk 417: 208,500,000 total rows\n",
      "  Read 11,900,000 lines from file...\n",
      "  Read 11,950,000 lines from file...\n",
      "  Processed chunk 418: 209,000,000 total rows\n",
      "  Read 12,000,000 lines from file...\n",
      "  Processed chunk 419: 209,500,000 total rows\n",
      "  Read 12,050,000 lines from file...\n",
      "  Processed chunk 420: 210,000,000 total rows\n",
      "  Read 12,100,000 lines from file...\n",
      "  Processed chunk 421: 210,500,000 total rows\n",
      "  Read 12,150,000 lines from file...\n",
      "  Processed chunk 422: 211,000,000 total rows\n",
      "  Read 12,200,000 lines from file...\n",
      "  Read 12,250,000 lines from file...\n",
      "  Processed chunk 423: 211,500,000 total rows\n",
      "  Read 12,300,000 lines from file...\n",
      "  Processed chunk 424: 212,000,000 total rows\n",
      "  Read 12,350,000 lines from file...\n",
      "  Processed chunk 425: 212,500,000 total rows\n",
      "  Read 12,400,000 lines from file...\n",
      "  Processed chunk 426: 213,000,000 total rows\n",
      "  Read 12,450,000 lines from file...\n",
      "  Processed chunk 427: 213,500,000 total rows\n",
      "  Read 12,500,000 lines from file...\n",
      "  Read 12,550,000 lines from file...\n",
      "  Processed chunk 428: 214,000,000 total rows\n",
      "  Read 12,600,000 lines from file...\n",
      "  Processed chunk 429: 214,500,000 total rows\n",
      "  Read 12,650,000 lines from file...\n",
      "  Processed chunk 430: 215,000,000 total rows\n",
      "  Read 12,700,000 lines from file...\n",
      "  Processed chunk 431: 215,500,000 total rows\n",
      "  Read 12,750,000 lines from file...\n",
      "  Processed chunk 432: 216,000,000 total rows\n",
      "  Read 12,800,000 lines from file...\n",
      "  Read 12,850,000 lines from file...\n",
      "  Processed chunk 433: 216,500,000 total rows\n",
      "  Final chunk 434: 216,716,096 total rows\n",
      "Created 434 chunk files. Now combining...\n",
      "Combining chunks with memory-efficient lazy loading...\n",
      "  Combining chunk 1/434\n",
      "  Combining chunk 2/434\n",
      "  Combining chunk 3/434\n",
      "  Combining chunk 4/434\n",
      "  Combining chunk 5/434\n",
      "  Combining chunk 6/434\n",
      "  Combining chunk 7/434\n",
      "  Combining chunk 8/434\n",
      "  Combining chunk 9/434\n",
      "  Combining chunk 10/434\n",
      "  Combining chunk 11/434\n",
      "  Combining chunk 12/434\n",
      "  Combining chunk 13/434\n",
      "  Combining chunk 14/434\n",
      "  Combining chunk 15/434\n",
      "  Combining chunk 16/434\n",
      "  Combining chunk 17/434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining chunk 18/434\n",
      "  Combining chunk 19/434\n",
      "  Combining chunk 20/434\n",
      "  Combining chunk 21/434\n",
      "  Combining chunk 22/434\n",
      "  Combining chunk 23/434\n",
      "  Combining chunk 24/434\n",
      "  Combining chunk 25/434\n",
      "  Combining chunk 26/434\n",
      "  Combining chunk 27/434\n",
      "  Combining chunk 28/434\n",
      "  Combining chunk 29/434\n",
      "  Combining chunk 30/434\n",
      "  Combining chunk 31/434\n",
      "  Combining chunk 32/434\n",
      "  Combining chunk 33/434\n",
      "  Combining chunk 34/434\n",
      "  Combining chunk 35/434\n",
      "  Combining chunk 36/434\n",
      "  Combining chunk 37/434\n",
      "  Combining chunk 38/434\n",
      "  Combining chunk 39/434\n",
      "  Combining chunk 40/434\n",
      "  Combining chunk 41/434\n",
      "  Combining chunk 42/434\n",
      "  Combining chunk 43/434\n",
      "  Combining chunk 44/434\n",
      "  Combining chunk 45/434\n",
      "  Combining chunk 46/434\n",
      "  Combining chunk 47/434\n",
      "  Combining chunk 48/434\n",
      "  Combining chunk 49/434\n",
      "  Combining chunk 50/434\n",
      "  Combining chunk 51/434\n",
      "  Combining chunk 52/434\n",
      "  Combining chunk 53/434\n",
      "  Combining chunk 54/434\n",
      "  Combining chunk 55/434\n",
      "  Combining chunk 56/434\n",
      "  Combining chunk 57/434\n",
      "  Combining chunk 58/434\n",
      "  Combining chunk 59/434\n",
      "  Combining chunk 60/434\n",
      "  Combining chunk 61/434\n",
      "  Combining chunk 62/434\n",
      "  Combining chunk 63/434\n",
      "  Combining chunk 64/434\n",
      "  Combining chunk 65/434\n",
      "  Combining chunk 66/434\n",
      "  Combining chunk 67/434\n",
      "  Combining chunk 68/434\n",
      "  Combining chunk 69/434\n",
      "  Combining chunk 70/434\n",
      "  Combining chunk 71/434\n",
      "  Combining chunk 72/434\n",
      "  Combining chunk 73/434\n",
      "  Combining chunk 74/434\n",
      "  Combining chunk 75/434\n",
      "  Combining chunk 76/434\n",
      "  Combining chunk 77/434\n",
      "  Combining chunk 78/434\n",
      "  Combining chunk 79/434\n",
      "  Combining chunk 80/434\n",
      "  Combining chunk 81/434\n",
      "  Combining chunk 82/434\n",
      "  Combining chunk 83/434\n",
      "  Combining chunk 84/434\n",
      "  Combining chunk 85/434\n",
      "  Combining chunk 86/434\n",
      "  Combining chunk 87/434\n",
      "  Combining chunk 88/434\n",
      "  Combining chunk 89/434\n",
      "  Combining chunk 90/434\n",
      "  Combining chunk 91/434\n",
      "  Combining chunk 92/434\n",
      "  Combining chunk 93/434\n",
      "  Combining chunk 94/434\n",
      "  Combining chunk 95/434\n",
      "  Combining chunk 96/434\n",
      "  Combining chunk 97/434\n",
      "  Combining chunk 98/434\n",
      "  Combining chunk 99/434\n",
      "  Combining chunk 100/434\n",
      "  Combining chunk 101/434\n",
      "  Combining chunk 102/434\n",
      "  Combining chunk 103/434\n",
      "  Combining chunk 104/434\n",
      "  Combining chunk 105/434\n",
      "  Combining chunk 106/434\n",
      "  Combining chunk 107/434\n",
      "  Combining chunk 108/434\n",
      "  Combining chunk 109/434\n",
      "  Combining chunk 110/434\n",
      "  Combining chunk 111/434\n",
      "  Combining chunk 112/434\n",
      "  Combining chunk 113/434\n",
      "  Combining chunk 114/434\n",
      "  Combining chunk 115/434\n",
      "  Combining chunk 116/434\n",
      "  Combining chunk 117/434\n",
      "  Combining chunk 118/434\n",
      "  Combining chunk 119/434\n",
      "  Combining chunk 120/434\n",
      "  Combining chunk 121/434\n",
      "  Combining chunk 122/434\n",
      "  Combining chunk 123/434\n",
      "  Combining chunk 124/434\n",
      "  Combining chunk 125/434\n",
      "  Combining chunk 126/434\n",
      "  Combining chunk 127/434\n",
      "  Combining chunk 128/434\n",
      "  Combining chunk 129/434\n",
      "  Combining chunk 130/434\n",
      "  Combining chunk 131/434\n",
      "  Combining chunk 132/434\n",
      "  Combining chunk 133/434\n",
      "  Combining chunk 134/434\n",
      "  Combining chunk 135/434\n",
      "  Combining chunk 136/434\n",
      "  Combining chunk 137/434\n",
      "  Combining chunk 138/434\n",
      "  Combining chunk 139/434\n",
      "  Combining chunk 140/434\n",
      "  Combining chunk 141/434\n",
      "  Combining chunk 142/434\n",
      "  Combining chunk 143/434\n",
      "  Combining chunk 144/434\n",
      "  Combining chunk 145/434\n",
      "  Combining chunk 146/434\n",
      "  Combining chunk 147/434\n",
      "  Combining chunk 148/434\n",
      "  Combining chunk 149/434\n",
      "  Combining chunk 150/434\n",
      "  Combining chunk 151/434\n",
      "  Combining chunk 152/434\n",
      "  Combining chunk 153/434\n",
      "  Combining chunk 154/434\n",
      "  Combining chunk 155/434\n",
      "  Combining chunk 156/434\n",
      "  Combining chunk 157/434\n",
      "  Combining chunk 158/434\n",
      "  Combining chunk 159/434\n",
      "  Combining chunk 160/434\n",
      "  Combining chunk 161/434\n",
      "  Combining chunk 162/434\n",
      "  Combining chunk 163/434\n",
      "  Combining chunk 164/434\n",
      "  Combining chunk 165/434\n",
      "  Combining chunk 166/434\n",
      "  Combining chunk 167/434\n",
      "  Combining chunk 168/434\n",
      "  Combining chunk 169/434\n",
      "  Combining chunk 170/434\n",
      "  Combining chunk 171/434\n",
      "  Combining chunk 172/434\n",
      "  Combining chunk 173/434\n",
      "  Combining chunk 174/434\n",
      "  Combining chunk 175/434\n",
      "  Combining chunk 176/434\n",
      "  Combining chunk 177/434\n",
      "  Combining chunk 178/434\n",
      "  Combining chunk 179/434\n",
      "  Combining chunk 180/434\n",
      "  Combining chunk 181/434\n",
      "  Combining chunk 182/434\n",
      "  Combining chunk 183/434\n",
      "  Combining chunk 184/434\n",
      "  Combining chunk 185/434\n",
      "  Combining chunk 186/434\n",
      "  Combining chunk 187/434\n",
      "  Combining chunk 188/434\n",
      "  Combining chunk 189/434\n",
      "  Combining chunk 190/434\n",
      "  Combining chunk 191/434\n",
      "  Combining chunk 192/434\n",
      "  Combining chunk 193/434\n",
      "  Combining chunk 194/434\n",
      "  Combining chunk 195/434\n",
      "  Combining chunk 196/434\n",
      "  Combining chunk 197/434\n",
      "  Combining chunk 198/434\n",
      "  Combining chunk 199/434\n",
      "  Combining chunk 200/434\n",
      "  Combining chunk 201/434\n",
      "  Combining chunk 202/434\n",
      "  Combining chunk 203/434\n",
      "  Combining chunk 204/434\n",
      "  Combining chunk 205/434\n",
      "  Combining chunk 206/434\n",
      "  Combining chunk 207/434\n",
      "  Combining chunk 208/434\n",
      "  Combining chunk 209/434\n",
      "  Combining chunk 210/434\n",
      "  Combining chunk 211/434\n",
      "  Combining chunk 212/434\n",
      "  Combining chunk 213/434\n",
      "  Combining chunk 214/434\n",
      "  Combining chunk 215/434\n",
      "  Combining chunk 216/434\n",
      "  Combining chunk 217/434\n",
      "  Combining chunk 218/434\n",
      "  Combining chunk 219/434\n",
      "  Combining chunk 220/434\n",
      "  Combining chunk 221/434\n",
      "  Combining chunk 222/434\n",
      "  Combining chunk 223/434\n",
      "  Combining chunk 224/434\n",
      "  Combining chunk 225/434\n",
      "  Combining chunk 226/434\n",
      "  Combining chunk 227/434\n",
      "  Combining chunk 228/434\n",
      "  Combining chunk 229/434\n",
      "  Combining chunk 230/434\n",
      "  Combining chunk 231/434\n",
      "  Combining chunk 232/434\n",
      "  Combining chunk 233/434\n",
      "  Combining chunk 234/434\n",
      "  Combining chunk 235/434\n",
      "  Combining chunk 236/434\n",
      "  Combining chunk 237/434\n",
      "  Combining chunk 238/434\n",
      "  Combining chunk 239/434\n",
      "  Combining chunk 240/434\n",
      "  Combining chunk 241/434\n",
      "  Combining chunk 242/434\n",
      "  Combining chunk 243/434\n",
      "  Combining chunk 244/434\n",
      "  Combining chunk 245/434\n",
      "  Combining chunk 246/434\n",
      "  Combining chunk 247/434\n",
      "  Combining chunk 248/434\n",
      "  Combining chunk 249/434\n",
      "  Combining chunk 250/434\n",
      "  Combining chunk 251/434\n",
      "  Combining chunk 252/434\n",
      "  Combining chunk 253/434\n",
      "  Combining chunk 254/434\n",
      "  Combining chunk 255/434\n",
      "  Combining chunk 256/434\n",
      "  Combining chunk 257/434\n",
      "  Combining chunk 258/434\n",
      "  Combining chunk 259/434\n",
      "  Combining chunk 260/434\n",
      "  Combining chunk 261/434\n",
      "  Combining chunk 262/434\n",
      "  Combining chunk 263/434\n",
      "  Combining chunk 264/434\n",
      "  Combining chunk 265/434\n",
      "  Combining chunk 266/434\n",
      "  Combining chunk 267/434\n",
      "  Combining chunk 268/434\n",
      "  Combining chunk 269/434\n",
      "  Combining chunk 270/434\n",
      "  Combining chunk 271/434\n",
      "  Combining chunk 272/434\n",
      "  Combining chunk 273/434\n",
      "  Combining chunk 274/434\n",
      "  Combining chunk 275/434\n",
      "  Combining chunk 276/434\n",
      "  Combining chunk 277/434\n",
      "  Combining chunk 278/434\n",
      "  Combining chunk 279/434\n",
      "  Combining chunk 280/434\n",
      "  Combining chunk 281/434\n",
      "  Combining chunk 282/434\n",
      "  Combining chunk 283/434\n",
      "  Combining chunk 284/434\n",
      "  Combining chunk 285/434\n",
      "  Combining chunk 286/434\n",
      "  Combining chunk 287/434\n",
      "  Combining chunk 288/434\n",
      "  Combining chunk 289/434\n",
      "  Combining chunk 290/434\n",
      "  Combining chunk 291/434\n",
      "  Combining chunk 292/434\n",
      "  Combining chunk 293/434\n",
      "  Combining chunk 294/434\n",
      "  Combining chunk 295/434\n",
      "  Combining chunk 296/434\n",
      "  Combining chunk 297/434\n",
      "  Combining chunk 298/434\n",
      "  Combining chunk 299/434\n",
      "  Combining chunk 300/434\n",
      "  Combining chunk 301/434\n",
      "  Combining chunk 302/434\n",
      "  Combining chunk 303/434\n",
      "  Combining chunk 304/434\n",
      "  Combining chunk 305/434\n",
      "  Combining chunk 306/434\n",
      "  Combining chunk 307/434\n",
      "  Combining chunk 308/434\n",
      "  Combining chunk 309/434\n",
      "  Combining chunk 310/434\n",
      "  Combining chunk 311/434\n",
      "  Combining chunk 312/434\n",
      "  Combining chunk 313/434\n",
      "  Combining chunk 314/434\n",
      "  Combining chunk 315/434\n",
      "  Combining chunk 316/434\n",
      "  Combining chunk 317/434\n",
      "  Combining chunk 318/434\n",
      "  Combining chunk 319/434\n",
      "  Combining chunk 320/434\n",
      "  Combining chunk 321/434\n",
      "  Combining chunk 322/434\n",
      "  Combining chunk 323/434\n",
      "  Combining chunk 324/434\n",
      "  Combining chunk 325/434\n",
      "  Combining chunk 326/434\n",
      "  Combining chunk 327/434\n",
      "  Combining chunk 328/434\n",
      "  Combining chunk 329/434\n",
      "  Combining chunk 330/434\n",
      "  Combining chunk 331/434\n",
      "  Combining chunk 332/434\n",
      "  Combining chunk 333/434\n",
      "  Combining chunk 334/434\n",
      "  Combining chunk 335/434\n",
      "  Combining chunk 336/434\n",
      "  Combining chunk 337/434\n",
      "  Combining chunk 338/434\n",
      "  Combining chunk 339/434\n",
      "  Combining chunk 340/434\n",
      "  Combining chunk 341/434\n",
      "  Combining chunk 342/434\n",
      "  Combining chunk 343/434\n",
      "  Combining chunk 344/434\n",
      "  Combining chunk 345/434\n",
      "  Combining chunk 346/434\n",
      "  Combining chunk 347/434\n",
      "  Combining chunk 348/434\n",
      "  Combining chunk 349/434\n",
      "  Combining chunk 350/434\n",
      "  Combining chunk 351/434\n",
      "  Combining chunk 352/434\n",
      "  Combining chunk 353/434\n",
      "  Combining chunk 354/434\n",
      "  Combining chunk 355/434\n",
      "  Combining chunk 356/434\n",
      "  Combining chunk 357/434\n",
      "  Combining chunk 358/434\n",
      "  Combining chunk 359/434\n",
      "  Combining chunk 360/434\n",
      "  Combining chunk 361/434\n",
      "  Combining chunk 362/434\n",
      "  Combining chunk 363/434\n",
      "  Combining chunk 364/434\n",
      "  Combining chunk 365/434\n",
      "  Combining chunk 366/434\n",
      "  Combining chunk 367/434\n",
      "  Combining chunk 368/434\n",
      "  Combining chunk 369/434\n",
      "  Combining chunk 370/434\n",
      "  Combining chunk 371/434\n",
      "  Combining chunk 372/434\n",
      "  Combining chunk 373/434\n",
      "  Combining chunk 374/434\n",
      "  Combining chunk 375/434\n",
      "  Combining chunk 376/434\n",
      "  Combining chunk 377/434\n",
      "  Combining chunk 378/434\n",
      "  Combining chunk 379/434\n",
      "  Combining chunk 380/434\n",
      "  Combining chunk 381/434\n",
      "  Combining chunk 382/434\n",
      "  Combining chunk 383/434\n",
      "  Combining chunk 384/434\n",
      "  Combining chunk 385/434\n",
      "  Combining chunk 386/434\n",
      "  Combining chunk 387/434\n",
      "  Combining chunk 388/434\n",
      "  Combining chunk 389/434\n",
      "  Combining chunk 390/434\n",
      "  Combining chunk 391/434\n",
      "  Combining chunk 392/434\n",
      "  Combining chunk 393/434\n",
      "  Combining chunk 394/434\n",
      "  Combining chunk 395/434\n",
      "  Combining chunk 396/434\n",
      "  Combining chunk 397/434\n",
      "  Combining chunk 398/434\n",
      "  Combining chunk 399/434\n",
      "  Combining chunk 400/434\n",
      "  Combining chunk 401/434\n",
      "  Combining chunk 402/434\n",
      "  Combining chunk 403/434\n",
      "  Combining chunk 404/434\n",
      "  Combining chunk 405/434\n",
      "  Combining chunk 406/434\n",
      "  Combining chunk 407/434\n",
      "  Combining chunk 408/434\n",
      "  Combining chunk 409/434\n",
      "  Combining chunk 410/434\n",
      "  Combining chunk 411/434\n",
      "  Combining chunk 412/434\n",
      "  Combining chunk 413/434\n",
      "  Combining chunk 414/434\n",
      "  Combining chunk 415/434\n",
      "  Combining chunk 416/434\n",
      "  Combining chunk 417/434\n",
      "  Combining chunk 418/434\n",
      "  Combining chunk 419/434\n",
      "  Combining chunk 420/434\n",
      "  Combining chunk 421/434\n",
      "  Combining chunk 422/434\n",
      "  Combining chunk 423/434\n",
      "  Combining chunk 424/434\n",
      "  Combining chunk 425/434\n",
      "  Combining chunk 426/434\n",
      "  Combining chunk 427/434\n",
      "  Combining chunk 428/434\n",
      "  Combining chunk 429/434\n",
      "  Combining chunk 430/434\n",
      "  Combining chunk 431/434\n",
      "  Combining chunk 432/434\n",
      "  Combining chunk 433/434\n",
      "  Combining chunk 434/434\n",
      "Sorting and finalizing DataFrame...\n",
      "Final DataFrame shape: (216716096, 4)\n",
      "Saving optimized data to /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/train.parquet\n",
      "Saving as JSONL to /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/train.jsonl\n",
      "\n",
      "Training Data Statistics:\n",
      "  Total events: 216,716,096\n",
      "  Unique sessions: 12,899,779\n",
      "  Unique items: 1,855,603\n",
      "  Event types: ['clicks', 'carts', 'orders']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3041826221.py:155: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  type_counts = combined_df.group_by(\"type\").agg(pl.count().alias(\"count\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Event distribution:\n",
      "    carts: 16,896,191 (7.80%)\n",
      "    orders: 5,098,951 (2.35%)\n",
      "    clicks: 194,720,954 (89.85%)\n",
      "Cleaning up temporary chunk files...\n",
      "✅ Training Data conversion completed successfully!\n",
      "Converting Test Data from JSONL to optimized format (Memory Optimized)...\n",
      "Processing /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data-extracted/test.jsonl in memory-efficient chunks...\n",
      "  Read 50,000 lines from file...\n",
      "  Read 100,000 lines from file...\n",
      "  Processed chunk 1: 500,000 total rows\n",
      "  Read 150,000 lines from file...\n",
      "  Read 200,000 lines from file...\n",
      "  Processed chunk 2: 1,000,000 total rows\n",
      "  Read 250,000 lines from file...\n",
      "  Read 300,000 lines from file...\n",
      "  Processed chunk 3: 1,500,000 total rows\n",
      "  Read 350,000 lines from file...\n",
      "  Read 400,000 lines from file...\n",
      "  Read 450,000 lines from file...\n",
      "  Processed chunk 4: 2,000,000 total rows\n",
      "  Read 500,000 lines from file...\n",
      "  Read 550,000 lines from file...\n",
      "  Processed chunk 5: 2,500,000 total rows\n",
      "  Read 600,000 lines from file...\n",
      "  Read 650,000 lines from file...\n",
      "  Processed chunk 6: 3,000,000 total rows\n",
      "  Read 700,000 lines from file...\n",
      "  Read 750,000 lines from file...\n",
      "  Read 800,000 lines from file...\n",
      "  Processed chunk 7: 3,500,000 total rows\n",
      "  Read 850,000 lines from file...\n",
      "  Read 900,000 lines from file...\n",
      "  Processed chunk 8: 4,000,000 total rows\n",
      "  Read 950,000 lines from file...\n",
      "  Read 1,000,000 lines from file...\n",
      "  Read 1,050,000 lines from file...\n",
      "  Processed chunk 9: 4,500,000 total rows\n",
      "  Read 1,100,000 lines from file...\n",
      "  Read 1,150,000 lines from file...\n",
      "  Processed chunk 10: 5,000,000 total rows\n",
      "  Read 1,200,000 lines from file...\n",
      "  Read 1,250,000 lines from file...\n",
      "  Processed chunk 11: 5,500,000 total rows\n",
      "  Read 1,300,000 lines from file...\n",
      "  Read 1,350,000 lines from file...\n",
      "  Read 1,400,000 lines from file...\n",
      "  Processed chunk 12: 6,000,000 total rows\n",
      "  Read 1,450,000 lines from file...\n",
      "  Read 1,500,000 lines from file...\n",
      "  Read 1,550,000 lines from file...\n",
      "  Processed chunk 13: 6,500,000 total rows\n",
      "  Read 1,600,000 lines from file...\n",
      "  Read 1,650,000 lines from file...\n",
      "  Final chunk 14: 6,928,123 total rows\n",
      "Created 14 chunk files. Now combining...\n",
      "Combining chunks with memory-efficient lazy loading...\n",
      "  Combining chunk 1/14\n",
      "  Combining chunk 2/14\n",
      "  Combining chunk 3/14\n",
      "  Combining chunk 4/14\n",
      "  Combining chunk 5/14\n",
      "  Combining chunk 6/14\n",
      "  Combining chunk 7/14\n",
      "  Combining chunk 8/14\n",
      "  Combining chunk 9/14\n",
      "  Combining chunk 10/14\n",
      "  Combining chunk 11/14\n",
      "  Combining chunk 12/14\n",
      "  Combining chunk 13/14\n",
      "  Combining chunk 14/14\n",
      "Sorting and finalizing DataFrame...\n",
      "Final DataFrame shape: (6928123, 4)\n",
      "Saving optimized data to /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/test.parquet\n",
      "Saving as JSONL to /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/test.jsonl\n",
      "\n",
      "Test Data Statistics:\n",
      "  Total events: 6,928,123\n",
      "  Unique sessions: 1,671,803\n",
      "  Unique items: 783,486\n",
      "  Event types: ['clicks', 'carts', 'orders']\n",
      "  Event distribution:\n",
      "    clicks: 6,292,632 (90.83%)\n",
      "    carts: 570,011 (8.23%)\n",
      "    orders: 65,480 (0.95%)\n",
      "Cleaning up temporary chunk files...\n",
      "✅ Test Data conversion completed successfully!\n",
      "✅ Memory-optimized conversion completed successfully!\n",
      "Memory cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "def convert_jsonl_to_optimized_format_fixed(input_path: str, output_path: str, dataset_name: str):\n",
    "    \"\"\"\n",
    "    Memory-efficient conversion from JSONL to optimized format using streaming and chunking\n",
    "    \"\"\"\n",
    "    print(f\"Converting {dataset_name} from JSONL to optimized format (Memory Optimized)...\")\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"❌ Input file not found: {input_path}\")\n",
    "        return False\n",
    "\n",
    "    # Check if output already exists\n",
    "    parquet_path = output_path.replace('.jsonl', '.parquet')\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"✅ {dataset_name} already converted\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {input_path} in memory-efficient chunks...\")\n",
    "\n",
    "        # Process in very small chunks to avoid memory issues\n",
    "        CHUNK_SIZE = 500000  # Reduced chunk size\n",
    "        chunk_files = []\n",
    "        chunk_counter = 0\n",
    "\n",
    "        # Create temporary directory for chunks\n",
    "        temp_dir = f\"/tmp/otto_chunks_{dataset_name.lower().replace(' ', '_')}\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        with open(input_path, 'r') as f:\n",
    "            chunk_rows = []\n",
    "            total_processed = 0\n",
    "\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    session_id = data['session']\n",
    "\n",
    "                    # Process events for this session\n",
    "                    for event in data['events']:\n",
    "                        chunk_rows.append({\n",
    "                            'session': session_id,\n",
    "                            'aid': event['aid'],\n",
    "                            'ts': event['ts'],\n",
    "                            'type': event['type']\n",
    "                        })\n",
    "\n",
    "                        # Process chunk when it reaches CHUNK_SIZE\n",
    "                        if len(chunk_rows) >= CHUNK_SIZE:\n",
    "                            # Convert chunk to DataFrame and save immediately\n",
    "                            chunk_df = pl.DataFrame(chunk_rows)\n",
    "\n",
    "                            # Optimize data types for the chunk\n",
    "                            chunk_df = chunk_df.with_columns([\n",
    "                                pl.col(\"session\").cast(pl.UInt32),\n",
    "                                pl.col(\"aid\").cast(pl.UInt32),\n",
    "                                pl.col(\"ts\").cast(pl.UInt64),\n",
    "                                pl.col(\"type\").cast(pl.Categorical)\n",
    "                            ])\n",
    "\n",
    "                            # Save chunk to temporary file\n",
    "                            chunk_file = os.path.join(temp_dir, f\"chunk_{chunk_counter}.parquet\")\n",
    "                            chunk_df.write_parquet(chunk_file)\n",
    "                            chunk_files.append(chunk_file)\n",
    "\n",
    "                            total_processed += len(chunk_rows)\n",
    "                            chunk_counter += 1\n",
    "\n",
    "                            print(f\"  Processed chunk {chunk_counter}: {total_processed:,} total rows\")\n",
    "\n",
    "                            # Clear chunk from memory\n",
    "                            chunk_rows = []\n",
    "                            del chunk_df\n",
    "                            import gc\n",
    "                            gc.collect()\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  Warning: Skipping invalid JSON at line {i+1}\")\n",
    "                    continue\n",
    "                except KeyError as e:\n",
    "                    print(f\"  Warning: Missing key at line {i+1}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Error processing line {i+1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Progress update every 50k lines\n",
    "                if i > 0 and i % 50000 == 0:\n",
    "                    print(f\"  Read {i:,} lines from file...\")\n",
    "\n",
    "            # Process remaining rows\n",
    "            if chunk_rows:\n",
    "                chunk_df = pl.DataFrame(chunk_rows)\n",
    "                chunk_df = chunk_df.with_columns([\n",
    "                    pl.col(\"session\").cast(pl.UInt32),\n",
    "                    pl.col(\"aid\").cast(pl.UInt32),\n",
    "                    pl.col(\"ts\").cast(pl.UInt64),\n",
    "                    pl.col(\"type\").cast(pl.Categorical)\n",
    "                ])\n",
    "\n",
    "                chunk_file = os.path.join(temp_dir, f\"chunk_{chunk_counter}.parquet\")\n",
    "                chunk_df.write_parquet(chunk_file)\n",
    "                chunk_files.append(chunk_file)\n",
    "\n",
    "                total_processed += len(chunk_rows)\n",
    "                chunk_counter += 1\n",
    "                print(f\"  Final chunk {chunk_counter}: {total_processed:,} total rows\")\n",
    "\n",
    "                del chunk_df, chunk_rows\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "        print(f\"Created {len(chunk_files)} chunk files. Now combining...\")\n",
    "\n",
    "        # Combine all chunks using lazy evaluation\n",
    "        print(\"Combining chunks with memory-efficient lazy loading...\")\n",
    "\n",
    "        # Read and combine chunks one by one\n",
    "        combined_df = None\n",
    "        for i, chunk_file in enumerate(chunk_files):\n",
    "            print(f\"  Combining chunk {i+1}/{len(chunk_files)}\")\n",
    "\n",
    "            chunk_df = pl.read_parquet(chunk_file)\n",
    "\n",
    "            if combined_df is None:\n",
    "                combined_df = chunk_df\n",
    "            else:\n",
    "                combined_df = pl.concat([combined_df, chunk_df])\n",
    "\n",
    "            # Periodic memory cleanup\n",
    "            if i % 10 == 0:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "        print(\"Sorting and finalizing DataFrame...\")\n",
    "        # Sort for better compression and access patterns\n",
    "        combined_df = combined_df.sort([\"session\", \"ts\"])\n",
    "\n",
    "        print(f\"Final DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "        # Save as parquet\n",
    "        print(f\"Saving optimized data to {parquet_path}\")\n",
    "        combined_df.write_parquet(parquet_path)\n",
    "\n",
    "        # Also save as JSONL for compatibility\n",
    "        print(f\"Saving as JSONL to {output_path}\")\n",
    "        combined_df.write_ndjson(output_path)\n",
    "\n",
    "        # Print dataset statistics\n",
    "        print(f\"\\n{dataset_name} Statistics:\")\n",
    "        print(f\"  Total events: {len(combined_df):,}\")\n",
    "        print(f\"  Unique sessions: {combined_df.select(pl.col('session')).n_unique():,}\")\n",
    "        print(f\"  Unique items: {combined_df.select(pl.col('aid')).n_unique():,}\")\n",
    "        print(f\"  Event types: {combined_df.select(pl.col('type')).unique().to_series().to_list()}\")\n",
    "\n",
    "        type_counts = combined_df.group_by(\"type\").agg(pl.count().alias(\"count\"))\n",
    "        print(f\"  Event distribution:\")\n",
    "        for row in type_counts.iter_rows():\n",
    "            event_type, count = row\n",
    "            percentage = count / len(combined_df) * 100\n",
    "            print(f\"    {event_type}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "        # Cleanup temporary files\n",
    "        print(\"Cleaning up temporary chunk files...\")\n",
    "        for chunk_file in chunk_files:\n",
    "            try:\n",
    "                os.remove(chunk_file)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            os.rmdir(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"✅ {dataset_name} conversion completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error converting {dataset_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Alternative streaming approach using generators (even more memory efficient)\n",
    "def convert_jsonl_streaming_approach(input_path: str, output_path: str, dataset_name: str):\n",
    "    \"\"\"\n",
    "    Ultra memory-efficient streaming approach using generators\n",
    "    \"\"\"\n",
    "    print(f\"Converting {dataset_name} using streaming approach...\")\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"❌ Input file not found: {input_path}\")\n",
    "        return False\n",
    "\n",
    "    parquet_path = output_path.replace('.jsonl', '.parquet')\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"✅ {dataset_name} already converted\")\n",
    "        return True\n",
    "\n",
    "    def event_generator():\n",
    "        \"\"\"Generator that yields events one by one\"\"\"\n",
    "        with open(input_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    session_id = data['session']\n",
    "\n",
    "                    for event in data['events']:\n",
    "                        yield {\n",
    "                            'session': session_id,\n",
    "                            'aid': event['aid'],\n",
    "                            'ts': event['ts'],\n",
    "                            'type': event['type']\n",
    "                        }\n",
    "\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    continue  # Skip invalid lines\n",
    "\n",
    "                if i % 100000 == 0 and i > 0:\n",
    "                    print(f\"  Processed {i:,} lines...\")\n",
    "\n",
    "    try:\n",
    "        # Process in small batches using the generator\n",
    "        BATCH_SIZE = 5000  # Very small batch size\n",
    "        batch = []\n",
    "        all_batches = []\n",
    "\n",
    "        print(\"Processing events in small batches...\")\n",
    "        for event in event_generator():\n",
    "            batch.append(event)\n",
    "\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                # Convert batch to DataFrame\n",
    "                batch_df = pl.DataFrame(batch).with_columns([\n",
    "                    pl.col(\"session\").cast(pl.UInt32),\n",
    "                    pl.col(\"aid\").cast(pl.UInt32),\n",
    "                    pl.col(\"ts\").cast(pl.UInt64),\n",
    "                    pl.col(\"type\").cast(pl.Categorical)\n",
    "                ])\n",
    "\n",
    "                all_batches.append(batch_df)\n",
    "                batch = []\n",
    "\n",
    "                if len(all_batches) % 100 == 0:\n",
    "                    print(f\"  Processed {len(all_batches) * BATCH_SIZE:,} events...\")\n",
    "\n",
    "                    # Periodically combine and clear batches to manage memory\n",
    "                    if len(all_batches) >= 200:  # Combine every 1M events\n",
    "                        print(\"  Combining batches to manage memory...\")\n",
    "                        combined_batch = pl.concat(all_batches)\n",
    "                        all_batches = [combined_batch]\n",
    "                        import gc\n",
    "                        gc.collect()\n",
    "\n",
    "        # Process remaining events\n",
    "        if batch:\n",
    "            batch_df = pl.DataFrame(batch).with_columns([\n",
    "                pl.col(\"session\").cast(pl.UInt32),\n",
    "                pl.col(\"aid\").cast(pl.UInt32),\n",
    "                pl.col(\"ts\").cast(pl.UInt64),\n",
    "                pl.col(\"type\").cast(pl.Categorical)\n",
    "            ])\n",
    "            all_batches.append(batch_df)\n",
    "\n",
    "        # Final combination\n",
    "        print(\"Final combination of all batches...\")\n",
    "        final_df = pl.concat(all_batches).sort([\"session\", \"ts\"])\n",
    "\n",
    "        # Save results\n",
    "        print(f\"Saving to {parquet_path}\")\n",
    "        final_df.write_parquet(parquet_path)\n",
    "        final_df.write_ndjson(output_path)\n",
    "\n",
    "        print(f\"✅ {dataset_name} conversion completed!\")\n",
    "        print(f\"  Total events: {len(final_df):,}\")\n",
    "        print(f\"  Memory efficient processing successful!\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Streaming conversion failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Updated main conversion section - replace the original conversion calls with this:\n",
    "print(\"=\"*60)\n",
    "print(\"MEMORY-OPTIMIZED DATA CONVERSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, try the chunk-based approach\n",
    "print(\"Attempting chunk-based conversion...\")\n",
    "\n",
    "# Convert training data with fixed approach\n",
    "train_input = os.path.join(config.EXTRACT_PATH, 'train.jsonl')\n",
    "train_output = os.path.join(config.DATA_PATH, 'train.jsonl')\n",
    "\n",
    "train_success = convert_jsonl_to_optimized_format_fixed(train_input, train_output, \"Training Data\")\n",
    "\n",
    "# If chunk-based approach fails, try streaming approach\n",
    "if not train_success:\n",
    "    print(\"\\nChunk-based approach failed. Trying streaming approach...\")\n",
    "    train_success = convert_jsonl_streaming_approach(train_input, train_output, \"Training Data\")\n",
    "\n",
    "# Convert test data\n",
    "test_input = os.path.join(config.EXTRACT_PATH, 'test.jsonl')\n",
    "test_output = os.path.join(config.DATA_PATH, 'test.jsonl')\n",
    "\n",
    "test_success = convert_jsonl_to_optimized_format_fixed(test_input, test_output, \"Test Data\")\n",
    "\n",
    "if not test_success:\n",
    "    print(\"\\nChunk-based approach failed for test data. Trying streaming approach...\")\n",
    "    test_success = convert_jsonl_streaming_approach(test_input, test_output, \"Test Data\")\n",
    "\n",
    "# Final check\n",
    "if not (train_success and test_success):\n",
    "    print(\"❌ CRITICAL: Data conversion failed even with memory optimization!\")\n",
    "    print(\"Suggestions:\")\n",
    "    print(\"1. Restart Colab runtime and try again\")\n",
    "    print(\"2. Use 'High-RAM' runtime setting\")\n",
    "    print(\"3. Process train and test data separately in different sessions\")\n",
    "    print(\"4. Consider using a subset of data for initial testing\")\n",
    "else:\n",
    "    print(\"✅ Memory-optimized conversion completed successfully!\")\n",
    "\n",
    "# Memory cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "print(f\"Memory cleanup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oME6KudtBjx"
   },
   "source": [
    "## Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnNEQ8d5tB3c",
    "outputId": "8e30a30a-e450-4b95-81dd-841239656d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating converted data...\n",
      "✅ Training Parquet: 1535.0 MB\n",
      "✅ Test Parquet: 55.3 MB\n",
      "✅ Training JSONL: 14102.2 MB\n",
      "✅ Test JSONL: 458.0 MB\n",
      "\n",
      "Validating data integrity...\n",
      "Training data: 216,716,096 rows, 4 columns\n",
      "Test data: 6,928,123 rows, 4 columns\n",
      "✅ Training data has all required columns\n",
      "✅ Test data has all required columns\n",
      "\n",
      "Data types validation:\n",
      "✅ session: UInt32\n",
      "✅ aid: UInt32\n",
      "✅ ts: UInt64\n",
      "✅ type: Categorical(ordering='physical')\n",
      "\n",
      "Null values check:\n",
      "✅ Training: No null values\n",
      "✅ Test: No null values\n",
      "✅ No session overlap between train and test\n",
      "\n",
      "✅ Data validation completed successfully\n",
      "✅ Data preparation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def validate_converted_data():\n",
    "    \"\"\"\n",
    "    Validate the converted data files\n",
    "    \"\"\"\n",
    "    print(\"Validating converted data...\")\n",
    "\n",
    "    # Check if files exist\n",
    "    train_parquet = os.path.join(config.DATA_PATH, 'train.parquet')\n",
    "    test_parquet = os.path.join(config.DATA_PATH, 'test.parquet')\n",
    "    train_jsonl = os.path.join(config.DATA_PATH, 'train.jsonl')\n",
    "    test_jsonl = os.path.join(config.DATA_PATH, 'test.jsonl')\n",
    "\n",
    "    files_to_check = [\n",
    "        (\"Training Parquet\", train_parquet),\n",
    "        (\"Test Parquet\", test_parquet),\n",
    "        (\"Training JSONL\", train_jsonl),\n",
    "        (\"Test JSONL\", test_jsonl)\n",
    "    ]\n",
    "\n",
    "    all_exist = True\n",
    "    for name, path in files_to_check:\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"✅ {name}: {size_mb:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"❌ {name}: Not found\")\n",
    "            all_exist = False\n",
    "\n",
    "    if not all_exist:\n",
    "        return False\n",
    "\n",
    "    # Validate data integrity\n",
    "    try:\n",
    "        print(\"\\nValidating data integrity...\")\n",
    "\n",
    "        # Load and check training data\n",
    "        train_df = pl.read_parquet(train_parquet)\n",
    "        test_df = pl.read_parquet(test_parquet)\n",
    "\n",
    "        print(f\"Training data: {train_df.shape[0]:,} rows, {train_df.shape[1]} columns\")\n",
    "        print(f\"Test data: {test_df.shape[0]:,} rows, {test_df.shape[1]} columns\")\n",
    "\n",
    "        # Check required columns\n",
    "        required_cols = ['session', 'aid', 'ts', 'type']\n",
    "        for df_name, df in [(\"Training\", train_df), (\"Test\", test_df)]:\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                print(f\"❌ {df_name} data missing columns: {missing_cols}\")\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"✅ {df_name} data has all required columns\")\n",
    "\n",
    "        # Check data types\n",
    "        print(f\"\\nData types validation:\")\n",
    "        for col in required_cols:\n",
    "            train_type = train_df[col].dtype\n",
    "            test_type = test_df[col].dtype\n",
    "            if train_type != test_type:\n",
    "                print(f\"❌ Type mismatch for {col}: train={train_type}, test={test_type}\")\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"✅ {col}: {train_type}\")\n",
    "\n",
    "        # Check for null values\n",
    "        print(f\"\\nNull values check:\")\n",
    "        for df_name, df in [(\"Training\", train_df), (\"Test\", test_df)]:\n",
    "            null_counts = df.null_count()\n",
    "            has_nulls = False\n",
    "            for row in null_counts.iter_rows():\n",
    "                for i, count in enumerate(row):\n",
    "                    if count > 0:\n",
    "                        col_name = df.columns[i]\n",
    "                        print(f\"⚠️  {df_name} {col_name}: {count} null values\")\n",
    "                        has_nulls = True\n",
    "            if not has_nulls:\n",
    "                print(f\"✅ {df_name}: No null values\")\n",
    "\n",
    "        # Check session overlap between train and test\n",
    "        train_sessions = set(train_df.select(\"session\").unique().to_series().to_list())\n",
    "        test_sessions = set(test_df.select(\"session\").unique().to_series().to_list())\n",
    "\n",
    "        overlap = train_sessions & test_sessions\n",
    "        if overlap:\n",
    "            print(f\"⚠️  Session overlap between train and test: {len(overlap):,} sessions\")\n",
    "        else:\n",
    "            print(f\"✅ No session overlap between train and test\")\n",
    "\n",
    "        print(f\"\\n✅ Data validation completed successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate converted data\n",
    "validation_success = validate_converted_data()\n",
    "\n",
    "if not validation_success:\n",
    "    print(\"❌ Data validation failed\")\n",
    "else:\n",
    "    print(\"✅ Data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3NZV4SZtJUO"
   },
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VIOQPNmtJqs",
    "outputId": "357fe80f-6847-456e-a1c0-f27ef88f6c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA PREPARATION SUMMARY\n",
      "============================================================\n",
      "Prepared data location: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/\n",
      "Output location: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/\n",
      "\n",
      "Generated files:\n",
      "  ✅ train.parquet: 1535.0 MB - Training data (Parquet format)\n",
      "  ✅ test.parquet: 55.3 MB - Test data (Parquet format)\n",
      "  ✅ train.jsonl: 14102.2 MB - Training data (JSONL format)\n",
      "  ✅ test.jsonl: 458.0 MB - Test data (JSONL format)\n",
      "\n",
      "Dataset statistics:\n",
      "  Training events: 216,716,096\n",
      "  Test events: 6,928,123\n",
      "  Training sessions: 12,899,779\n",
      "  Test sessions: 1,671,803\n",
      "  Unique items (train): 1,855,603\n",
      "  Unique items (test): 783,486\n",
      "\n",
      "✅ Data preparation completed successfully!\n",
      "Next step: Run the main solution notebook\n",
      "Make sure to update the DATA_PATH in the main notebook to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-data/\n",
      "============================================================\n",
      "Configuration saved to: /content/drive/MyDrive/Colab Notebooks/CML/Assignment 1/content/otto-output/data_config.json\n"
     ]
    }
   ],
   "source": [
    "def print_preparation_summary():\n",
    "    \"\"\"\n",
    "    Print summary of data preparation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA PREPARATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # File locations\n",
    "    print(f\"Prepared data location: {config.DATA_PATH}\")\n",
    "    print(f\"Output location: {config.OUTPUT_PATH}\")\n",
    "\n",
    "    # File sizes\n",
    "    files = [\n",
    "        ('train.parquet', 'Training data (Parquet format)'),\n",
    "        ('test.parquet', 'Test data (Parquet format)'),\n",
    "        ('train.jsonl', 'Training data (JSONL format)'),\n",
    "        ('test.jsonl', 'Test data (JSONL format)')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for filename, description in files:\n",
    "        filepath = os.path.join(config.DATA_PATH, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "            print(f\"  ✅ {filename}: {size_mb:.1f} MB - {description}\")\n",
    "        else:\n",
    "            print(f\"  ❌ {filename}: Missing - {description}\")\n",
    "\n",
    "    # Data statistics\n",
    "    try:\n",
    "        train_df = pl.read_parquet(os.path.join(config.DATA_PATH, 'train.parquet'))\n",
    "        test_df = pl.read_parquet(os.path.join(config.DATA_PATH, 'test.parquet'))\n",
    "\n",
    "        print(f\"\\nDataset statistics:\")\n",
    "        print(f\"  Training events: {len(train_df):,}\")\n",
    "        print(f\"  Test events: {len(test_df):,}\")\n",
    "        print(f\"  Training sessions: {train_df.select('session').n_unique():,}\")\n",
    "        print(f\"  Test sessions: {test_df.select('session').n_unique():,}\")\n",
    "        print(f\"  Unique items (train): {train_df.select('aid').n_unique():,}\")\n",
    "        print(f\"  Unique items (test): {test_df.select('aid').n_unique():,}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not load statistics: {e}\")\n",
    "\n",
    "    print(f\"\\n✅ Data preparation completed successfully!\")\n",
    "    print(f\"Next step: Run the main solution notebook\")\n",
    "    print(f\"Make sure to update the DATA_PATH in the main notebook to: {config.DATA_PATH}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print_preparation_summary()\n",
    "\n",
    "# Save configuration for main notebook\n",
    "config_dict = {\n",
    "    'DATA_PATH': config.DATA_PATH,\n",
    "    'OUTPUT_PATH': config.OUTPUT_PATH,\n",
    "    'preparation_completed': True,\n",
    "    'validation_passed': validation_success\n",
    "}\n",
    "\n",
    "config_path = os.path.join(config.OUTPUT_PATH, 'data_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved to: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
